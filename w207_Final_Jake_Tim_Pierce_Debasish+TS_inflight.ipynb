{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Spring 2019 Final Project\n",
    "## Kaggle Competition: Forest Cover Prediction\n",
    "**Pierce Coggins, Jake Mitchell, Debasish Mukhopadhyay, and Tim Slade**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents/Section Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction](#introduction)\n",
    "- [Housekeeping](#housekeeping)\n",
    "  - [Importing Libraries and Helper Functions](#import)\n",
    "  - [Splitting Data into Train and Test Sets](#splittingData)\n",
    "- [About the Data](#aboutTheData)\n",
    "  - [Initial Exploration of the Challenge](#summary)\n",
    "  - [Where do we start?](#exploratoryDataAnalysis)\n",
    "  - [Checking Label Imbalance](#checkingLabelImbalance)\n",
    "  - [Cleaning the Data](#dataCleaning)\n",
    "- [Feature Engineering](#featureEngineering)\n",
    "  - Describe a basic model that we will use to test the usefulness of new features (LR or NB)\n",
    "  - Normalization\n",
    "  - Review each added or removed feature\n",
    "- [Models](#models)\n",
    "  - [Logistic Regression](#logistic)\n",
    "  - \n",
    "- [Error Analysis]\n",
    "  - Summary and remediation of errors in model\n",
    "- [Results](#results)\n",
    "  - What went well, what went poorly\n",
    "  - Final comparison of models on test data\n",
    "- [Conclusion](#conclusion)\n",
    "- [Annexes](#annexA)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, our objective is to predict the forest cover type (defined as the predominant type of tree cover) for an area of land in Colorado given only cartographic input variables.  This problem and dataset was initially posted on Kaggle in 2015.  We have elected to take on this problem as it uniquely allows for various machine learning techniques to be applied.  This report provides a detailed explanation of our methodology from data exploration through to model creation and evaluation.\n",
    "\n",
    "The problem of understanding what type of vegetation is present in a given area of wilderness is surprisingly important.  In this example we are exploring the forests of Colorado, which are home to a diverse set of trees each with their own enviornmental benefits and vulnerabilities.  For example, many of the pine trees in Colorado are susceptible to the [mountain pine beetle](https://csfs.colostate.edu/forest-management/common-forest-insects-diseases/mountain-pine-beetle/), while Spruce and Fir trees are much less suceptible.  Without manually surveying thousands of miles of wilderness, it is very difficult to distinguish these types of trees from one another as they look very similar from the air.  However, it is relatively easy to get cartographic data for larger wilderness areas. Given that this is the challenge at hand, if it were possible to accurately predict the predominant forest cover type from cartographic data then all Colorado forests could be mapped using this method. That information would be invaluable to firefighters and forest service personnel to direct their resources to the areas of wilderness that may be more vulnerable to certain pests or environmental hazards.\n",
    "\n",
    "If you would like to learn more about the problem or try for yourself, all information and data can be found within the kaggle competition:<br>[Kaggle's Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"housekeeping\"></a>\n",
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "### Importing Libraries, Helper Functions, and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T17:16:42.944326Z",
     "start_time": "2019-04-12T17:11:56.473050Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Dropbox\\BerkeleyMIDS\\projects\\w207_final_project\\w207_final_helper_functions.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m'''Used to generate the comparison graphs for the results discussion'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcompareModel_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshare_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange0to1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_legend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Tall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Random\\nForest'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imgs/modelCompRandomForest_nolgd_tall.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcompareModel_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msupport_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshare_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange0to1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_legend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Tall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Support Vector\\nMachines'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imgs/modelCompSupportVector_nolgd_tall.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcompareModel_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknn3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshare_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange0to1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_legend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Tall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'3-Nearest\\nNeighbors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imgs/modelComp3NearestNeighbors_nolgd_tall.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcompareModel_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshare_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange0to1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_legend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Tall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Logistic\\nRegression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imgs/modelCompLogisticRegression_nolgd_tall.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_forest' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# %matplotlib inline\n",
    "# %matplotlib notebook\n",
    "%matplotlib qt\n",
    "\n",
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "# Plotting and printing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "from matplotlib.image import imread\n",
    "import pprint\n",
    "\n",
    "# Model-building libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler, StandardScaler, RobustScaler, Normalizer, scale\n",
    "\n",
    "# SK-learn libraries for learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Run the helper functions notebook\n",
    "%run w207_final_helper_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"splittingData\"></a>\n",
    "### Splitting Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, the forest cover types we aim to predict are included with the features we plan to use to predict them. Our first step is to separate our input variables from our output variable. We also want to split the dataset into _train_ and _test_ subsets; this will give us insight into how well our chosen models and parameters will perform against out-of-sample data.\n",
    "\n",
    "The original dataset contained 15,120 observations. We will train our models on 90% of the data and hold out 10% for testing. We thus expect to have approximately 13,608 observations in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T17:16:43.764988Z",
     "start_time": "2019-04-12T17:16:42.952325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                 | Observations |  Features  |\n",
      "----------------------------------------------\n",
      "Training dataset |    13608     |     54     |\n",
      "Training labels  |    13608     |     --     |\n",
      "  Test dataset   |     1512     |     54     |\n",
      "  Test labels    |     1512     |     --     |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "full_data = pd.DataFrame.from_csv('./train.csv')\n",
    "full_data.shape\n",
    "\n",
    "# Separating out the labels\n",
    "full_labels = full_data['Cover_Type']\n",
    "full_features = full_data.drop('Cover_Type', axis=1)\n",
    "\n",
    "# Setting seed so we get consistent results from our splitting\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(full_features, full_labels, test_size=0.10)\n",
    "\n",
    "# Verifying our data shapes are as expected\n",
    "print(f'''\n",
    "{'':^16} | {'Observations':^12} | {'Features':^10} |\n",
    "{'-'*46}\n",
    "{'Training dataset':^16} | {X_train.shape[0]:^12} | {X_train.shape[1]:^10} |\n",
    "{'Training labels':^16} | {y_train.shape[0]:^12} | {'--':^10} |\n",
    "{'Test dataset':^16} | {X_test.shape[0]:^12} | {X_test.shape[1]:^10} |\n",
    "{'Test labels':^16} | {y_test.shape[0]:^12} | {'--':^10} |\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aboutTheData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aboutTheData\"></a>\n",
    "# About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from several wilderness areas in northern Colorado, specifically the Rawah Wilderness Area, Neota Wilderness Area, Comanche Peak Wilderness Area and the Cache la Poudre Wilderness Area.  These are all fairly remote areas of Colorado, which is likely why they were chosen; there is unlikely to be evidence of human influence in these regions, simplifying our prediction task.\n",
    "\n",
    "The features in the dataset are all cartographic measures of a 30x30m square plot of land.  We have 10 simple features. The 11th and 12th - `wilderness_area` and `soil_type` - are categorical variables comprised of 4 and 40 dummy variables respectively. We therefore have a total of 54 features to work with.\n",
    "The list below contains a short description of each feature, including its range, median, and mean when relevant. (See [Annex A](#annexA) for the associated code and further discussion of the exploratory data analysis).\n",
    "\n",
    "- `Elevation`: _Elevation in meters_\n",
    "  - **Range**: 1863 to 3849 | **Mean**: 2749.3 | **Median**: 2752\n",
    "\n",
    "\n",
    "- `Aspect`: _Aspect in degrees azimuth. i.e., degrees clockwise from a line pointed at true North. So North = 0$^\\circ$, East = 90$^\\circ$, South = 180$^\\circ$, and West = 270$^\\circ$_\n",
    "  - **Range**: 0 to 360 | **Mean**: 156.7 | **Median**: 126.0\n",
    "\n",
    "\n",
    "- `Slope`: _Slope in degrees. 0$^\\circ$ would indicate a flat plane; greater values represent steeper slopes._\n",
    "  - **Range**: 0 to 52 | **Mean**: 16.5 | **Median**: 15.0 \n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Hydrology`: _Horizontal distance to nearest surface water features. Units unspecified._\n",
    "  - **Range**: 0 to 1343 | **Mean**: 227.2 | **Median**: 180 \n",
    "\n",
    "\n",
    "- `Vertical_Distance_To_Hydrology`: _Vertical distance to nearest surface water features. Units unspecified._\n",
    "  - **Range**: -146 to 554 | **Mean**: 51.1 | **Median**: 32.0\n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Roadways`: _Horizontal distance to nearest roadway. Units unspecified._\n",
    "  - **Range**: 0 to 6890 | **Mean**: 1714.0 | **Median**: 1316\n",
    "\n",
    "\n",
    "- `Hillshade_9am`: _(0 to 255 index) - Hillshade index at 9am, summer solstice_\n",
    "  - **Range**: 0 to 254 | **Mean**: 212.7 | **Median**: 220\n",
    "\n",
    "\n",
    "- `Hillshade_Noon`: _(0 to 255 index) - Hillshade index at noon, summer solstice_\n",
    "  - **Range**: 99 to 254 | **Mean**: 219.0 | **Median**: 223\n",
    "\n",
    "\n",
    "- `Hillshade_3pm`: _(0 to 255 index) - Hillshade index at 3pm, summer solstice_\n",
    "  - **Range**: 0 to 248 | **Mean**: 135.1 | **Median**: 138.0\n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Fire_Points`: _Horizontal distance to nearest wildfire ignition points. Units unspecified._\n",
    "  - **Range**: 0 to 6993 | **Mean**: 1511.2 | **Median**: 1256 \n",
    "\n",
    "\n",
    "- `Wilderness_Area`: _(4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation_\n",
    "  - % of cases - **Area 1**: 24% || **Area 2**: 3% || **Area 3**: 42% || **Area 4**: 31% \n",
    "\n",
    "\n",
    "- `Soil_Type`: _(40 binary columns, 0 = absence or 1 = presence) - Soil type designation_\n",
    "  - The soil types descriptions can be found at the [Kaggle Competition Data Page](https://www.kaggle.com/c/forest-cover-type-prediction/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\" > </a>\n",
    "### Initial Exploration of the Challenge\n",
    "The categorical label we are trying to predict is contained in the `Cover_Type` variable, and is split up into 7  designations. While the tree species discussed in the Colorado State Forest Service's [_Colorado's Major Tree Species_](https://csfs.colostate.edu/colorado-trees/colorados-major-tree-species/) article do not map perfectly to these categories, the article provides some context that may prove useful.\n",
    "\n",
    "#### <span style='color:blue'>Category 1</span>: 'Spruce/Fir'\n",
    "- Species that might fit into this category include the **Blue Spruce** (which thrives at an altitude of 6700-11500 ft in sandy soils near moisture), the **Engelmann Spruce** (8000-11000 ft, moist north-facing slopes), the **Subalpine Fir** (8000-12000 ft, cold high-elevation forests), and the **White Fir** (7900-10200 ft, moist soils in valleys).\n",
    "\n",
    "<center>Blue Spruce</center> | <center>Engelmann Spruce</center> | <center>Subalpine Fir</center> | <center>White Fir</center>\n",
    "- | - | - | -\n",
    "<img src=\"imgs/1_blue-spruce-tree.jpg\" alt=\"BlueSpruce\" style=\"width: 250px;\"/>  | <img src=\"imgs/1_engelmann-spruce.jpg\" alt=\"EngelmannSpruce\" style=\"width: 250px;\"/> | <img src=\"imgs/1_subalpine-fir.jpg\" alt=\"SubalpineFir\" style=\"width: 250px;\"/> | <img src=\"imgs/1_white-fir-tree.jpg\" alt=\"WhiteFir\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 2</span>: 'Lodgepole Pine' and <span style='color:blue'>Category 3</span>: 'Ponderosa Pine'\n",
    "- The **Lodgepole Pine** thrives in well-drained soils at high elevations (6000-11000 ft).\n",
    "- The **Ponderosa Pine** thrives in dry, nutrient-poor soils at elevations of 6300-9500 ft. It is often found with Douglas Firs.\n",
    "\n",
    "<center>Lodgepole Pine</center> | <center>Ponderosa Pine</center> |\n",
    "- |-|\n",
    "<img src=\"imgs/2_lodgepole-pine.jpg\" alt=\"LodgepolePine\" style=\"width: 250px;\"/> | <img src=\"imgs/3_ponderosa-pine.jpg\" alt=\"PonderosaPine\" style=\"width: 250px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 4</span>: 'Cottonwood/Willow'\n",
    "- This category includes the **Plains Cottonwood** (which thrives at altitudes of 3500-6500 ft near sources of water), the **Narrowleaf Cottonwood** (5000-8000 ft, moist soils along streams), and the **Peachleaf Willow** (3500-7500 ft, near water sources).\n",
    "\n",
    "<center>Plains Cottonwood</center> | <center>Narrowleaf Cottonwood</center> | <center>Peachleaf Willow</center> |\n",
    "- |- |- |\n",
    "<img src=\"imgs/4_plains-cottonwood.jpg\" alt=\"PlainsCottonwood\" style=\"width: 250px;\"/> |<img src=\"imgs/4_narrowleaf-cottonwood.jpg\" alt=\"NarrowleafCottonwood\" style=\"width: 250px;\"/> |<img src=\"imgs/4_peachleaf-willow.jpg\" alt=\"PeachleafWillow\" style=\"width: 250px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 5</span>: 'Aspen' and <span style='color:blue'>Category 6</span>: 'Douglas Fir'\n",
    "- The **Quaking Aspen** thrives at altitudes of 6500-11500 ft. While it can be in many soil types, it is most often found on sandy and gravelly slopes.\n",
    "- The **Douglas Fir** thrives at altitudes of 6000-9500 ft and thrives in rocky soils and moist northern facing slopes.\n",
    "\n",
    "<center>Quaking Aspen</center> | <center>Douglas Fir</center> |\n",
    "- | - |\n",
    "<img src=\"imgs/5_aspen.jpg\" alt=\"QuakingAspen\" style=\"width: 250px;\"/> | <img src=\"imgs/6_douglas-fir.jpg\" alt=\"DouglasFir\" style=\"width: 250px;\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 7</span>: 'Krummholz'\n",
    "- Interestingly, _krummholz_ is not a species of tree; it is a type of tree formation (which can emerge across various tree species) that results from long-term wind exposure, often strong and cold winds. Per [Wikipedia](https://en.wikipedia.org/wiki/Krummholz), Subalpine Fir and Engelmann Spruce are often associated with Krummholz conditions (as is Lodgepole Pine, although that is more common in British Columbia). This category, while not specific to a species may be a better indication of a certain subset of environmental vulnerabilies than species alone.\n",
    "\n",
    "<center>Krummholz Banner Tree</center> | <center>Krummholz White Pine</center> | <center>Krummholz Bristlecone</center> \n",
    "- |- |- |\n",
    "<img src=\"imgs/7_krummholz-banner-tree.jpg\" alt=\"KrummholzBannerTree (Photo credit to John Spooner - flickr.com, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=5007578)\" style=\"width: 250px;\"/> | <img src=\"imgs/7_krummholz-white-pine.jpg\" alt=\"KrummholzWhitePine (Photo credit to Walter Siegmund [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)] https://commons.wikimedia.org/wiki/File:Pinus_albicaulis_7872.JPG\" style=\"width: 350px;\"/> |  <img src=\"imgs/7_krummholz-windswept-bristlecone.jpg\" alt=\"KrummholzBristlecone\" style=\"width: 400px;\"/> | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploratoryDataAnlysis\"> </a>\n",
    "### Where do we start?\n",
    "\n",
    "The brief descriptions we've seen already suggest some avenues of exploration: elevation and hydrology seem to be of primary importance.\n",
    "\n",
    "#### What can we learn from elevation alone?\n",
    "\n",
    "We will start by visualizing the elevation ranges across the various tree species mentioned above. We expect elevation to be important as it is the primary indicator of environmental conditions in mountainous regions. The graph below illustrates the ranges in which the species of trees discussed the Colorado State Forest Service's [_Colorado's Major Tree Species_](https://csfs.colostate.edu/colorado-trees/colorados-major-tree-species/) are most commonly found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T16:28:41.024009Z",
     "start_time": "2019-04-06T16:28:40.675304Z"
    }
   },
   "source": [
    "<img src=\"imgs/altitudeRanges4.png\" alt=\"ElevationRangesIdealized\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that lower elevations would be strongly suggestive of `Cottonwood/Willow`, while higher elevations might be more suggestive of the `Spruce/Fir`, `Lodgepole Pine`, `Aspen`, and `Krummholz`. The graph above is based upon data aggregated from outside sources. It's important that we verify this against our actual dataset, as it may indicate otherwise. The graphs below present the observed _elevation_ ranges and quartiles by `Cover_Type` in our data.\n",
    "\n",
    "| <center>Elevation Ranges</center> | <center>Elevation Quartiles</center>\n",
    "|-|-\n",
    "|<img src=\"imgs/elevationRanges.png\" alt=\"ElevationRanges\" style=\"width: 600px;\"/> |<img src=\"imgs/elevationQuartiles.png\" alt=\"ElevationQuartiles\" style=\"width: 600px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the ranges, our dataset appears to differ from our initial visualization in that the `Cottonwood/Willow` `Cover_Type` does not seem to occur at markedly lower elevations. However, when looking at the quartiles, patterns emerge that appear similar to what we would expect from our initial visualization: `Cottonwood/Willow` tends to cluster at lower elevations, with the higher elevations dominated by `Spruce/Fir` and `Krummholz` cover types.\n",
    "\n",
    "The separations are surprisingly clean, suggesting that `Elevation` will be a powerful feature in our models. It might be especially powerful if we could develop a method to cluster the altitudes based on interquartile ranges presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"> </a>\n",
    "#### What if we bring water into the picture?\n",
    "The other feature that the article suggests might be highly salient is moisture. How does the picture evolve if we add a measure of distance to water?\n",
    "\n",
    "The graph below is a scatterplot of the Euclidean distance (derived from the `Horizontal_Distance_To_Hydrology` and `Vertical_Distance_To_Hydrology` features) and the `Elevation`, with data points colored by the `Cover_Type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T17:10:56.612522Z",
     "start_time": "2019-04-07T17:10:56.592521Z"
    }
   },
   "source": [
    "<img src=\"imgs/hydrologyAndElevationScatter.png\" alt=\"HydrologyAndElevationScatter\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance to hydrology appears to be informative: `Cover_Type`s 3, 4, and 6 are essentially not found when the distance to water exceeds 750. That said, distance to hydrology is likely correlated with our primary distinguishing feature, `Elevation`.\n",
    "\n",
    "#### What if we consider exposure to sunlight and wind?\n",
    "Intuitively, the amount of sunlight to which a given plot of land is exposed would likely influence the predominant forest cover type. In our dataset, the `Hillshade` variables is likely to contain this information.\n",
    "\n",
    "The plot below compares the 1st quartile, median, and 3rd quartiles for each measure of `Hillshade` and each category of `Cover_Type`.\n",
    "\n",
    "<img src=\"imgs/hillshadeQuartiles.png\" alt=\"HillshadeQuartiles\" style=\"width: 600px;\"/>\n",
    "\n",
    "While the median `Hillshade` values appear to vary slightly across categories in the morning and afternoon, the interquartile range is overlapping across categories. The main takeaway from this is that `Hillshade` is unlikely to be a strong indicator of `Cover_Type` on its own.\n",
    "\n",
    "Exposure to sunlight and wind would also be affected by the `Aspect` variable, which can most easily be understood as the cardinal direction a given slope of land is facing (0$^\\circ$ is true North, 90$^\\circ$ is due East, 180$^\\circ$ is South, 270$^\\circ$ is due West). While the exact nature of the interaction between these features may not be clear *a priori*, we can attempt to collapse the effect into a single feature by taking the first principal component of the `Hillshade_9am` and `Hillshade_3pm` features with the `Aspect` feature.\n",
    "\n",
    "The graph below plots this first principal component against `Elevation`, since we already know `Elevation` is likely our most informative input variable.\n",
    "\n",
    "<img src=\"imgs/hillshadeAspectPcaScatter.png\" alt=\"hillshadeAspectPcaScatter\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns we see are weak at best. While the `Douglas Fir` category appears to be more prevalent for greater and lesser values of this first principal component, and the `Ponderosa Pine` appears to be slightly more prevalent nearer to zero, it is clear that the `Elevation` remains the dominant feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the 'Kitchen Sink' approach?\n",
    "\n",
    "So far we've examined `Elevation`, `Hydrology`, `Aspect`, and `Hillshade` features on the basis of the Colorado State Forestry write-up, but we should also take a more comprehensive look at all of our input variables and how they might be interacting with one another in ways not captured in our initial research.\n",
    "\n",
    "The graph below is a scatterplot matrix incorporating all of the raw simple features in our dataset, as well as the `Euclidean_Distance_To_Hydrology` feature we composed from the horizontal and vertical distances to hydrology.\n",
    "\n",
    "<img src=\"imgs/scatterplotMatrixElevationAspectWaterHillshade.png\" alt=\"scatterplotMatrixElevationAspectWaterHillshade\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `Elevation` remains the feature that provides the cleanest separation between `Cover_Type`s, two additional features (`Horizontal_Distance_To_Roadways` and `Horizontal_Distance_To_Fire_Points`) perform notably well when identifying the `Lodgepole Pine` specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkingLabelImbalance\"> </a>\n",
    "### Checking Label Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful for us to understand whether we have an imbalanced dataset (i.e., one where certain labels/categories are overrepresented relative to others.) Here we'll quickly describe our training and test labels and just make sure our classes are balanced. We can do this both graphically and numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T17:16:44.509699Z",
     "start_time": "2019-04-12T17:16:43.768988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3), sharey=True)\n",
    "bins = np.arange(8) + 0.5\n",
    "ax1.hist(y_train, bins, width = 0.8)\n",
    "ax1.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax1.set_title('Labels')\n",
    "ax2.hist(y_test, bins, width = 0.8)\n",
    "ax2.set_title('Test Labels')\n",
    "plt.xticks(range(8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily this data set is balanced equally between all of the different cover_types.  This will make building models much easier because no additional work will be required to correct for any imbalances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataCleaning\"> </a>\n",
    "### Cleaning the Data\n",
    "While exploring the data (see [Annex A](#annexA)), we noticed that `Soil_Type7` and `Soil_Type15` variables are always false. Because there is no information gained from these features, it contributes nothing to our dataset and will not have any impact on our models.\n",
    "\n",
    "Additionally, we noticed the `Hillshade_9am` and `Hillshade_3pm` features are missing several values.  We choose to replace these values with the median value for those features.  This will allow the areas with missing values to be more accurately classified as they no longer have un-usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T17:16:47.631899Z",
     "start_time": "2019-04-12T17:16:44.524196Z"
    }
   },
   "outputs": [],
   "source": [
    "engineered_features = pd.DataFrame.copy(full_features)\n",
    "\n",
    "# Removing uninformative features\n",
    "engineered_features = engineered_features.drop(['Soil_Type7', 'Soil_Type15'], axis=1)\n",
    "\n",
    "# Replacing Hillshade_9am and Hillshade_3pm 0 values with median\n",
    "median_hillshade_9am = np.median(engineered_features['Hillshade_9am'])\n",
    "engineered_features['Hillshade_9am'] = engineered_features.apply(lambda row: median_hillshade_9am if row.Hillshade_9am == 0 else row.Hillshade_9am, axis=1)\n",
    "median_hillshade_3pm = np.median(engineered_features['Hillshade_3pm'])\n",
    "engineered_features['Hillshade_3pm'] = engineered_features.apply(lambda row: median_hillshade_3pm if row.Hillshade_3pm == 0 else row.Hillshade_3pm, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"featureEngineering\"></a>\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful machine learning projects often depend heavily on feature engineering. The most important feature in a dataset may be a latent one - that is, 'hidden' behind other features which serves as proxies for it. In such a case, the latent feature needs to be explicitly extracted. While we are exploring the potential of various synthetic/constructed features, we will also try to remove original features which prove to be uninformative. Doing so will reduce the noise passed into our models and help to isolate only the most important features. We will keep our engineered and source datasets separate by creating a deep copy of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance to Hydrology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in our [Exploratory Data Analysis](#ref1), the `Cover_Type` can be visually broken up based on their distance to hydrology, both horizontally and vertically.  By combining these features into a single feature, we can reduce the overall number of features without much loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation of Hydrology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elevation and Hydrology are very important features when it comes to predicting the `Cover_Type` of an area.  By subtracting the vertical distance to hydrology from the elevation, we can identify the elevation of the hydrology itself.  This may prove useful by providing a feature that would be able to distinguish between an alpine lake and a valley stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Distance to Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in our [Exploratory Data Analysis](#aboutTheData), distance metrics group the data pretty well for classification.  With this we can engineer a new feature that incorporates the mean distance to hydrology, fire points, and roadways. While distance to hydrology was a strong indicator, the latter two features provide a fair approximation of the remoteness of the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set features 40 different types of soils.  When compared to the 7 possible labels, this number of soil types seems a bit extreme.  Different types of trees favor more rocky soils, and so combining all of the stony soil types into a single feature will allow a model to more easily pick up on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hillshade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>212.704299</td>\n",
       "      <td>135.091997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.561287</td>\n",
       "      <td>45.895189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>196.000000</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>220.000000</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>235.000000</td>\n",
       "      <td>167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>254.000000</td>\n",
       "      <td>248.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hillshade_9am  Hillshade_3pm\n",
       "count   15120.000000   15120.000000\n",
       "mean      212.704299     135.091997\n",
       "std        30.561287      45.895189\n",
       "min         0.000000       0.000000\n",
       "25%       196.000000     106.000000\n",
       "50%       220.000000     138.000000\n",
       "75%       235.000000     167.000000\n",
       "max       254.000000     248.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_features[['Hillshade_9am', 'Hillshade_3pm']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.249Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "engineered_features['Euclidean_Distance_To_Hydrology'] = engineered_features.apply(lambda row: math.sqrt(row.Horizontal_Distance_To_Hydrology**2 + row.Vertical_Distance_To_Hydrology**2), axis=1)\n",
    "engineered_features['Elevation_Of_Hydrology'] = engineered_features['Elevation']-engineered_features['Vertical_Distance_To_Hydrology']\n",
    "engineered_features['Mean_Distance_To_Feature'] = (engineered_features['Horizontal_Distance_To_Hydrology']+engineered_features['Horizontal_Distance_To_Roadways']+engineered_features['Horizontal_Distance_To_Fire_Points'])/3\n",
    "engineered_features['Stony'] = engineered_features[['Soil_Type1', 'Soil_Type2', 'Soil_Type6', 'Soil_Type9', 'Soil_Type12', 'Soil_Type18', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', ]].any(axis=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "e_X_train, e_X_test, e_y_train, e_y_test = train_test_split(engineered_features, full_labels, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Test Feature Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without _a priori_ knowledge of how the interplay between soil types, topography, hydrology, etc. affects forest cover, we need a way to view the performance of new features.  As such we will use a simple Gaussian Naive Bayes model to do predictions, and quanitify the results using cross-validation.  We will be tracking performance across precision, recall, and f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naïve Bayes\n",
    "\n",
    "One reasonable place to begin might be a Naïve Bayes classifier. While it is unlikely that all of the features at our disposal are _strictly_ independent, we may be able to relax the assumption of independence enough to explore how a NB model performs.\n",
    "\n",
    "We don't want a Bernoulli NB model as our features do not take on binary values exclusively. We also don't want a Multinomial NB model as it assumes integer feature counts. A Gaussian NB, on the other hand, might work well. While it assumes that the likelihoods of the features are Gaussian - and this is not necessarily strictly the case - it may be worth trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base Data GaussianNB, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.72 | recall:  0.50 | f1-score:  0.59 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.13 | recall:  0.73 | f1-score:  0.22 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.73 | recall:  0.43 | f1-score:  0.54 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.92 | recall:  0.68 | f1-score:  0.78 | \n",
      "\t\t\tAspen                | precision:  0.73 | recall:  0.59 | f1-score:  0.65 | \n",
      "\t\t\tDouglas Fir          | precision:  0.07 | recall:  0.77 | f1-score:  0.12 | \n",
      "\t\t\tKrummholz            | precision:  0.82 | recall:  0.86 | f1-score:  0.84 | \n",
      "\t\t\tmacro avg            | precision:  0.59 | recall:  0.65 | f1-score:  0.54 | \n",
      "\t\t\tmicro avg            | precision:  0.59 | recall:  0.59 | f1-score:  0.59 | \n",
      "\t\t\tweighted avg         | precision:  0.76 | recall:  0.59 | f1-score:  0.65 | \n",
      "\n",
      "Model: Base Data GaussianNB, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.73 | recall:  0.53 | f1-score:  0.61 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.16 | recall:  0.73 | f1-score:  0.26 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.73 | recall:  0.44 | f1-score:  0.55 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.93 | recall:  0.69 | f1-score:  0.79 | \n",
      "\t\t\tAspen                | precision:  0.79 | recall:  0.62 | f1-score:  0.69 | \n",
      "\t\t\tDouglas Fir          | precision:  0.08 | recall:  0.80 | f1-score:  0.14 | \n",
      "\t\t\tKrummholz            | precision:  0.86 | recall:  0.85 | f1-score:  0.85 | \n",
      "\t\t\tmacro avg            | precision:  0.61 | recall:  0.66 | f1-score:  0.56 | \n",
      "\t\t\tmicro avg            | precision:  0.61 | recall:  0.61 | f1-score:  0.61 | \n",
      "\t\t\tweighted avg         | precision:  0.77 | recall:  0.61 | f1-score:  0.66 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on the base data\n",
    "cross_validate_model(GaussianNB(), X_train, y_train, name='Base Data GaussianNB', verbose=True)\n",
    "\n",
    "# Testing on the engineered data\n",
    "cross_validate_model(GaussianNB(), e_X_train, e_y_train, name='Base Data GaussianNB', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we will use two metrics to determine how well a particular model performs, precision and recall.  All in all, just throwing a Gaussian Naive Bayes classifier at the data performed better than expected.  It achieved a 76% weighted precision across 5 fold cross validation.  \n",
    "\n",
    "The engineered features do not provide as much improvement as we had initially hoped.  They resulted in 1-2% improvement across all of the feature engineering mentioned above.  It may be worth noting that the engineered features improved classification of categories that had worse performance more than those that were already well classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed Engineered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every feature that is engineered is a useful addition to the data set.  Randomly adding new features can add unnecessary noise to the dataset without add any new information.  We have listed the failed features below.  Some highlights include mountain width and prominence (from the `Elevation` and `Slope` features), and a few different ways to view the elevation of an area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.397Z"
    }
   },
   "outputs": [],
   "source": [
    "# engineered_features['Elevation_Away_From_Hydrology'] = engineered_features['Elevation']-engineered_features['Horizontal_Distance_To_Hydrology']\n",
    "# engineered_features['Mountain_Width'] = engineered_features.apply(lambda row: row.Elevation/math.tan(math.radians(row.Slope+.1)), axis=1)\n",
    "# engineered_features['Mountain_Prominence'] = engineered_features.apply(lambda row: row.Elevation/math.sin(math.radians(row.Slope+.1)), axis=1)\n",
    "# engineered_features['Mean_Hillshade'] = engineered_features.apply(lambda row: (row.Hillshade_9am + row.Hillshade_Noon + row.Hillshade_3pm)/3, axis=1)\n",
    "# engineered_features['Morning_Hillshade'] = engineered_features.apply(lambda row: (row.Hillshade_9am * row.Hillshade_Noon), axis=1)\n",
    "# engineered_features['Norm_Horizontal_Distance_To_Hydrology'] = engineered_features['Horizontal_Distance_To_Hydrology']/(np.mean(engineered_features['Horizontal_Distance_To_Hydrology']))\n",
    "# engineered_features['Norm_Elevation'] = engineered_features['Elevation']/(np.mean(engineered_features['Elevation']))\n",
    "# engineered_features['Log_Elevation'] = engineered_features.apply(lambda row: math.log(row.Elevation), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is a very important aspect of preparing data for consumption by machine learning models.  It brings all of the features into a standardized range, preventing our models from haivng bias toward certain features on the basis of relative scale. By ensuring all features are kept within a standardized range, we eliminate the possibility that certain features could dominate the weighting and prediction process simply by virtue of being represented on a larger scale than that another potentially more meaningful feature.  We will experiment with several different types of standardization to see which is the most effective. Specifically we will test Min-Max scaling, standard scaling, robust scaling and sklearn's normalizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating standardization using K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with Naïve Bayes models is that they are more or less invariant to feature scaling, and therefore cannot be used when testing standardization methods.  For this reason, We will use the `KNearestClassifier` with *K = 3* when testing our performance on scaled data.  In testing a variety of values for `K`, we found that *K = 3* consistently produced the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.66 | recall:  0.74 | f1-score:  0.70 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.58 | recall:  0.71 | f1-score:  0.64 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.74 | recall:  0.83 | f1-score:  0.78 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.96 | recall:  0.90 | f1-score:  0.93 | \n",
      "\t\t\tAspen                | precision:  0.96 | recall:  0.83 | f1-score:  0.89 | \n",
      "\t\t\tDouglas Fir          | precision:  0.85 | recall:  0.80 | f1-score:  0.82 | \n",
      "\t\t\tKrummholz            | precision:  0.97 | recall:  0.88 | f1-score:  0.92 | \n",
      "\t\t\tmacro avg            | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "\t\t\tmicro avg            | precision:  0.82 | recall:  0.82 | f1-score:  0.82 | \n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on the unscaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KNearestClassifier` performed remarkably well on the base data with 84% precision and 82% recall.  This is a surprisingly good result for an out-of-the-box model.  However with such a low K, we would need to run this against a larger test set to ensure we aren't overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested standardization using each of the following methods:\n",
    "+ `MinMaxScaler` with ranges of [-1, 1] and [0, 1]\n",
    "+ `StandardScaler` with the range [0, 1]\n",
    "+ `RobustScaler`\n",
    "+ `Normalizer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.628Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "mm_neg1_1_scaled_df = apply_scaler(MinMaxScaler(feature_range=(-1, 1)), e_X_train)\n",
    "print(\"MinMaxScaler [-1,1]\")\n",
    "mm_neg1_1_scaled_df.head(5)\n",
    "\n",
    "mm_0_1_scaled_df = apply_scaler(MinMaxScaler(feature_range=(0, 1)), e_X_train)\n",
    "print(\"MinMaxScaler [0,1]\")\n",
    "mm_0_1_scaled_df.head(5)\n",
    "\n",
    "standard_scaled_df = apply_scaler(StandardScaler(), e_X_train)\n",
    "print(\"StandardScaler [0,1]\")\n",
    "standard_scaled_df.head(5)\n",
    "\n",
    "r_scaled_df = apply_scaler(RobustScaler(), e_X_train)\n",
    "print(\"RobustScaler [0,1]\")\n",
    "r_scaled_df.head(5)\n",
    "\n",
    "n_scaled_df = apply_scaler(Normalizer(), e_X_train)\n",
    "print(n_scaled_df.shape)\n",
    "n_scaled_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block above demonstrates the effect of applying each of these methods. (Removing the `%%capture` line will allow the output to be generated.) Output is currently omitted for simplicity sake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, MinMax scaled [-1,1]\n",
      "\t\t\tweighted avg         | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "Model: 3 Nearest Neighbors, MinMax scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "Model: 3 Nearest Neighbors, Standard scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.81 | recall:  0.80 | f1-score:  0.80 | \n",
      "Model: 3 Nearest Neighbors, Robust scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.81 | recall:  0.80 | f1-score:  0.80 | \n",
      "Model: 3 Nearest Neighbors, Normalized\n",
      "\t\t\tweighted avg         | precision:  0.73 | recall:  0.70 | f1-score:  0.71 | \n",
      "Model: 3 Nearest Neighbors\n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n"
     ]
    }
   ],
   "source": [
    "# Testing on the [-1,1] scaled data again for reference\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), mm_neg1_1_scaled_df, e_y_train, name='3 Nearest Neighbors, MinMax scaled [-1,1]')\n",
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), mm_0_1_scaled_df, e_y_train, name='3 Nearest Neighbors, MinMax scaled [0,1]')\n",
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), standard_scaled_df, e_y_train, name='3 Nearest Neighbors, Standard scaled [0,1]')\n",
    "# Testing on the Robust scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), r_scaled_df, e_y_train, name='3 Nearest Neighbors, Robust scaled [0,1]')\n",
    "# Testing on the Normalized data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), n_scaled_df, e_y_train, name='3 Nearest Neighbors, Normalized')\n",
    "# Testing on the unscaled data again for reference\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test is to determine how much the scaled features affect the KNN model, so we have included the previous basic KNearestNeighbors results for reference.  \n",
    "+ The range used when applying the `MinMaxScaler` does not materially affect the results.\n",
    "+ The `StandardScaler` actually performs slightly worse than the `MinMaxScaler` when used on a KNN-3 model.\n",
    "+ The `RobustScaler` performs approximately as well as the previous models.\n",
    "+ The `Normalizer` generated the worst results of all.\n",
    "\n",
    "The main takeaway is that no approach to standardization materially improved the performance of our KNN model. As a result we will not go into any more depth on standardization, and spend our time exploring alternative models instead in hopes of improving overall performance. Many of the sklearn models that we will deploy apply some built-in standardization before training. Since manually standardizing the features has not improved performance, the remainder of our exploration will primarily use unstandardized features, with a few exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistic\"> </a>\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will run a Logistic Regression as this is a fairly simple model, and for this reason has the unique characteristic of being interpretable (it is possible to extract the coefficients for individual features). Although this is a fairly straight forward model, when given enough data it can perform remarkably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.65 | recall:  0.62 | f1-score:  0.64 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.50 | recall:  0.58 | f1-score:  0.54 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.54 | recall:  0.59 | f1-score:  0.56 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.88 | recall:  0.79 | f1-score:  0.83 | \n",
      "\t\t\tAspen                | precision:  0.69 | recall:  0.62 | f1-score:  0.65 | \n",
      "\t\t\tDouglas Fir          | precision:  0.54 | recall:  0.56 | f1-score:  0.55 | \n",
      "\t\t\tKrummholz            | precision:  0.84 | recall:  0.87 | f1-score:  0.86 | \n",
      "\t\t\tmacro avg            | precision:  0.66 | recall:  0.66 | f1-score:  0.66 | \n",
      "\t\t\tmicro avg            | precision:  0.66 | recall:  0.66 | f1-score:  0.66 | \n",
      "\t\t\tweighted avg         | precision:  0.67 | recall:  0.66 | f1-score:  0.67 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate_model(LogisticRegression(), e_X_train, e_y_train, name='Logistic Regression', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic logistic regression gives us a baseline against which to compare other models. The basic one-vs-many `LogisticRegression` classifier achieved an average precision of 67%, meaning about 2/3 of its predictions were correct. None of the f1 scores were strong (above 0.90), but it did outperform our base Naïve Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"KNN\"> </a>\n",
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed K Nearest Neighbors above when testing various methods of standardization.  KNN is a good fit for this type of problem because there are a large number of examples relative to the number of classes.  This means that every new data point will have many 'neighbors' to choose from. We found that the ideal number of neighbors was three as there is enough data to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:57.894Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.66 | recall:  0.74 | f1-score:  0.70 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.58 | recall:  0.71 | f1-score:  0.64 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.74 | recall:  0.83 | f1-score:  0.78 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.96 | recall:  0.90 | f1-score:  0.93 | \n",
      "\t\t\tAspen                | precision:  0.96 | recall:  0.83 | f1-score:  0.89 | \n",
      "\t\t\tDouglas Fir          | precision:  0.85 | recall:  0.80 | f1-score:  0.82 | \n",
      "\t\t\tKrummholz            | precision:  0.97 | recall:  0.88 | f1-score:  0.92 | \n",
      "\t\t\tmacro avg            | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "\t\t\tmicro avg            | precision:  0.82 | recall:  0.82 | f1-score:  0.82 | \n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our KNN model is unique in that it perform relatively well across all labels.  The lowest precision score it achieves is 58% on `Lodgepole Pines`.  As we saw in the [About the Data](#aboutTheData) section, Lodgepole pine trees thrive in areas that can be covered by many types of trees.  This makes classifying them especially hard with KNN as the Lodgepole covered areas often have neighbors of other cover types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"svm\"> </a>\n",
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are a common tool in a data scientist's kit.  They generally perform well on datasets that are semi linearly separable, but they are typically slow to train.  We will take a look at the efficacy of SVMs on this data set, as it may give an indication as to whether or not our data is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic Linear Support Vector machine \n",
    "cross_validate_model(LinearSVC(), e_X_train, e_y_train, name='linearSVC', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard linearSVC produces very poor results.  It has extremely low recall, and predicts the `Aspen` and `Ponderosa Pine` categories very poorly.  We will take a look at how it does on scaled data, as SVC's generally require some standardization.  The recommended scaling is mean 0 var 1, but we will see how the existing ones do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic Linear Support Vector machine \n",
    "cross_validate_model(LinearSVC(), e_X_train, e_y_train, name='linearSVC, Unscaled')\n",
    "cross_validate_model(LinearSVC(), mm_neg1_1_scaled_df, e_y_train, name='linearSVC, MinMax scaled [-1,1]')\n",
    "scaled_X_train = scale(e_X_train)\n",
    "scaled_X_train_df = pd.DataFrame(data=scaled_X_train,    # values{'1': {'f1-score': [0.0,\n",
    "   0.4797238999137187,\n",
    "   0.20856610800744876,\n",
    "   0.1308411214953271,\n",
    "   0.0],\n",
    "  'precision': [0.0,\n",
    "   0.7220779220779221,\n",
    "   0.14545454545454545,\n",
    "   0.07272727272727272,\n",
    "   0.0],\n",
    "  'recall': [0.0,\n",
    "   0.35917312661498707,\n",
    "   0.3684210526315789,\n",
    "   0.6511627906976745,\n",
    "   0.0]},\n",
    " '2': {'f1-score': [0.35232668566001896,\n",
    "   0.025188916876574305,\n",
    "   0.0,\n",
    "   0.31393298059964725,\n",
    "   0.0],\n",
    "  'precision': [0.9537275064267352,\n",
    "   0.012853470437017995,\n",
    "   0.0,\n",
    "   0.22879177377892032,\n",
    "   0.0],\n",
    "  'recall': [0.2160745486313337, 0.625, 0.0, 0.5, 0.0]},\n",
    " '3': {'f1-score': [0.07263922518159807,\n",
    "   0.3103448275862069,\n",
    "   0.2828875045804324,\n",
    "   0.410377358490566,\n",
    "   0.0],\n",
    "  'precision': [0.038461538461538464,\n",
    "   0.3,\n",
    "   0.9897435897435898,\n",
    "   0.4461538461538462,\n",
    "   0.0],\n",
    "  'recall': [0.6521739130434783,\n",
    "   0.32142857142857145,\n",
    "   0.16502778965369816,\n",
    "   0.3799126637554585,\n",
    "   0.0]},\n",
    " '4': {'f1-score': [0.7808676307007787,\n",
    "   0.6100254885301614,\n",
    "   0.1635514018691589,\n",
    "   0.4725897920604915,\n",
    "   0.5795644891122278],\n",
    "  'precision': [0.9023136246786633,\n",
    "   0.922879177377892,\n",
    "   0.08997429305912596,\n",
    "   0.3213367609254499,\n",
    "   0.44473007712082263],\n",
    "  'recall': [0.6882352941176471,\n",
    "   0.45558375634517767,\n",
    "   0.8974358974358975,\n",
    "   0.8928571428571429,\n",
    "   0.8317307692307693]},\n",
    " '5': {'f1-score': [0.0, 0.2997858672376874, 0.0, 0.39447731755424054, 0.0],\n",
    "  'precision': [0.0, 0.1794871794871795, 0.0, 0.2564102564102564, 0.0],\n",
    "  'recall': [0.0, 0.9090909090909091, 0.0, 0.8547008547008547, 0.0]},\n",
    " '6': {'f1-score': [0.019950124688279305,\n",
    "   0.3638275499474238,\n",
    "   0.0,\n",
    "   0.38575268817204306,\n",
    "   0.25975820379965464],\n",
    "  'precision': [0.0103359173126615,\n",
    "   0.4470284237726098,\n",
    "   0.0,\n",
    "   0.7435233160621761,\n",
    "   0.9740932642487047],\n",
    "  'recall': [0.2857142857142857,\n",
    "   0.3067375886524823,\n",
    "   0.0,\n",
    "   0.2604355716878403,\n",
    "   0.14986050219210842]},\n",
    " '7': {'f1-score': [0.7206572769953051,\n",
    "   0.4452690166975881,\n",
    "   0.5473321858864026,\n",
    "   0.7013953488372093,\n",
    "   0.0],\n",
    "  'precision': [0.7831632653061225,\n",
    "   0.30612244897959184,\n",
    "   0.40561224489795916,\n",
    "   0.9617346938775511,\n",
    "   0.0],\n",
    "  'recall': [0.6673913043478261,\n",
    "   0.8163265306122449,\n",
    "   0.8412698412698413,\n",
    "   0.5519765739385066,\n",
    "   0.0]},\n",
    " 'macro avg': {'f1-score': [0.2780629918894258,\n",
    "   0.36202365239848,\n",
    "   0.17176245719192038,\n",
    "   0.4013380867442179,\n",
    "   0.11990324184455463],\n",
    "  'precision': [0.38400026459796016,\n",
    "   0.41292123173317336,\n",
    "   0.23296923902217434,\n",
    "   0.43295398856221035,\n",
    "   0.20268904876707533],\n",
    "  'recall': [0.3585127636935101,\n",
    "   0.5419057832491961,\n",
    "   0.3245935115701451,\n",
    "   0.5844350853767825,\n",
    "   0.14022732448898254]},\n",
    " 'micro avg': {'f1-score': [0.38472834067547723,\n",
    "   0.41219691403379866,\n",
    "   0.23373759647188533,\n",
    "   0.43366409408305767,\n",
    "   0.20183823529411765],\n",
    "  'precision': [0.38472834067547723,\n",
    "   0.41219691403379866,\n",
    "   0.23373759647188533,\n",
    "   0.43366409408305767,\n",
    "   0.20183823529411765],\n",
    "  'recall': [0.38472834067547723,\n",
    "   0.41219691403379866,\n",
    "   0.23373759647188533,\n",
    "   0.43366409408305767,\n",
    "   0.20183823529411765]},\n",
    " 'weighted avg': {'f1-score': [0.49069007425782046,\n",
    "   0.46249456556707264,\n",
    "   0.29518567050209144,\n",
    "   0.4652436499902374,\n",
    "   0.2839274805399547],\n",
    "  'precision': [0.9027219153409074,\n",
    "   0.6268789401373445,\n",
    "   0.8883826016296831,\n",
    "   0.6613029425750938,\n",
    "   0.932538182368063],\n",
    "  'recall': [0.38472834067547723,\n",
    "   0.41219691403379866,\n",
    "   0.23373759647188533,\n",
    "   0.43366409408305767,\n",
    "   0.20183823529411765]}}\n",
    "                         columns=e_X_train.columns)  # 1st row as the column names\n",
    "cross_validate_model(LinearSVC(), scaled_X_train_df, e_y_train, name='linearSVC, Scaled to mean=0, variance=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that using scaled features made `LinearSVC` recall significantly better at the expense of precision. It's worth noting that standardized features improved our SVC's f1-score by 27%. However, when comparing the overall performance against KNN, the improved performance is still well below other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rf\"> </a>\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests can be extremely effective on datasets with a myriad of features that each contain a little information.  The base data set contains 54 features, 44 of which are binary, making the generation of trees relatively painless. In addition, Random Forest models are fairly interpretable, as the most salient features can be extracted from the model to get a better understand of how each class is being predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.280Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_validate_model(RandomForestClassifier(), e_X_train, e_y_train, name='RandomForest', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base `RandomForestClassifier` performs quite well out of the box, achieving a respectable 84% across all of our metrics.  Lets quickly evaluate wether we can improve on this baseline using standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.313Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(RandomForestClassifier(n_estimators = 10), mm_0_1_scaled_df, e_y_train, name='RandomForest, MinMax scaled [0,1]')\n",
    "# Testing on the unscaled data again for reference\n",
    "cross_validate_model(RandomForestClassifier(n_estimators = 10), e_X_train, e_y_train, name='RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the scaled features did not have an effect on the performance of the model.  Random Forests are made up of trees that only deal with a few features at a time, and do not care if a feature goes from [0,1] or [0,10000].  The tree's decision boundaries are set based on whatever scale that particular feature is at. Since the baseline random forest generated strong results, we will continue to explore a few variations on the random forest, such as Gradient Boosting and Ensemble methods, to see if we can make any improvements to performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gradient\"> </a>\n",
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Decision Trees are a cousin to the Random Forests.  When a Random Forest is built, it builds many trees in parallel trying to maximize the information gain of each tree.  In Gradient Boosting, trees are made iteratively with each tree attempting to correct the errors of the previous one.  On the whole, Gradient Boosted Decision Trees tend to perform a little better than Random Forests, while maintaining similarly nice properties when it comes to interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.405Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_validate_model(XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4), e_X_train, e_y_train, name=f'Gradient Boosted Decision Trees (XGBoost)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting has proven to be the most effective model so far, achieving the very high score of 0.88 across the board.  This is a pretty great result because classifying new data using a `XGBClassifier` is extremely quick despite the long time it takes to train a new model. \n",
    "\n",
    "Since random forest models are intepretable, we can gaugue the relative importance of the input variables to get a better sense of which variables are most useful in predicting `Cover_Type` (See below).  As expected the `Elevation` of an area is the most important predictor of `Cover_Type`.  Following the elevation of an area, we see the distances to roadways, fire points, and water all making significant contributions to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Initial RF and identify most important features\n",
    "initialRF = RandomForestClassifier(n_estimators = 10)\n",
    "initialRF.fit(e_X_train, e_y_train)\n",
    "feat_importance(initialRF, e_X_train, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=ensemble> </a>\n",
    "### Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All machine learning models have their strengths and weaknesses. For example, most of the models we tested did well when classifying Cottonwood/Willow, but poorly for Lodgepole Pine data points. One way to help mitigate this problem is to combine several models into an ensemble model. An ensemble model allows multiple individual models to predict a data point, and then uses the results from all of them to decide what the true label is. We had three models that performed well during training; the Random Forest model, the K Nearest Neighbors model, and the Gradient Boosted Decision Trees (XGBoost) model. We chose to use 'soft' voting, which uses the sum of the predicted probabilities across all of the models to decide on a final lable. This type of voting is very good when the models that are used in the ensemble excel at predicting different types of labels. By combining the three models, we can expect to improve our performance.\n",
    "\n",
    "While identifying the best possible models for an ensemble, we also tested various additional models suchs as MLP Classifier, Ada Boost, Quadratic Discriminant Analysis and Gaussian Process Classifier using a gridseach for best possible hyper paramerts. We fount that none of the other model combinations could improve the KNN, Random Forest and XGBoost ensemble. The original code has been moved to helper functions to remove clutters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.510Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators = 10)\n",
    "clf4 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf5 = XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4)\n",
    "\n",
    "eClf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('Randomforest', clf2), \n",
    "        ('3 Nearest Neighbors', clf4), \n",
    "        ('XGBoost', clf5)\n",
    "    ], \n",
    "    voting='soft', \n",
    "    n_jobs=-1)\n",
    "\n",
    "cross_validate_model(eClf, e_X_train, e_y_train, name='ensemble', verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see the improvement that we expected by combining the models. A few items to note, it that the ensemble is almost perfect when predicting Cottonwood/Willow, and had a remarkable improvement on Lodgepole Pine over even the XGBoost model. In cases where a given model is unsure about a data point, none of the predicted probabilities are very high, the other models can out vote it if they are more sure of the result. Ensemble models have diminishing returns though, meaning that adding more and more models will provide less and less addition benefit. Using our top 3 models appears to be a sweet spot in the number and quality of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master Model Result List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.575Z"
    }
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record=False):\n",
    "    test_model(LogisticRegression(), e_X_train, e_y_train, e_X_test, e_y_test, name=\"LogisticRegression\")\n",
    "    \n",
    "    test_model(GaussianNB(), e_X_train, e_y_train, e_X_test, e_y_test, name=\"GaussianNB\")\n",
    "    \n",
    "    test_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, e_X_test, e_y_test, name=\"3 Nearest Neighbors\")\n",
    "    \n",
    "    test_model(LinearSVC(), e_X_train, e_y_train, e_X_test, e_y_test, name='linearSVC')\n",
    "    \n",
    "    test_model(RandomForestClassifier(n_estimators = 10), e_X_train, e_y_train, e_X_test, e_y_test, name='RandomForest')\n",
    "    \n",
    "    test_model(XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4), e_X_train, e_y_train, e_X_test, e_y_test, name=f'Gradient Boosted Decision Trees (XGBoost)')\n",
    "    \n",
    "    test_model(eClf, e_X_train, e_y_train, e_X_test, e_y_test, name=f'Ensemble Model')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "    \n",
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to work on this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final result, reached a precision, recall and F-score of 89%. When we see the results in a confusion matrix, we can see that we get a Jackard similarity score of .89 with a hamming loss of .11. We also notice that model has difficulty in classifying Spure/Fir and mixing it with Lodgepole Pine and vice-versa. Our model is also misclassifying Doglas Fir with Ponderosa Pine. We need to apply further data engineering to improve accuracy of the models. We can also try to find models that can classify these covers better and try to add them in the ensemble to improve our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.794Z"
    }
   },
   "outputs": [],
   "source": [
    "eClf.fit(e_X_train, e_y_train)\n",
    "pred_y = eClf.predict(e_X_test)\n",
    "get_confusion_matrix(pred_y, e_y_test)\n",
    "print(\"Jaccard similarity score: %0.2f with hamming loss of %0.2f\"%(jaccard_similarity_score(e_y_test,pred_y),hamming_loss(e_y_test,pred_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Important Features \n",
    "The story that came out of the data was, in many ways, were quire counter intuitive. At the end of the analysis, the key components.\n",
    "    1. Elevation (0.162398)\n",
    "    2. Elevation_Of_Hydrology (0.151600)\n",
    "    3. Mean_Distance_To_Feature (0.070873)\n",
    "    4. Horizontal_Distance_To_Roadways (0.070431)\n",
    "    5. Euclidean_Distance_To_Hydrology (0.057154)\n",
    "    6. Horizontal_Distance_To_Fire_Points (0.051782)\n",
    "    7. Hillshade_Noon (0.036232)\n",
    "    8. Aspect (0.035743)\n",
    "    9. Hillshade_9am (0.035738)\n",
    "    10. Vertical_Distance_To_Hydrology (0.035153)\n",
    "We were surpised that all the details of the soil type, at the end, did not really matter for this dataset. It is true in this case may not be universally true.\n",
    "\n",
    "#### What can be done to improve further \n",
    "    1. More Data Engineering\n",
    "    Due to the time restrictions and large number of variables, we were not able to extract all possibly new features that could improve the models further.\n",
    "    2. Mode model and parameter testing in ensemble\n",
    "    The processing power needed to run complex combination of models, parameters take very long time to proces. and hence we could not test all possible combinations.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annexes\"></a>\n",
    "    \n",
    "# Annexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annexA\"></a>\n",
    "\n",
    "## Annex A: Exploratory Data Analysis\n",
    "\n",
    "This appendix contains some of our exploratory data analysis. This includes the code used to generate the 4-number summaries of our data reflected in the [_About the Data_](#aboutTheData) and other summaries. The most informative portions are replicated in the main body of the report.\n",
    "\n",
    "After we load the data from the source file, we examine the basic characteristics of the dataset.\n",
    "  1. We expect to see all of the features discussed above represented in our column names\n",
    "  1. As there is no separate dataset containing the labels for our observations, we would expect to see the 'Cover_Type' variable in our data\n",
    "  1. We would expect to see a shape of (15120, 55) - the 54 features plus our label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:58.997Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Columns: {full_data.columns}')\n",
    "print(f'Shape: {full_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the first several observations to get a sense for the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:59.069Z"
    }
   },
   "outputs": [],
   "source": [
    "full_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to get a high-level summary of each of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:59.127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Small function to give us a bird's-eye summary of the data\n",
    "def five_num_summary(df, column):\n",
    "    print(f'Column: {column:<35} | ' +\n",
    "          f'Max value: {np.max(df[column]):>6} | ' + \n",
    "          f'Min value: {np.min(df[column]):>7.2f} | ' +\n",
    "          f'Mean: {np.mean(df[column]):>7.2f} | ' +\n",
    "          f'Median: {np.median(df[column]):>7.2f}')\n",
    "\n",
    "for col_name in full_features.columns:\n",
    "    five_num_summary(full_features, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:59.137Z"
    }
   },
   "outputs": [],
   "source": [
    "print(stats.describe(full_labels))\n",
    "print(stats.describe(y_test))\n",
    "for i in range(0, 8):\n",
    "        print(f'i = {i}: Train Ct: {(full_labels==i).sum():>5} | Test Ct: {(y_test==i).sum():>5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our classes are quite well-balanced in both our training data and the test data.\n",
    "\n",
    "This is good both because we will not need to deliberately compensate for imbalances and because our model will be unable to achieve reasonable performance simply by guessing the modal category. (Doing so would give accuracy on the training set of 1741/12096 = 0.145, and then accuracy on the test set of 411/3024 = 0.136.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing of note is that the `Soil_Type7` and `Soil_Type15` are never true, so this feature tells us nothing.  These features should be removed before any modeling is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:59.275Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = np.arange(0, 360, 10)\n",
    "cut = [0, 45, 90, 135, 180, 225, 270, 315, 360]\n",
    "\n",
    "print(bins)\n",
    "pd.cut(bins, cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:59.287Z"
    }
   },
   "outputs": [],
   "source": [
    "full_data['Total_Hillshade'] = full_data[['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']].sum(axis=1)\n",
    "full_data[['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Total_Hillshade']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-12T17:11:59.313Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make 3D scatterplot to explore water, elevation, and hillshade concurrently\n",
    "\n",
    "%matplotlib qt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sparsifier = np.random.randint(0, full_features.shape[0], 5000)\n",
    "\n",
    "sparsified = full_features.iloc[sparsifier,:]\n",
    "sparse_labels = full_labels.iloc[sparsifier]\n",
    "# print(f'Length of sparsified dataset\\n: {sparsified}')\n",
    "\n",
    "full_features['Euclidean_Distance_To_Hydrology'] = np.sqrt(full_features['Horizontal_Distance_To_Hydrology']**2 + full_features['Vertical_Distance_To_Hydrology']**2)\n",
    "dist_to_water = sparsified['Euclidean_Distance_To_Hydrology']\n",
    "altitude = sparsified['Elevation']\n",
    "hillshade = sparsified['Hillshade_3pm']\n",
    "color_dict = {1: '#A7C6ED', 2: '#BA0C2F', 3: '#651D32', 4: '#8C8985',\n",
    "              5: '#212721', 6: '#002F6C', 7: '#FFC000'}\n",
    "coloration = [color_dict[x] for x in sparse_labels]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(hillshade, dist_to_water, altitude, color=coloration, alpha=0.6)\n",
    "# ax.title('Forest cover categorization\\nby distance to water and hillshade')\n",
    "ax.view_init(30, 115)\n",
    "# mouse_init(rotate_btn=1, zoom_btn=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
