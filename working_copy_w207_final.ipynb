{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Spring 2019 Final Project\n",
    "## Kaggle Competition: Forest Cover Prediction\n",
    "**Pierce Coggins, Jake Mitchell, Debasish Mukhopadhyay, and Tim Slade**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents/Section Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction](#introduction)\n",
    "  - In which we discuss the problem and why it matters\n",
    "  - [Housekeeping](#housekeeping)\n",
    "    - In which we deal with basic prep and setup issues\n",
    "- [About the Data](#aboutTheData)\n",
    "  - EDA, charts, data cleaning\n",
    "- [Feature Engineering](#featureEngineering)\n",
    "  - Describe a basic model that we will use to test the usefulness of new features (LR or NB)\n",
    "  - Normalization\n",
    "  - Each added or removed feature\n",
    "- [Models](#models)\n",
    "  - Maybe choose 4 to test out?  Don't want this section to get too lengthy, and each model should be covered in some detail\n",
    "- [Results](#results)\n",
    "  - What went well, what went poorly\n",
    "  - Final comparison of models on test data\n",
    "- [Conclusion](#conclusion)\n",
    "- [Annexes](#annexA)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, we will attempt to predict the forest cover type (defined as the predominant type of tree cover) for a given area of land in Colorado given only cartographic variables as inputs.  This problem and dataset were initially posted as a Kaggle competition in 2015.  We have chosen to tackle this problem as it allows for many different machine learning techniques to be attempted and explored.  The report will go through the process of building a capable model from data cleaning through final testing.\n",
    "\n",
    "The problem of understanding what type of vegetation is present in a difficult to access area is a surprisingly important one.  In this particular example the forests of Colorado are very diverse, and each type of tree cover has its own benefits and dangers.  For example many of the pine trees in Colorado are susceptible to the [mountain pine beetle](https://csfs.colostate.edu/forest-management/common-forest-insects-diseases/mountain-pine-beetle/), while the Spruce and Fir trees are relatively safe from the beetles.  Without directly going to every location in the mountains of Colorado, it is very difficult to distinguish these types of trees as they look very similar from the air.  It is relatively easy to get cartographic data for a large swath of the mountains however, and if it is possible to accurately predict the tree type from the cartographic information alone then all of the Colorado forest could be mapped by likely forest cover type. That information would be invaluable to firefighters and forest service personnel to direct their efforts where it will have the most impact.\n",
    "\n",
    "If you would like to learn more about the problem or try for yourself, all information and data can be found from the kaggle competition:<br>[Kaggle's Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"housekeeping\"></a>\n",
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries, Helper Functions, and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:31:31.906270Z",
     "start_time": "2019-04-07T12:31:15.497727Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %matplotlib inline\n",
    "# %matplotlib notebook\n",
    "%matplotlib qt\n",
    "\n",
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "# Plotting and printing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "from matplotlib.image import imread\n",
    "import pprint\n",
    "\n",
    "# Model-building libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler, StandardScaler, RobustScaler, Normalizer, scale\n",
    "\n",
    "# SK-learn libraries for learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Run the helper functions notebook\n",
    "%run w207_final_helper_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forest cover types we aim to predict are bundled with the features used to predict them. Our first step is therefore to separate them out, lest we accidentally let our models peek at the outcomes. We also want to split the dataset into _train_ and _test_ subsets; this will give us insight into how well our chosen models and parameters will perform against out-of-sample data.\n",
    "\n",
    "The original dataset contained 15,120 observations. We will train our models on 90% of the data and hold out 10% for testing. We thus expect to have approximately 0.9 * 15,120 = 13,608 observations in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                 | Observations |  Features  |\n",
      "----------------------------------------------\n",
      "Training dataset |    13608     |     54     |\n",
      "Training labels  |    13608     |     --     |\n",
      "  Test dataset   |     1512     |     54     |\n",
      "  Test labels    |     1512     |     --     |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "full_data = pd.DataFrame.from_csv('./train.csv')\n",
    "full_data.shape\n",
    "\n",
    "# Separating out the labels\n",
    "full_labels = full_data['Cover_Type']\n",
    "full_features = full_data.drop('Cover_Type', axis=1)\n",
    "\n",
    "# Setting seed so we get consistent results from our splitting\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(full_features, full_labels, test_size=0.10)\n",
    "\n",
    "# Verifying our data shapes are as expected\n",
    "print(f'''\n",
    "{'':^16} | {'Observations':^12} | {'Features':^10} |\n",
    "{'-'*46}\n",
    "{'Training dataset':^16} | {X_train.shape[0]:^12} | {X_train.shape[1]:^10} |\n",
    "{'Training labels':^16} | {y_train.shape[0]:^12} | {'--':^10} |\n",
    "{'Test dataset':^16} | {X_test.shape[0]:^12} | {X_test.shape[1]:^10} |\n",
    "{'Test labels':^16} | {y_test.shape[0]:^12} | {'--':^10} |\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aboutTheData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from several wilderness areas in northern Colorado, specifically the Rawah Wilderness Area, Neota Wilderness Area, Comanche Peak Wilderness Area, and the Cache la Poudre Wilderness Area.  These are all fairly remote areas of Colorado which is why they were chosen, as there is less human influence in these places.\n",
    "\n",
    "The features in the dataset are all cartographic measures of a 30x30m square plot of land.  We have 10 simple features. The 11th and 12th - `wilderness_area` and `soil_type` - are categorical variables which are represented as 4 and 40 binary columns respectively in our dataset. We therefore have a total of 10 + 4 + 40 = 54 features to work with.\n",
    "The list below contains a short description of each feature, including where relevant its range, median, and mean. (See [Annex A](#annexA) for the associated code and further discussion of the exploratory data analysis).\n",
    "\n",
    "- `Elevation`: _Elevation in meters_\n",
    "  - **Range**: 1863 to 3849 | **Mean**: 2749.3 | **Median**: 2752\n",
    "\n",
    "\n",
    "- `Aspect`: _Aspect in degrees azimuth. i.e., degrees clockwise from a line pointed at true North. So North = 0$^\\circ$, East = 90$^\\circ$, South = 180$^\\circ$, and West = 270$^\\circ$_\n",
    "  - **Range**: 0 to 360 | **Mean**: 156.7 | **Median**: 126.0\n",
    "\n",
    "\n",
    "- `Slope`: _Slope in degrees. 0$^\\circ$ would indicate a flat plane; greater values represent steeper slopes._\n",
    "  - **Range**: 0 to 52 | **Mean**: 16.5 | **Median**: 15.0 \n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Hydrology`: _Horizontal distance to nearest surface water features. Units unspecified._\n",
    "  - **Range**: 0 to 1343 | **Mean**: 227.2 | **Median**: 180 \n",
    "\n",
    "\n",
    "- `Vertical_Distance_To_Hydrology`: _Vertical distance to nearest surface water features. Units unspecified._\n",
    "  - **Range**: -146 to 554 | **Mean**: 51.1 | **Median**: 32.0\n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Roadways`: _Horizontal distance to nearest roadway. Units unspecified._\n",
    "  - **Range**: 0 to 6890 | **Mean**: 1714.0 | **Median**: 1316\n",
    "\n",
    "\n",
    "- `Hillshade_9am`: _(0 to 255 index) - Hillshade index at 9am, summer solstice_\n",
    "  - **Range**: 0 to 254 | **Mean**: 212.7 | **Median**: 220\n",
    "\n",
    "\n",
    "- `Hillshade_Noon`: _(0 to 255 index) - Hillshade index at noon, summer solstice_\n",
    "  - **Range**: 99 to 254 | **Mean**: 219.0 | **Median**: 223\n",
    "\n",
    "\n",
    "- `Hillshade_3pm`: _(0 to 255 index) - Hillshade index at 3pm, summer solstice_\n",
    "  - **Range**: 0 to 248 | **Mean**: 135.1 | **Median**: 138.0\n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Fire_Points`: _Horizontal distance to nearest wildfire ignition points. Units unspecified._\n",
    "  - **Range**: 0 to 6993 | **Mean**: 1511.2 | **Median**: 1256 \n",
    "\n",
    "\n",
    "- `Wilderness_Area`: _(4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation_\n",
    "  - % of cases - **Area 1**: 24% || **Area 2**: 3% || **Area 3**: 42% || **Area 4**: 31% \n",
    "\n",
    "\n",
    "- `Soil_Type`: _(40 binary columns, 0 = absence or 1 = presence) - Soil type designation_\n",
    "  - The soil types descriptions can be found at the [Kaggle Competition Data Page](https://www.kaggle.com/c/forest-cover-type-prediction/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Exploration of the Challenge\n",
    "The label indicating our data's categorization is contained in the`Cover_Type` variable, and is split up into 7 different designations. While the tree species discussed in the Colorado State Forest Service's [_Colorado's Major Tree Species_](https://csfs.colostate.edu/colorado-trees/colorados-major-tree-species/) article do not map perfectly to these categories, the article provides some insights that may prove useful in our categorization exercise.\n",
    "\n",
    "#### <span style='color:blue'>Category 1</span>: 'Spruce/Fir'\n",
    "- Species that might fit into this category include the **Blue Spruce** (which thrives at an altitude of 6700-11500 ft in sandy soils near moisture), the **Engelmann Spruce** (8000-11000 ft, moist north-facing slopes), the **Subalpine Fir** (8000-12000 ft, cold high-elevation forests), and the **White Fir** (7900-10200 ft, moist soils in valleys).\n",
    "\n",
    "<center>Blue Spruce</center> | <center>Engelmann Spruce</center> | <center>Subalpine Fir</center> | <center>White Fir</center>\n",
    "- | - | - | -\n",
    "<img src=\"imgs/1_blue-spruce-tree.jpg\" alt=\"BlueSpruce\" style=\"width: 250px;\"/>  | <img src=\"imgs/1_engelmann-spruce.jpg\" alt=\"EngelmannSpruce\" style=\"width: 250px;\"/> | <img src=\"imgs/1_subalpine-fir.jpg\" alt=\"SubalpineFir\" style=\"width: 250px;\"/> | <img src=\"imgs/1_white-fir-tree.jpg\" alt=\"WhiteFir\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 2</span>: 'Lodgepole Pine' and <span style='color:blue'>Category 3</span>: 'Ponderosa Pine'\n",
    "- The **Lodgepole Pine** thrives in well-drained soils at high elevations (6000-11000 ft).\n",
    "- The **Ponderosa Pine** thrives in dry, nutrient-poor soils at elevations of 6300-9500 ft. It is often found with Douglas Firs.\n",
    "\n",
    "<center>Lodgepole Pine</center> | <center>Ponderosa Pine</center> |\n",
    "- |-|\n",
    "<img src=\"imgs/2_lodgepole-pine.jpg\" alt=\"LodgepolePine\" style=\"width: 250px;\"/> | <img src=\"imgs/3_ponderosa-pine.jpg\" alt=\"PonderosaPine\" style=\"width: 250px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 4</span>: 'Cottonwood/Willow'\n",
    "- Species that might fit into this category include the **Plains Cottonwood** (which thrives at altitudes of 3500-6500 ft near sources of water), the **Narrowleaf Cottonwood** (5000-8000 ft, moist soils along streams), and the **Peachleaf Willow** (3500-7500 ft, near water sources).\n",
    "\n",
    "<center>Plains Cottonwood</center> | <center>Narrowleaf Cottonwood</center> | <center>Peachleaf Willow</center> |\n",
    "- |- |- |\n",
    "<img src=\"imgs/4_plains-cottonwood.jpg\" alt=\"PlainsCottonwood\" style=\"width: 250px;\"/> |<img src=\"imgs/4_narrowleaf-cottonwood.jpg\" alt=\"NarrowleafCottonwood\" style=\"width: 250px;\"/> |<img src=\"imgs/4_peachleaf-willow.jpg\" alt=\"PeachleafWillow\" style=\"width: 250px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 5</span>: 'Aspen' and <span style='color:blue'>Category 6</span>: 'Douglas Fir'\n",
    "- The **Quaking Aspen** thrives at altitudes of 6500-11500 ft. While it can be in many soil types, it is especially found on sandy and gravelly slopes.\n",
    "- The **Douglas Fir** thrives at altitudes of 6000-9500 ft in rocky soils of moist northern slopes.\n",
    "\n",
    "<center>Quaking Aspen</center> | <center>Douglas Fir</center> |\n",
    "- | - |\n",
    "<img src=\"imgs/5_aspen.jpg\" alt=\"QuakingAspen\" style=\"width: 250px;\"/> | <img src=\"imgs/6_douglas-fir.jpg\" alt=\"DouglasFir\" style=\"width: 250px;\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 7</span>: 'Krummholz'\n",
    "- Interestingly, _krummholz_ is not a species of tree; it is a type of tree formation (which can emerge among various tree species) that results from consistent long-term exposure to strong, cold winds. Per [Wikipedia](https://en.wikipedia.org/wiki/Krummholz), Subalpine Fir and Engelmann Spruce are often associated with Krummholz conditions (as is Lodgepole Pine, although that is more common in British Columbia).\n",
    "\n",
    "<center>Krummholz Banner Tree</center> | <center>Krummholz White Pine</center> | <center>Krummholz Bristlecone</center> \n",
    "- |- |- |\n",
    "<img src=\"imgs/7_krummholz-banner-tree.jpg\" alt=\"KrummholzBannerTree (Photo credit to John Spooner - flickr.com, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=5007578)\" style=\"width: 250px;\"/> | <img src=\"imgs/7_krummholz-white-pine.jpg\" alt=\"KrummholzWhitePine (Photo credit to Walter Siegmund [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)] https://commons.wikimedia.org/wiki/File:Pinus_albicaulis_7872.JPG\" style=\"width: 350px;\"/> |  <img src=\"imgs/7_krummholz-windswept-bristlecone.jpg\" alt=\"KrummholzBristlecone\" style=\"width: 400px;\"/> | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do we start?\n",
    "\n",
    "The brief descriptions we've seen already suggest some avenues of exploration: altitude ranges and access to water seem to be of primary importance.\n",
    "\n",
    "#### What can we learn from elevation alone?\n",
    "\n",
    "One place to begin would be to plot out the idealized elevation ranges within which the various tree species thrive. There may be certain elevations where certain tree species would be far more prevalent than others. The graph below illustrates the ranges in which the species of trees discussed the Colorado State Forest Service's [_Colorado's Major Tree Species_](https://csfs.colostate.edu/colorado-trees/colorados-major-tree-species/) thrive, per the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T16:28:41.024009Z",
     "start_time": "2019-04-06T16:28:40.675304Z"
    }
   },
   "source": [
    "<img src=\"imgs/altitudeRanges4.png\" alt=\"ElevationRangesIdealized\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that lower elevations would be strongly suggestive of the `Cottonwood/Willow` `Cover_Type`, while higher elevations might be more suggestive of the `Spruce/Fir`, `Lodgepole Pine`, `Aspen`, and `Krummholz` `Cover_Type`s. The graph above is based upon idealized data from outside sources, though, and our actual dataset might tell a different story. The graphs below present the observed _elevation_ ranges and quartiles by `Cover_Type` in our data.\n",
    "\n",
    "| <center>Elevation Ranges</center> | <center>Elevation Quartiles</center>\n",
    "|-|-\n",
    "|<img src=\"imgs/elevationRanges.png\" alt=\"ElevationRanges\" style=\"width: 600px;\"/> |<img src=\"imgs/elevationQuartiles.png\" alt=\"ElevationQuartiles\" style=\"width: 600px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the ranges, our dataset appears to differ from the idealized one in that the `Cottonwood/Willow` `Cover_Type` does not seem to occur at markedly lower elevations. When looking at the quartiles, though, patterns emerge that appear similar to what we would expect from the idealized presentation: `Cottonwood/Willow` tends to cluster at lower elevations, with the higher elevations dominated by `Spruce/Fir` and `Krummholz` cover types.\n",
    "\n",
    "The separations are surprisingly clean, suggesting that `Elevation` will be a powerful feature in our models. It might be especially powerful if we could develop a method to cluster the altitudes into the interquartile ranges presented in the model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we bring water into the picture?\n",
    "The other feature that the article suggests might be highly salient is moisture. How does the picture evolve if we add a measure of the distance to water to the mix?\n",
    "\n",
    "The graph below is a scatterplot of the Euclidean distance (derived from the `Horizontal_Distance_To_Hydrology` and `Vertical_Distance_To_Hydrology` features) and the `Elevation`, with data points colored by the `Cover_Type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T17:10:56.612522Z",
     "start_time": "2019-04-07T17:10:56.592521Z"
    }
   },
   "source": [
    "<img src=\"imgs/hydrologyAndElevationScatter.png\" alt=\"HydrologyAndElevationScatter\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance to hydrology appears to be informative: `Cover_Type`s 3, 4, and 6 are essentially not found when the distance to water exceeds 750. That said, it remains clear that `Elevation` is the predominant distinguishing feature.\n",
    "\n",
    "#### What if we consider exposure to sunlight and wind?\n",
    "From a layperson's perspective, the amount of sunlight to which a given plot of land is exposed would seem likely to influence the vegetation which thrives there. In our dataset, the `Hillshade` variables encode this information.\n",
    "\n",
    "The plot below compares the 1st quartile, median, and 3rd quartiles for each measure of `Hillshade` for each category of `Cover_Type`.\n",
    "\n",
    "<img src=\"imgs/hillshadeQuartiles.png\" alt=\"HillshadeQuartiles\" style=\"width: 600px;\"/>\n",
    "\n",
    "While the median `Hillshade` values appear to vary a little across categories in the morning and afternoon, the interquartile range is largely overlaps across categories. The overall impression is that `Hillshade` is unlikely to be determinative on its own.\n",
    "\n",
    "Exposure to sunlight and wind would also be affected by the `Aspect`, which is essentially the compass direction (0$^\\circ$ is true North, 90$^\\circ$ is East, 180$^\\circ$ is South, 270$^\\circ$ is West) the plot is facing. While the exact nature of the interaction between these features may not be clear *a priori*, we can attempt to collapse the effect into a single feature by taking the first principal component of the morning and afternoon hillshade features with the `Aspect` feature.\n",
    "\n",
    "The graph below plots this first principal component against `Elevation`, as we already know `Elevation` is a strongly informative feature.\n",
    "\n",
    "<img src=\"imgs/hillshadeAspectPcaScatter.png\" alt=\"hillshadeAspectPcaScatter\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns we see are weak at best. While the `Douglas Fir` category appears to be more prevalent at the margins of this first PC and the `Ponderosa Pine` appears to be slightly more prevalent nearer to zero, it is clear that the `Elevation` remains the dominant feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the 'Kitchen Sink' approach?\n",
    "\n",
    "So far we've examined `Elevation`, `Hydrology`, `Aspect`, and `Hillshade` features on the basis of the write-ups regarding the various tree species. But what if we just took a look at all of our key features and how they relate to one another?\n",
    "\n",
    "The graph below is a scatterplot matrix incorporating all of the raw simple features in our data, as well as the `Euclidean_Distance_To_Hydrology` feature we composed from the horizontal and vertical distances to hydrology.\n",
    "\n",
    "<img src=\"imgs/scatterplotMatrixElevationAspectWaterHillshade.png\" alt=\"scatterplotMatrixElevationAspectWaterHillshade\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `Elevation` remains the feature that seems to provide the cleanest separation between `Cover_Type`s, two additional features seem to perform pretty well at discriminating the `Lodgepole Pine`s: `Horizontal_Distance_To_Roadways` and `Horizontal_Distance_To_Fire_Points`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "While exploring the data (see [Annex A](#annexA)), we noted that the `Soil_Type7` and `Soil_Type15` variables are never true. Because there is no variation in this feature, it contributes nothing to any of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:45:02.019656Z",
     "start_time": "2019-04-07T08:44:58.123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing uninformative features\n",
    "full_features = full_features.drop(['Soil_Type7', 'Soil_Type15'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"featureEngineering\"></a>\n",
    "\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is a major part of any data science project.  Often times the most important feature in a data set is hidden behind several other features and needs to manually be pulled out.  We will also try to remove unimportant features to reduce the noise that is passed into our models.  In order to keep the engineered and base data sets separate we must first create a new copy of the data that we are free to manipulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euchlidean Distance to Hydrology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the About the Data section, the `cover_types` can be visually broken up based on their distance to hydrology, both horizontally and vertically.  By combining the features into a single feature, we can reduce the overall number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation of Hydrology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elevation and Hydrology are very important features when it comes to predicting the cover type of an area.  By subtracting the vertical distance to hydrology from the elevation, we can find what the elevation of the hydrology itself it.  This may prove useful by providing a feature that would be able to discern an alpine lake vs a valley stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Distance to Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the About the Data section, the distance metrics group the data pretty well for classification.  Engineering a new feature that is the mean distance to hydrology, fire points, and roadways gives a decent approximation for how remote an area is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set features 40 different types of soils.  When compared to the 7 possible labels, this number of soil types seems a bit extreme.  Different types of trees favor more rocky soils, and so combining all of the stony soil types into a single feature will allow a model to more easily pick up on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hillshade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>212.704299</td>\n",
       "      <td>135.091997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.561287</td>\n",
       "      <td>45.895189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>196.000000</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>220.000000</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>235.000000</td>\n",
       "      <td>167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>254.000000</td>\n",
       "      <td>248.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hillshade_9am  Hillshade_3pm\n",
       "count   15120.000000   15120.000000\n",
       "mean      212.704299     135.091997\n",
       "std        30.561287      45.895189\n",
       "min         0.000000       0.000000\n",
       "25%       196.000000     106.000000\n",
       "50%       220.000000     138.000000\n",
       "75%       235.000000     167.000000\n",
       "max       254.000000     248.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_features[['Hillshade_9am', 'Hillshade_3pm']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice about the data is that the `Hillshade_9am` and `Hillshade_3pm` features are missing several values.  We choose to replace these values with the median value for those features.  This will allow the areas with missing values to be more accurately classified as they no longer have un-usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "engineered_features = pd.DataFrame.copy(full_features)\n",
    "engineered_features['Euclidean_Distance_To_Hydrology'] = engineered_features.apply(lambda row: math.sqrt(row.Horizontal_Distance_To_Hydrology**2 + row.Vertical_Distance_To_Hydrology**2), axis=1)\n",
    "engineered_features['Elevation_Of_Hydrology'] = engineered_features['Elevation']-engineered_features['Vertical_Distance_To_Hydrology']\n",
    "engineered_features['Mean_Distance_To_Feature'] = (engineered_features['Horizontal_Distance_To_Hydrology']+engineered_features['Horizontal_Distance_To_Roadways']+engineered_features['Horizontal_Distance_To_Fire_Points'])/3\n",
    "engineered_features['Stony'] = engineered_features[['Soil_Type1', 'Soil_Type2', 'Soil_Type6', 'Soil_Type9', 'Soil_Type12', 'Soil_Type18', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', ]].any(axis=1)\n",
    "median_hillshade_9am = np.median(engineered_features['Hillshade_9am'])\n",
    "engineered_features['Hillshade_9am'] = engineered_features.apply(lambda row: median_hillshade_9am if row.Hillshade_9am == 0 else row.Hillshade_9am, axis=1)\n",
    "median_hillshade_3pm = np.median(engineered_features['Hillshade_3pm'])\n",
    "engineered_features['Hillshade_3pm'] = engineered_features.apply(lambda row: median_hillshade_3pm if row.Hillshade_3pm == 0 else row.Hillshade_3pm, axis=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "e_X_train, e_X_test, e_y_train, e_y_test = train_test_split(engineered_features, full_labels, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Test Feature Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without _a priori_ knowledge of how the interplay between soil types, topography, hydrology, etc. affects forest cover, we need a way to view the performance of new features.  As such we will use a simple Gaussian Naive Bayes model to do predictions, and quanitify the results using cross-validation.  We will be tracking performance across precision, recall, and f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naïve Bayes\n",
    "\n",
    "One reasonable place to begin might be a Naïve Bayes classifier. While it is unlikely that all of the features at our disposal are _strictly_ independent, we may be able to relax the assumption of independence enough to explore how a NB model performs.\n",
    "\n",
    "We don't want a Bernoulli NB model: our features are not uniformly binary-valued. We also don't want a Multinomial NB model: per the documentation, it assumes integer feature counts. A Gaussian NB, on the other hand, might work well. While it assumes that the likelihoods of the features are Gaussian - and this is not necessarily strictly the case - it may be worth trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:45:02.052160Z",
     "start_time": "2019-04-07T08:45:00.212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base Data GaussianNB, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.72 | recall:  0.50 | f1-score:  0.59 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.13 | recall:  0.73 | f1-score:  0.22 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.73 | recall:  0.43 | f1-score:  0.54 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.92 | recall:  0.68 | f1-score:  0.78 | \n",
      "\t\t\tAspen                | precision:  0.73 | recall:  0.59 | f1-score:  0.65 | \n",
      "\t\t\tDouglas Fir          | precision:  0.07 | recall:  0.77 | f1-score:  0.12 | \n",
      "\t\t\tKrummholz            | precision:  0.82 | recall:  0.86 | f1-score:  0.84 | \n",
      "\t\t\tmacro avg            | precision:  0.59 | recall:  0.65 | f1-score:  0.54 | \n",
      "\t\t\tmicro avg            | precision:  0.59 | recall:  0.59 | f1-score:  0.59 | \n",
      "\t\t\tweighted avg         | precision:  0.76 | recall:  0.59 | f1-score:  0.65 | \n",
      "\n",
      "Model: Base Data GaussianNB, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.73 | recall:  0.53 | f1-score:  0.61 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.16 | recall:  0.73 | f1-score:  0.26 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.73 | recall:  0.44 | f1-score:  0.55 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.93 | recall:  0.69 | f1-score:  0.79 | \n",
      "\t\t\tAspen                | precision:  0.79 | recall:  0.62 | f1-score:  0.69 | \n",
      "\t\t\tDouglas Fir          | precision:  0.08 | recall:  0.80 | f1-score:  0.14 | \n",
      "\t\t\tKrummholz            | precision:  0.86 | recall:  0.85 | f1-score:  0.85 | \n",
      "\t\t\tmacro avg            | precision:  0.61 | recall:  0.66 | f1-score:  0.56 | \n",
      "\t\t\tmicro avg            | precision:  0.61 | recall:  0.61 | f1-score:  0.61 | \n",
      "\t\t\tweighted avg         | precision:  0.77 | recall:  0.61 | f1-score:  0.66 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on the base data\n",
    "cross_validate_model(GaussianNB(), X_train, y_train, name='Base Data GaussianNB', verbose=True)\n",
    "\n",
    "# Testing on the engineered data\n",
    "cross_validate_model(GaussianNB(), e_X_train, e_y_train, name='Base Data GaussianNB', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we will use two metrics to determine how well a particular model performs, precision and recall.  All in all, just throwing a Gaussian Naive Bayes classifier at the data performed better than expected.  It achieved a 76% weighted precision across 5 fold cross validation.  \n",
    "\n",
    "The engineered features do not provide as much improvement as hoped.  They resulted in 1-2% improvements across all of the metrics.  One positive however is that the improved features seem to help the poorly classified labels more than the already well classified labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed Engineered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every feature that is engineered is a useful addition to the data set.  Randomly adding new features can add noise to the dataset without providing any new information.  We have listed the failed features below.  Some highlights include Mountain width and prominance from the Elevation and Slope features, and a few different ways to view the elevation of an area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineered_features['Elevation_Away_From_Hydrology'] = engineered_features['Elevation']-engineered_features['Horizontal_Distance_To_Hydrology']\n",
    "# engineered_features['Moutain_Width'] = engineered_features.apply(lambda row: row.Elevation/math.tan(math.radians(row.Slope+.1)), axis=1)\n",
    "# engineered_features['Moutain_Prominence'] = engineered_features.apply(lambda row: row.Elevation/math.sin(math.radians(row.Slope+.1)), axis=1)\n",
    "# engineered_features['Mean_Hillshade'] = engineered_features.apply(lambda row: (row.Hillshade_9am + row.Hillshade_Noon + row.Hillshade_3pm)/3, axis=1)\n",
    "# engineered_features['Morning_Hillshade'] = engineered_features.apply(lambda row: (row.Hillshade_9am * row.Hillshade_Noon), axis=1)\n",
    "# engineered_features['Norm_Horizontal_Distance_To_Hydrology'] = engineered_features['Horizontal_Distance_To_Hydrology']/(np.mean(engineered_features['Horizontal_Distance_To_Hydrology']))\n",
    "# engineered_features['Norm_Elevation'] = engineered_features['Elevation']/(np.mean(engineered_features['Elevation']))\n",
    "# engineered_features['Log_Elevation'] = engineered_features.apply(lambda row: math.log(row.Elevation), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is a very important aspect of preparing data for consumption by machine learning models.  It brings all of the features into a simlilar range, meaning that the models will not end up with widely varying coefficients due to the differing scales of features.  By ensuring all features exist within a given range, we eliminate the possibility that a feature could dominate the weighting and prediction process simply by virtue of having a range that is a few orders of magnitude greater than that of another (potentially more meaningful) feature.   We will experiment with several different types of standardization to see which is the most effective. Specifically we will test Min-Max scaling, standard scaling, robust scaling and sklearn's normalizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Test Standardization: K Nearest Neighbors\n",
    "\n",
    "One issue with Naive Bayes is that they are more or less invariant to feature scaling, and therefore cannot be used when testing different standardization methods.  We will use the `KNearestClassifier` with k = 3 when testing out our performance on scaled data.  We found that k = 3 consistently produces the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.66 | recall:  0.74 | f1-score:  0.70 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.58 | recall:  0.71 | f1-score:  0.64 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.74 | recall:  0.83 | f1-score:  0.78 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.96 | recall:  0.90 | f1-score:  0.93 | \n",
      "\t\t\tAspen                | precision:  0.96 | recall:  0.83 | f1-score:  0.89 | \n",
      "\t\t\tDouglas Fir          | precision:  0.85 | recall:  0.80 | f1-score:  0.82 | \n",
      "\t\t\tKrummholz            | precision:  0.97 | recall:  0.88 | f1-score:  0.92 | \n",
      "\t\t\tmacro avg            | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "\t\t\tmicro avg            | precision:  0.82 | recall:  0.82 | f1-score:  0.82 | \n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on the unscaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KNearestClassifier` performed remarkably well on the base data with 83% precision and 81% recall.  This is a remarkably good result for a very basic model.  However with such a low K, it is important to keep overfitting in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MinMaxScaler` module, scales the minimum value in the set to the lower bound, and the max to the upper bound.  All points in-between are adjusted to the new scale.  This has the effect of grouping inliers together, making the data consistend, but also makes the data very narrow.\n",
    "\n",
    "A range of [-1, 1] may be preferable to a range of [0, 1] because the `Elevation` and `Vertical_Distance_To_Hydrology` variables could legitimately include negative values. (`Elevation` does not, at least in our training data, but it could.)  \n",
    "\n",
    "Using the range of [0,1], however, will ensure that the majority of our features make sense when scaled.  This will make features like `Slope` and `Horizontal_Distance_To_Hydrology` make more sense.  It will also keep our binary features the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:45:02.054657Z",
     "start_time": "2019-04-07T08:45:01.102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler [-1,1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Euclidean_Distance_To_Hydrology</th>\n",
       "      <th>Elevation_Of_Hydrology</th>\n",
       "      <th>Mean_Distance_To_Feature</th>\n",
       "      <th>Stony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.112790</td>\n",
       "      <td>-0.644444</td>\n",
       "      <td>-0.153846</td>\n",
       "      <td>-0.900223</td>\n",
       "      <td>-0.534286</td>\n",
       "      <td>-0.812171</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>-0.384615</td>\n",
       "      <td>-0.830688</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.898119</td>\n",
       "      <td>0.146846</td>\n",
       "      <td>-0.822992</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.503525</td>\n",
       "      <td>-0.616667</td>\n",
       "      <td>-0.153846</td>\n",
       "      <td>-0.728965</td>\n",
       "      <td>-0.337143</td>\n",
       "      <td>-0.795202</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.392713</td>\n",
       "      <td>-0.740312</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.703309</td>\n",
       "      <td>-0.557394</td>\n",
       "      <td>-0.747331</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.325277</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.769231</td>\n",
       "      <td>-0.865972</td>\n",
       "      <td>-0.577143</td>\n",
       "      <td>-0.266823</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.109312</td>\n",
       "      <td>-0.304733</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.867316</td>\n",
       "      <td>-0.287487</td>\n",
       "      <td>-0.246480</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.441088</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>-0.346154</td>\n",
       "      <td>-0.873418</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>-0.596255</td>\n",
       "      <td>0.316327</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.279352</td>\n",
       "      <td>-0.876162</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.872782</td>\n",
       "      <td>-0.419855</td>\n",
       "      <td>-0.730620</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.673716</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>-0.256888</td>\n",
       "      <td>-0.171429</td>\n",
       "      <td>-0.841720</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.961290</td>\n",
       "      <td>0.716599</td>\n",
       "      <td>-0.730588</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.234510</td>\n",
       "      <td>-0.792141</td>\n",
       "      <td>-0.717623</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   0.112790 -0.644444 -0.153846                         -0.900223   \n",
       "1  -0.503525 -0.616667 -0.153846                         -0.728965   \n",
       "2  -0.325277 -0.700000 -0.769231                         -0.865972   \n",
       "3  -0.441088  0.944444 -0.346154                         -0.873418   \n",
       "4  -0.673716  0.277778  0.115385                         -0.256888   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                       -0.534286                        -0.812171   \n",
       "1                       -0.337143                        -0.795202   \n",
       "2                       -0.577143                        -0.266823   \n",
       "3                       -0.540000                        -0.596255   \n",
       "4                       -0.171429                        -0.841720   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0       0.795918        0.161290      -0.384615   \n",
       "1       0.826531        0.200000      -0.392713   \n",
       "2       0.693878        0.651613       0.109312   \n",
       "3       0.316327        0.419355       0.279352   \n",
       "4       0.051020        0.961290       0.716599   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  ...    Soil_Type35  Soil_Type36  \\\n",
       "0                           -0.830688  ...           -1.0         -1.0   \n",
       "1                           -0.740312  ...           -1.0         -1.0   \n",
       "2                           -0.304733  ...           -1.0         -1.0   \n",
       "3                           -0.876162  ...           -1.0         -1.0   \n",
       "4                           -0.730588  ...           -1.0         -1.0   \n",
       "\n",
       "   Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40  \\\n",
       "0         -1.0         -1.0         -1.0         -1.0   \n",
       "1         -1.0         -1.0         -1.0         -1.0   \n",
       "2         -1.0         -1.0         -1.0         -1.0   \n",
       "3         -1.0         -1.0         -1.0         -1.0   \n",
       "4         -1.0         -1.0         -1.0         -1.0   \n",
       "\n",
       "   Euclidean_Distance_To_Hydrology  Elevation_Of_Hydrology  \\\n",
       "0                        -0.898119                0.146846   \n",
       "1                        -0.703309               -0.557394   \n",
       "2                        -0.867316               -0.287487   \n",
       "3                        -0.872782               -0.419855   \n",
       "4                        -0.234510               -0.792141   \n",
       "\n",
       "   Mean_Distance_To_Feature  Stony  \n",
       "0                 -0.822992    1.0  \n",
       "1                 -0.747331    1.0  \n",
       "2                 -0.246480   -1.0  \n",
       "3                 -0.730620   -1.0  \n",
       "4                 -0.717623    1.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_neg1_1_scaled_df = apply_scaler(MinMaxScaler(feature_range=(-1, 1)), e_X_train)\n",
    "print(\"MinMaxScaler [-1,1]\")\n",
    "mm_neg1_1_scaled_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data to the range [-1, 1] appears to result in values that are incoherent. For instance, the values for the `Slope` and `Horizontal_Distance_To_Hydrology` features are now negative, which does not make conceptual sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler [0,1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Euclidean_Distance_To_Hydrology</th>\n",
       "      <th>Elevation_Of_Hydrology</th>\n",
       "      <th>Mean_Distance_To_Feature</th>\n",
       "      <th>Stony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.556395</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.049888</td>\n",
       "      <td>0.232857</td>\n",
       "      <td>0.093915</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050940</td>\n",
       "      <td>0.573423</td>\n",
       "      <td>0.088504</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.248238</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.135517</td>\n",
       "      <td>0.331429</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.303644</td>\n",
       "      <td>0.129844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148345</td>\n",
       "      <td>0.221303</td>\n",
       "      <td>0.126335</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.337362</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.067014</td>\n",
       "      <td>0.211429</td>\n",
       "      <td>0.366589</td>\n",
       "      <td>0.846939</td>\n",
       "      <td>0.825806</td>\n",
       "      <td>0.554656</td>\n",
       "      <td>0.347633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066342</td>\n",
       "      <td>0.356256</td>\n",
       "      <td>0.376760</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279456</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.201872</td>\n",
       "      <td>0.658163</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.639676</td>\n",
       "      <td>0.061919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063609</td>\n",
       "      <td>0.290072</td>\n",
       "      <td>0.134690</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.163142</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.371556</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.079140</td>\n",
       "      <td>0.525510</td>\n",
       "      <td>0.980645</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>0.134706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382745</td>\n",
       "      <td>0.103930</td>\n",
       "      <td>0.141188</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   0.556395  0.177778  0.423077                          0.049888   \n",
       "1   0.248238  0.191667  0.423077                          0.135517   \n",
       "2   0.337362  0.150000  0.115385                          0.067014   \n",
       "3   0.279456  0.972222  0.326923                          0.063291   \n",
       "4   0.163142  0.638889  0.557692                          0.371556   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                        0.232857                         0.093915   \n",
       "1                        0.331429                         0.102399   \n",
       "2                        0.211429                         0.366589   \n",
       "3                        0.230000                         0.201872   \n",
       "4                        0.414286                         0.079140   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0       0.897959        0.580645       0.307692   \n",
       "1       0.913265        0.600000       0.303644   \n",
       "2       0.846939        0.825806       0.554656   \n",
       "3       0.658163        0.709677       0.639676   \n",
       "4       0.525510        0.980645       0.858300   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  ...    Soil_Type35  Soil_Type36  \\\n",
       "0                            0.084656  ...            0.0          0.0   \n",
       "1                            0.129844  ...            0.0          0.0   \n",
       "2                            0.347633  ...            0.0          0.0   \n",
       "3                            0.061919  ...            0.0          0.0   \n",
       "4                            0.134706  ...            0.0          0.0   \n",
       "\n",
       "   Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40  \\\n",
       "0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0   \n",
       "2          0.0          0.0          0.0          0.0   \n",
       "3          0.0          0.0          0.0          0.0   \n",
       "4          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Euclidean_Distance_To_Hydrology  Elevation_Of_Hydrology  \\\n",
       "0                         0.050940                0.573423   \n",
       "1                         0.148345                0.221303   \n",
       "2                         0.066342                0.356256   \n",
       "3                         0.063609                0.290072   \n",
       "4                         0.382745                0.103930   \n",
       "\n",
       "   Mean_Distance_To_Feature  Stony  \n",
       "0                  0.088504    1.0  \n",
       "1                  0.126335    1.0  \n",
       "2                  0.376760    0.0  \n",
       "3                  0.134690    0.0  \n",
       "4                  0.141188    1.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_0_1_scaled_df = apply_scaler(MinMaxScaler(feature_range=(0, 1)), e_X_train)\n",
    "print(\"MinMaxScaler [0,1]\")\n",
    "mm_0_1_scaled_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the alternative scaler using the range [0, 1] seem to be a little more coherent. The primary feature whose transformation may be unexpected would be the `Vertical_Distance_To_Hydrology`, because in unscaled form it contained negative values. We should examine it under scaled conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Feature Scaling [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StandardScaler` scales the data to be within [0,1] by default, and also scales the data to a unit variance.  Using the `Standard Scaler` should help our scaled data stay in nice distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:45:03.490156Z",
     "start_time": "2019-04-07T08:45:03.376656Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler [0,1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Euclidean_Distance_To_Hydrology</th>\n",
       "      <th>Elevation_Of_Hydrology</th>\n",
       "      <th>Mean_Distance_To_Feature</th>\n",
       "      <th>Stony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.524048</td>\n",
       "      <td>-0.846207</td>\n",
       "      <td>0.645370</td>\n",
       "      <td>-0.764506</td>\n",
       "      <td>-0.558748</td>\n",
       "      <td>-0.807802</td>\n",
       "      <td>0.695076</td>\n",
       "      <td>-1.307127</td>\n",
       "      <td>-1.312308</td>\n",
       "      <td>-0.834120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08205</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>-0.044588</td>\n",
       "      <td>-0.228456</td>\n",
       "      <td>-0.213266</td>\n",
       "      <td>-0.174919</td>\n",
       "      <td>-0.776211</td>\n",
       "      <td>0.610912</td>\n",
       "      <td>-1.001750</td>\n",
       "      <td>0.950237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.942188</td>\n",
       "      <td>-0.800788</td>\n",
       "      <td>0.645370</td>\n",
       "      <td>-0.219322</td>\n",
       "      <td>0.563945</td>\n",
       "      <td>-0.763844</td>\n",
       "      <td>0.793233</td>\n",
       "      <td>-1.175931</td>\n",
       "      <td>-1.334638</td>\n",
       "      <td>-0.545296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08205</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>-0.044588</td>\n",
       "      <td>-0.228456</td>\n",
       "      <td>-0.213266</td>\n",
       "      <td>-0.174919</td>\n",
       "      <td>-0.165322</td>\n",
       "      <td>-1.032989</td>\n",
       "      <td>-0.772814</td>\n",
       "      <td>0.950237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.518129</td>\n",
       "      <td>-0.937044</td>\n",
       "      <td>-1.247065</td>\n",
       "      <td>-0.655470</td>\n",
       "      <td>-0.802812</td>\n",
       "      <td>0.604931</td>\n",
       "      <td>0.367886</td>\n",
       "      <td>0.354698</td>\n",
       "      <td>0.049816</td>\n",
       "      <td>0.846726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08205</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>-0.044588</td>\n",
       "      <td>-0.228456</td>\n",
       "      <td>-0.213266</td>\n",
       "      <td>-0.174919</td>\n",
       "      <td>-0.679617</td>\n",
       "      <td>-0.402948</td>\n",
       "      <td>0.742659</td>\n",
       "      <td>-1.052369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.793647</td>\n",
       "      <td>1.751748</td>\n",
       "      <td>0.053984</td>\n",
       "      <td>-0.679173</td>\n",
       "      <td>-0.591290</td>\n",
       "      <td>-0.248469</td>\n",
       "      <td>-0.842715</td>\n",
       "      <td>-0.432483</td>\n",
       "      <td>0.518743</td>\n",
       "      <td>-0.979446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08205</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>-0.044588</td>\n",
       "      <td>-0.228456</td>\n",
       "      <td>-0.213266</td>\n",
       "      <td>-0.174919</td>\n",
       "      <td>-0.696759</td>\n",
       "      <td>-0.711934</td>\n",
       "      <td>-0.722251</td>\n",
       "      <td>-1.052369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.347079</td>\n",
       "      <td>0.661697</td>\n",
       "      <td>1.473311</td>\n",
       "      <td>1.283491</td>\n",
       "      <td>1.507658</td>\n",
       "      <td>-0.884350</td>\n",
       "      <td>-1.693408</td>\n",
       "      <td>1.404272</td>\n",
       "      <td>1.724558</td>\n",
       "      <td>-0.514220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08205</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>-0.044588</td>\n",
       "      <td>-0.228456</td>\n",
       "      <td>-0.213266</td>\n",
       "      <td>-0.174919</td>\n",
       "      <td>1.304749</td>\n",
       "      <td>-1.580957</td>\n",
       "      <td>-0.682925</td>\n",
       "      <td>0.950237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   0.524048 -0.846207  0.645370                         -0.764506   \n",
       "1  -0.942188 -0.800788  0.645370                         -0.219322   \n",
       "2  -0.518129 -0.937044 -1.247065                         -0.655470   \n",
       "3  -0.793647  1.751748  0.053984                         -0.679173   \n",
       "4  -1.347079  0.661697  1.473311                          1.283491   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                       -0.558748                        -0.807802   \n",
       "1                        0.563945                        -0.763844   \n",
       "2                       -0.802812                         0.604931   \n",
       "3                       -0.591290                        -0.248469   \n",
       "4                        1.507658                        -0.884350   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0       0.695076       -1.307127      -1.312308   \n",
       "1       0.793233       -1.175931      -1.334638   \n",
       "2       0.367886        0.354698       0.049816   \n",
       "3      -0.842715       -0.432483       0.518743   \n",
       "4      -1.693408        1.404272       1.724558   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points    ...     Soil_Type35  Soil_Type36  \\\n",
       "0                           -0.834120    ...        -0.08205    -0.025726   \n",
       "1                           -0.545296    ...        -0.08205    -0.025726   \n",
       "2                            0.846726    ...        -0.08205    -0.025726   \n",
       "3                           -0.979446    ...        -0.08205    -0.025726   \n",
       "4                           -0.514220    ...        -0.08205    -0.025726   \n",
       "\n",
       "   Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40  \\\n",
       "0    -0.044588    -0.228456    -0.213266    -0.174919   \n",
       "1    -0.044588    -0.228456    -0.213266    -0.174919   \n",
       "2    -0.044588    -0.228456    -0.213266    -0.174919   \n",
       "3    -0.044588    -0.228456    -0.213266    -0.174919   \n",
       "4    -0.044588    -0.228456    -0.213266    -0.174919   \n",
       "\n",
       "   Euclidean_Distance_To_Hydrology  Elevation_Of_Hydrology  \\\n",
       "0                        -0.776211                0.610912   \n",
       "1                        -0.165322               -1.032989   \n",
       "2                        -0.679617               -0.402948   \n",
       "3                        -0.696759               -0.711934   \n",
       "4                         1.304749               -1.580957   \n",
       "\n",
       "   Mean_Distance_To_Feature     Stony  \n",
       "0                 -1.001750  0.950237  \n",
       "1                 -0.772814  0.950237  \n",
       "2                  0.742659 -1.052369  \n",
       "3                 -0.722251 -1.052369  \n",
       "4                 -0.682925  0.950237  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scaled_df = apply_scaler(StandardScaler(), e_X_train)\n",
    "print(\"StandardScaler [0,1]\")\n",
    "standard_scaled_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robust Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RobustScaler` behaves very similarly to the `MinMaxScaler` but it uses the inter-quartile range to scale features rather than just min and max.  This should allow it to be more robust to outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:45:04.439657Z",
     "start_time": "2019-04-07T08:45:04.305155Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler [0,1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Euclidean_Distance_To_Hydrology</th>\n",
       "      <th>Elevation_Of_Hydrology</th>\n",
       "      <th>Mean_Distance_To_Feature</th>\n",
       "      <th>Stony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.297799</td>\n",
       "      <td>-0.316327</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>-0.429658</td>\n",
       "      <td>-0.206667</td>\n",
       "      <td>-0.443780</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>-1.196429</td>\n",
       "      <td>-1.016667</td>\n",
       "      <td>-0.525896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.439403</td>\n",
       "      <td>0.342742</td>\n",
       "      <td>-0.555832</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.544017</td>\n",
       "      <td>-0.290816</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>-0.405248</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>-1.089286</td>\n",
       "      <td>-1.033333</td>\n",
       "      <td>-0.274104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040931</td>\n",
       "      <td>-0.572581</td>\n",
       "      <td>-0.368834</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.300550</td>\n",
       "      <td>-0.367347</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.342205</td>\n",
       "      <td>-0.406667</td>\n",
       "      <td>0.794552</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.939442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.363453</td>\n",
       "      <td>-0.221774</td>\n",
       "      <td>0.869025</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.458735</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.361217</td>\n",
       "      <td>-0.233333</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>-0.846154</td>\n",
       "      <td>-0.482143</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>-0.652590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.376931</td>\n",
       "      <td>-0.393817</td>\n",
       "      <td>-0.327533</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.776479</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.212928</td>\n",
       "      <td>1.486667</td>\n",
       "      <td>-0.510879</td>\n",
       "      <td>-1.512821</td>\n",
       "      <td>1.017857</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>-0.247012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.196829</td>\n",
       "      <td>-0.877688</td>\n",
       "      <td>-0.295411</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   0.297799 -0.316327  0.583333                         -0.429658   \n",
       "1  -0.544017 -0.290816  0.583333                          0.007605   \n",
       "2  -0.300550 -0.367347 -0.750000                         -0.342205   \n",
       "3  -0.458735  1.142857  0.166667                         -0.361217   \n",
       "4  -0.776479  0.530612  1.166667                          1.212928   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                       -0.206667                        -0.443780   \n",
       "1                        0.713333                        -0.405248   \n",
       "2                       -0.406667                         0.794552   \n",
       "3                       -0.233333                         0.046504   \n",
       "4                        1.486667                        -0.510879   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0       0.358974       -1.196429      -1.016667   \n",
       "1       0.435897       -1.089286      -1.033333   \n",
       "2       0.102564        0.160714       0.000000   \n",
       "3      -0.846154       -0.482143       0.350000   \n",
       "4      -1.512821        1.017857       1.250000   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  ...    Soil_Type35  Soil_Type36  \\\n",
       "0                           -0.525896  ...            0.0          0.0   \n",
       "1                           -0.274104  ...            0.0          0.0   \n",
       "2                            0.939442  ...            0.0          0.0   \n",
       "3                           -0.652590  ...            0.0          0.0   \n",
       "4                           -0.247012  ...            0.0          0.0   \n",
       "\n",
       "   Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40  \\\n",
       "0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0   \n",
       "2          0.0          0.0          0.0          0.0   \n",
       "3          0.0          0.0          0.0          0.0   \n",
       "4          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Euclidean_Distance_To_Hydrology  Elevation_Of_Hydrology  \\\n",
       "0                        -0.439403                0.342742   \n",
       "1                         0.040931               -0.572581   \n",
       "2                        -0.363453               -0.221774   \n",
       "3                        -0.376931               -0.393817   \n",
       "4                         1.196829               -0.877688   \n",
       "\n",
       "   Mean_Distance_To_Feature  Stony  \n",
       "0                 -0.555832    0.0  \n",
       "1                 -0.368834    0.0  \n",
       "2                  0.869025   -1.0  \n",
       "3                 -0.327533   -1.0  \n",
       "4                 -0.295411    0.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_scaled_df = apply_scaler(RobustScaler(), e_X_train)\n",
    "print(\"RobustScaler [0,1]\")\n",
    "r_scaled_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizer Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Normalizer` is a very extreme form of normalization that is generally used when all of the features are in similar ranges.  What is does is scales all features to the same range, and forms a hypersphere of all the feature maxes.  This means that all features are within 1 unit length of the center via cartesian distance across all feature planes.  This is not an ideal case for this scaler, but it may be interesting to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:45:05.373158Z",
     "start_time": "2019-04-07T08:45:04.682Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13608, 56)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Euclidean_Distance_To_Hydrology</th>\n",
       "      <th>Elevation_Of_Hydrology</th>\n",
       "      <th>Mean_Distance_To_Feature</th>\n",
       "      <th>Stony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.688584</td>\n",
       "      <td>0.014848</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.015544</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.148946</td>\n",
       "      <td>0.054289</td>\n",
       "      <td>0.043848</td>\n",
       "      <td>0.017864</td>\n",
       "      <td>0.137346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016037</td>\n",
       "      <td>0.684640</td>\n",
       "      <td>0.100612</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.664805</td>\n",
       "      <td>0.019470</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>0.051356</td>\n",
       "      <td>0.024267</td>\n",
       "      <td>0.197523</td>\n",
       "      <td>0.066876</td>\n",
       "      <td>0.054178</td>\n",
       "      <td>0.021445</td>\n",
       "      <td>0.256215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056801</td>\n",
       "      <td>0.640538</td>\n",
       "      <td>0.168365</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.479038</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.473931</td>\n",
       "      <td>0.042363</td>\n",
       "      <td>0.042930</td>\n",
       "      <td>0.026098</td>\n",
       "      <td>0.459747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.478659</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.638199</td>\n",
       "      <td>0.092378</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.022435</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.364233</td>\n",
       "      <td>0.049356</td>\n",
       "      <td>0.055163</td>\n",
       "      <td>0.041966</td>\n",
       "      <td>0.114285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022781</td>\n",
       "      <td>0.634240</td>\n",
       "      <td>0.166984</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.650569</td>\n",
       "      <td>0.068418</td>\n",
       "      <td>0.008627</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.042836</td>\n",
       "      <td>0.160932</td>\n",
       "      <td>0.047893</td>\n",
       "      <td>0.074665</td>\n",
       "      <td>0.063361</td>\n",
       "      <td>0.280218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>0.607733</td>\n",
       "      <td>0.196529</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.621685</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.069414</td>\n",
       "      <td>0.014023</td>\n",
       "      <td>0.305467</td>\n",
       "      <td>0.048379</td>\n",
       "      <td>0.051885</td>\n",
       "      <td>0.035525</td>\n",
       "      <td>0.291912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070816</td>\n",
       "      <td>0.607662</td>\n",
       "      <td>0.222264</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.617939</td>\n",
       "      <td>0.042740</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.141690</td>\n",
       "      <td>0.043279</td>\n",
       "      <td>0.384304</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>0.044716</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>0.184430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148152</td>\n",
       "      <td>0.574660</td>\n",
       "      <td>0.236808</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.659991</td>\n",
       "      <td>0.108395</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285419</td>\n",
       "      <td>0.059970</td>\n",
       "      <td>0.069912</td>\n",
       "      <td>0.054518</td>\n",
       "      <td>0.091398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.659991</td>\n",
       "      <td>0.125606</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.628566</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.343371</td>\n",
       "      <td>0.065752</td>\n",
       "      <td>0.053305</td>\n",
       "      <td>0.020023</td>\n",
       "      <td>0.238925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>0.617472</td>\n",
       "      <td>0.199510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.612945</td>\n",
       "      <td>0.091384</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.052182</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.360861</td>\n",
       "      <td>0.049326</td>\n",
       "      <td>0.054778</td>\n",
       "      <td>0.041019</td>\n",
       "      <td>0.275189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058753</td>\n",
       "      <td>0.585945</td>\n",
       "      <td>0.229411</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   0.688584  0.014848  0.005104                          0.015544   \n",
       "1   0.664805  0.019470  0.006208                          0.051356   \n",
       "2   0.479038  0.010212  0.001135                          0.017021   \n",
       "3   0.638199  0.092378  0.004487                          0.022435   \n",
       "4   0.650569  0.068418  0.008627                          0.148438   \n",
       "5   0.621685  0.000701  0.002103                          0.069414   \n",
       "6   0.617939  0.042740  0.001796                          0.141690   \n",
       "7   0.659991  0.108395  0.004810                          0.000000   \n",
       "8   0.628566  0.021647  0.005682                          0.016235   \n",
       "9   0.612945  0.091384  0.004154                          0.052182   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                        0.003944                         0.148946   \n",
       "1                        0.024267                         0.197523   \n",
       "2                        0.000378                         0.473931   \n",
       "3                        0.003959                         0.364233   \n",
       "4                        0.042836                         0.160932   \n",
       "5                        0.014023                         0.305467   \n",
       "6                        0.043279                         0.384304   \n",
       "7                        0.000000                         0.285419   \n",
       "8                        0.011094                         0.343371   \n",
       "9                        0.027000                         0.360861   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0       0.054289        0.043848       0.017864   \n",
       "1       0.066876        0.054178       0.021445   \n",
       "2       0.042363        0.042930       0.026098   \n",
       "3       0.049356        0.055163       0.041966   \n",
       "4       0.047893        0.074665       0.063361   \n",
       "5       0.048379        0.051885       0.035525   \n",
       "6       0.036096        0.044716       0.033223   \n",
       "7       0.059970        0.069912       0.054518   \n",
       "8       0.065752        0.053305       0.020023   \n",
       "9       0.049326        0.054778       0.041019   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points    ...     Soil_Type35  Soil_Type36  \\\n",
       "0                            0.137346    ...             0.0          0.0   \n",
       "1                            0.256215    ...             0.0          0.0   \n",
       "2                            0.459747    ...             0.0          0.0   \n",
       "3                            0.114285    ...             0.0          0.0   \n",
       "4                            0.280218    ...             0.0          0.0   \n",
       "5                            0.291912    ...             0.0          0.0   \n",
       "6                            0.184430    ...             0.0          0.0   \n",
       "7                            0.091398    ...             0.0          0.0   \n",
       "8                            0.238925    ...             0.0          0.0   \n",
       "9                            0.275189    ...             0.0          0.0   \n",
       "\n",
       "   Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40  \\\n",
       "0          0.0      0.00000          0.0          0.0   \n",
       "1          0.0      0.00000          0.0          0.0   \n",
       "2          0.0      0.00000          0.0          0.0   \n",
       "3          0.0      0.00000          0.0          0.0   \n",
       "4          0.0      0.00000          0.0          0.0   \n",
       "5          0.0      0.00000          0.0          0.0   \n",
       "6          0.0      0.00018          0.0          0.0   \n",
       "7          0.0      0.00000          0.0          0.0   \n",
       "8          0.0      0.00000          0.0          0.0   \n",
       "9          0.0      0.00000          0.0          0.0   \n",
       "\n",
       "   Euclidean_Distance_To_Hydrology  Elevation_Of_Hydrology  \\\n",
       "0                         0.016037                0.684640   \n",
       "1                         0.056801                0.640538   \n",
       "2                         0.017025                0.478659   \n",
       "3                         0.022781                0.634240   \n",
       "4                         0.154495                0.607733   \n",
       "5                         0.070816                0.607662   \n",
       "6                         0.148152                0.574660   \n",
       "7                         0.000000                0.659991   \n",
       "8                         0.019663                0.617472   \n",
       "9                         0.058753                0.585945   \n",
       "\n",
       "   Mean_Distance_To_Feature     Stony  \n",
       "0                  0.100612  0.000232  \n",
       "1                  0.168365  0.000282  \n",
       "2                  0.316900  0.000000  \n",
       "3                  0.166984  0.000000  \n",
       "4                  0.196529  0.000297  \n",
       "5                  0.222264  0.000234  \n",
       "6                  0.236808  0.000180  \n",
       "7                  0.125606  0.000000  \n",
       "8                  0.199510  0.000000  \n",
       "9                  0.229411  0.000000  \n",
       "\n",
       "[10 rows x 56 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_scaled_df = apply_scaler(Normalizer(), e_X_train)\n",
    "print(n_scaled_df.shape)\n",
    "n_scaled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:45:02.057657Z",
     "start_time": "2019-04-07T08:45:01.261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, MinMax scaled [-1,1]\n",
      "\t\t\tweighted avg         | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "Model: 3 Nearest Neighbors, MinMax scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "Model: 3 Nearest Neighbors, Standard scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.81 | recall:  0.80 | f1-score:  0.80 | \n",
      "Model: 3 Nearest Neighbors, Robust scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.81 | recall:  0.80 | f1-score:  0.80 | \n",
      "Model: 3 Nearest Neighbors, Normalized\n",
      "\t\t\tweighted avg         | precision:  0.73 | recall:  0.70 | f1-score:  0.71 | \n",
      "Model: 3 Nearest Neighbors\n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n"
     ]
    }
   ],
   "source": [
    "# Testing on the [-1,1] scaled data again for reference\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), mm_neg1_1_scaled_df, e_y_train, name='3 Nearest Neighbors, MinMax scaled [-1,1]')\n",
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), mm_0_1_scaled_df, e_y_train, name='3 Nearest Neighbors, MinMax scaled [0,1]')\n",
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), standard_scaled_df, e_y_train, name='3 Nearest Neighbors, Standard scaled [0,1]')\n",
    "# Testing on the Robust scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), r_scaled_df, e_y_train, name='3 Nearest Neighbors, Robust scaled [0,1]')\n",
    "# Testing on the Normalized data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), n_scaled_df, e_y_train, name='3 Nearest Neighbors, Normalized')\n",
    "# Testing on the unscaled data again for reference\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test is to determine how much the scaled features affect the GaussianNB model, so we have included the previous basic KNearestNeighbors results for referance.  \n",
    "\n",
    "First, it appears that the range that the features are scaled using the `MinMaxScaler` does not change the results.  This makes sense as the ratios between the features are linear to each other when using a `MinMaxScaler`.  Therefore we will use the [0,1] range as it causes the majority of our features to still make sense.\n",
    "\n",
    "The `StandardScaler` acutally performs slightly worse than the `MinMaxScaler` when used on a GaussianNB model.  The two perform very similarly by all measures, but the `StandardScaler` is just slightly worse.\n",
    "\n",
    "The `RobustScaler` averages out to the same performance as the others.\n",
    "\n",
    "Not surprisingly the `Normalizer` did not improve the results on this data set.  This is likely because the features have such extreme differences in ranges and variances.  For example, the `Slope` feature goes from 0-54, while the `Horizontal_Distance_To_Fire_Points` ranges from 0-6993.\n",
    "\n",
    "Looking at the results from the scaled data, we can see that our precision rose slightly, while our recall dropped by about 0.12.  This means that with the scaled features we get more of the labels correct, but we get fewer of the relevant results correct.  Looking at each label specifically we can see that `Cottonwood/Willow` jumps up to 1.00 precision and 0.46 recall, this is very suprising as 100% precision is very rare.  \n",
    "\n",
    "In many of the sklearn models that we will be using, the models themselves perform some standardization before training.  Overall, manually standardizing the features has not improved performance in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a great way to look at data.  It can be pulled apart to find the coefficients for individual features, and when given enough data can perform remarkably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.65 | recall:  0.62 | f1-score:  0.64 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.50 | recall:  0.58 | f1-score:  0.54 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.54 | recall:  0.59 | f1-score:  0.56 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.88 | recall:  0.79 | f1-score:  0.83 | \n",
      "\t\t\tAspen                | precision:  0.69 | recall:  0.62 | f1-score:  0.65 | \n",
      "\t\t\tDouglas Fir          | precision:  0.54 | recall:  0.56 | f1-score:  0.55 | \n",
      "\t\t\tKrummholz            | precision:  0.84 | recall:  0.87 | f1-score:  0.86 | \n",
      "\t\t\tmacro avg            | precision:  0.66 | recall:  0.66 | f1-score:  0.66 | \n",
      "\t\t\tmicro avg            | precision:  0.66 | recall:  0.66 | f1-score:  0.66 | \n",
      "\t\t\tweighted avg         | precision:  0.67 | recall:  0.66 | f1-score:  0.67 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate_model(LogisticRegression(), e_X_train, e_y_train, name='Logistic Regression', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us a good baseline to compare future algorithms to.  The basic one-vs-many `LogisticRegression` classifier achieved ~0.67 across the board, meaning about 2/3 of its predictions are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed K Nearest Neighbors above when testing how effective different forms of scaling were.  KNN is a good fit for this type of problem because there are a large number of examples relative to the number of classes.  This means that every new data point will have many 'neighbors' to choose from.  We found that the ideal number of neighbors was three as there is enough data to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.66 | recall:  0.74 | f1-score:  0.70 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.58 | recall:  0.71 | f1-score:  0.64 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.74 | recall:  0.83 | f1-score:  0.78 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.96 | recall:  0.90 | f1-score:  0.93 | \n",
      "\t\t\tAspen                | precision:  0.96 | recall:  0.83 | f1-score:  0.89 | \n",
      "\t\t\tDouglas Fir          | precision:  0.85 | recall:  0.80 | f1-score:  0.82 | \n",
      "\t\t\tKrummholz            | precision:  0.97 | recall:  0.88 | f1-score:  0.92 | \n",
      "\t\t\tmacro avg            | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "\t\t\tmicro avg            | precision:  0.82 | recall:  0.82 | f1-score:  0.82 | \n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What KNN does well is perform relatively well across all of the labels.  The lowest precision score it achieves is 56% on `Lodgepole Pines`.  As we saw in the About the Data section, Lodgepole pine trees live in areas that can be covered by many types of trees.  This makes classifying them especially hard with KNN as the Lodgepole covered areas often have neighbors of other cover types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are a common tool in a data scientist's kit.  They generally perform well on datasets that are semi linearly separable, but they are very slow to train.  We will take a look at the efficacy of SVMs on this data set, as it may give an indication as to how close to linearly separable this data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:46:42.652719Z",
     "start_time": "2019-04-07T08:45:05.680175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: linearSVC, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.19 | recall:  0.28 | f1-score:  0.16 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.24 | recall:  0.27 | f1-score:  0.14 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.35 | recall:  0.30 | f1-score:  0.22 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.54 | recall:  0.75 | f1-score:  0.52 | \n",
      "\t\t\tAspen                | precision:  0.09 | recall:  0.35 | f1-score:  0.14 | \n",
      "\t\t\tDouglas Fir          | precision:  0.43 | recall:  0.20 | f1-score:  0.21 | \n",
      "\t\t\tKrummholz            | precision:  0.49 | recall:  0.58 | f1-score:  0.48 | \n",
      "\t\t\tmacro avg            | precision:  0.33 | recall:  0.39 | f1-score:  0.27 | \n",
      "\t\t\tmicro avg            | precision:  0.33 | recall:  0.33 | f1-score:  0.33 | \n",
      "\t\t\tweighted avg         | precision:  0.80 | recall:  0.33 | f1-score:  0.40 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic Linear Support Vector machine \n",
    "cross_validate_model(LinearSVC(), e_X_train, e_y_train, name='linearSVC', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard linearSVC produces very poor results.  It has extremely low recall, and predicts the `Aspen` and `Ponderosa Pine` very poorly.  We will take a look at how it does on scaled data, as SVC's generally require some standardization.  The recommended scaling is mean 0 var 1, but we will see how the existing ones do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.961514Z",
     "start_time": "2019-04-07T08:46:42.659723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: linearSVC, Unscaled\n",
      "\t\t\tweighted avg         | precision:  0.80 | recall:  0.33 | f1-score:  0.40 | \n",
      "Model: linearSVC, MinMax scaled [-1,1]\n",
      "\t\t\tweighted avg         | precision:  0.68 | recall:  0.67 | f1-score:  0.67 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"D:\\Anaconda2\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"D:\\Anaconda2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"D:\\Anaconda2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"D:\\Anaconda2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"D:\\Anaconda2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 636, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"D:\\Anaconda2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 661, in _abort_queue\n",
      "    poller.poll(50)\n",
      "  File \"D:\\Anaconda2\\lib\\site-packages\\zmq\\sugar\\poll.py\", line 99, in poll\n",
      "    return zmq_poll(self.sockets, timeout=timeout)\n",
      "  File \"zmq/backend/cython/_poll.pyx\", line 123, in zmq.backend.cython._poll.zmq_poll\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Basic Linear Support Vector machine \n",
    "cross_validate_model(LinearSVC(), e_X_train, e_y_train, name='linearSVC, Unscaled')\n",
    "cross_validate_model(LinearSVC(), mm_neg1_1_scaled_df, e_y_train, name='linearSVC, MinMax scaled [-1,1]')\n",
    "scaled_X_train = scale(e_X_train)\n",
    "scaled_X_train_df = pd.DataFrame(data=scaled_X_train,    # values\n",
    "                         columns=e_X_train.columns)  # 1st row as the column names\n",
    "cross_validate_model(LinearSVC(), scaled_X_train_df, e_y_train, name='linearSVC, Scaled to mean=0, variance=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that using scaled features made the performance of the `LinearSVC` even worse, and that will be the end of the Support Vector Machins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests can be extremely effective on data sets with a myriad of features that each contain a little information.  The base data set contains 54 features, 44 of which are binary, making building many of the trees quick and easy.  In addition, Random Forest models can be pulled apart to take a look at which features are the most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.979530Z",
     "start_time": "2019-04-07T08:45:06.407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForest, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.76 | recall:  0.75 | f1-score:  0.75 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.66 | recall:  0.76 | f1-score:  0.70 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.82 | recall:  0.80 | f1-score:  0.81 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.97 | recall:  0.93 | f1-score:  0.95 | \n",
      "\t\t\tAspen                | precision:  0.93 | recall:  0.89 | f1-score:  0.91 | \n",
      "\t\t\tDouglas Fir          | precision:  0.82 | recall:  0.84 | f1-score:  0.83 | \n",
      "\t\t\tKrummholz            | precision:  0.96 | recall:  0.94 | f1-score:  0.95 | \n",
      "\t\t\tmacro avg            | precision:  0.84 | recall:  0.84 | f1-score:  0.84 | \n",
      "\t\t\tmicro avg            | precision:  0.84 | recall:  0.84 | f1-score:  0.84 | \n",
      "\t\t\tweighted avg         | precision:  0.85 | recall:  0.84 | f1-score:  0.85 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate_model(RandomForestClassifier(), e_X_train, e_y_train, name='RandomForest', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base `RandomForestClassifier` performs quite well out of the box, achieving a respectable ~0.83 across all of our metrics.  Lets see if using standardized data makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.981526Z",
     "start_time": "2019-04-07T08:45:06.413Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e_mm_0_1_scaled_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-25fba2b86d99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing on the [0,1] scaled data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcross_validate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_mm_0_1_scaled_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RandomForest, MinMax scaled [0,1]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Testing on the unscaled data again for reference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcross_validate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RandomForest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e_mm_0_1_scaled_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(RandomForestClassifier(n_estimators = 10), e_mm_0_1_scaled_df, e_y_train, name='RandomForest, MinMax scaled [0,1]')\n",
    "# Testing on the unscaled data again for reference\n",
    "cross_validate_model(RandomForestClassifier(n_estimators = 10), e_X_train, e_y_train, name='RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the scaled features did not have an effect on the performance of the model.  Random Forests are made up of trees that only deal with a few features at a time, and do not care if a feature goes from [0,1] or [0,10000].  The tree's decision boundaries are set based on whatever scale that particular feature is at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "In the above nalysis, we have seen some of the models were very preicssion and recall on certain types of trees. At the same time we can see that most of the models de well with Cottonwood/Willow and poorly with Longpole Pine. So, we were interested to see if we could use the strength of all different models and take a voting to maximize the accuracy of the models. We use an ensemble model for this purpose. However, we also know that if our hero model, Random Forest, is more accurate than other models, then there is a possibility that the noise level from the poorly performing model will bring down the overall accuracy of the model. So, we should write the code in such a way that we can also see inside the ensemble, how each model performed.\n",
    "\n",
    "So far we have seen KNN, Liner SVC and Logistic Regression models besides Random Forest. We should take the opportunity to check other classifiers and see if they can beat our RF model. It is also important to model an ensemble of these various models with and optimizing different parameters to see f we can find a better alternative model. This is a costly and time consming approach. Gridsearch with multiple model ensemble with 2 - 3 parameters often take hours of processing even when we are using high end machines with many CPU cores. We, therefore, individually tested and optimized parameters for addition models such as MLP Classifier, Ada Boost, Quadratic Discriminant Analysis and Gaussian Process Classifier. Addditionally, we also varied hyper parameters for KNN, Random Forest indvidually to see the best outcome in each model. Finally, we created an ensemble model with 9 models in a \"hard\" and a \"soft\" voting model to see if we can get the best outcome.  \n",
    "\n",
    "#### Results\n",
    "Initial result showed what we suspected. The overall effect of the ensemble models were lower than the random forest. This is because the other models were adding noise to the overall. We then started process of elimination to get the best possible combnation. We found out the above RF can be beaten by ensemble model only when we use KNN and RF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=1)\n",
    "clf1 =LinearSVC()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GaussianNB()\n",
    "clf4 = KNeighborsClassifier()\n",
    "clf5 = SVC(gamma=2, C=1)\n",
    "clf6 = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "clf7 = MLPClassifier(alpha=1,activation = 'tanh',solver = 'lbfgs' )\n",
    "#clf8 = AdaBoostClassifier()\n",
    "clf8 = XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4)\n",
    "clf9 = QuadraticDiscriminantAnalysis()\n",
    "eclf = VotingClassifier(estimators=[\n",
    "#                                    ('lsvc', clf1)\n",
    "#                                    , \n",
    "#                                    ('rf', clf2)\n",
    "#                                    , ('gnb', clf3)\n",
    "#                                   , \n",
    "                                    ('knn', clf4)\n",
    "#                                   , ('svc', clf5)\n",
    "#                                   , ('gpc', clf6)\n",
    "#                                   , ('mlp', clf7)\n",
    "                                   , ('GB', clf8)\n",
    "#                                   , ('qda', clf9)\n",
    "                                   ], voting='hard')\n",
    "\n",
    "params = {\n",
    "    #    'lsvc__C': [1.0, 100.0], \n",
    "    \"knn__n_neighbors\":[3,5,7,],\"knn__weights\":[\"uniform\",\"distance\"],\n",
    "         # 'rf__n_estimators': [5, 10], \"rf__criterion\": [\"gini\", \"entropy\"], \"rf__min_samples_split\":[5,7,10]\n",
    "     }\n",
    "\n",
    "model = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "#model.fit(scaled_FX_train, Fy_train)\n",
    "#testPrediction = model.predict(scaled_FX_test)\n",
    "model.fit(e_X_train, e_y_train)\n",
    "testPrediction = model.predict(e_X_test)\n",
    "#model.fit(X_train, y_train)\n",
    "#testPrediction = model.predict(X_test)\n",
    "testReport = metrics.classification_report(testPrediction, e_y_test, output_dict=True)\n",
    "verbose= True\n",
    "#print(f'Model: {name}')\n",
    "reportFields = ['precision', 'recall', 'f1-score']\n",
    "fields = sorted(testReport.keys()) if verbose else ['weighted avg']\n",
    "fieldLabels = [label_names[field] if field in label_names.keys() else field for field in fields]\n",
    "fieldLabels[-1] = \"Final\"\n",
    "for i in range(len(fields)):\n",
    "    output = f'\\t\\t{fieldLabels[i]:<20} | '\n",
    "    for outputField in reportFields:\n",
    "        output += f'{outputField}: {np.mean(testReport[fields[i]][outputField]):>5.2f} | '\n",
    "    print(output)\n",
    "for clf, label in zip([\n",
    "            #clf1,\n",
    "            clf2, clf3, clf4\n",
    "            #, clf5,clf6,clf7,clf8,clf9\n",
    "            , eclf]\n",
    "            , [ \n",
    "                #\"LinearSVC\",\n",
    "                \"RandomForestClassifier\"\n",
    "                #,\"GaussianNB()\"\n",
    "                ,\"KNeighborsClassifier\"\n",
    "                #,\"SVC\",\"GaussianProcessClassifier\"\n",
    "                #,\"MLPClassifier\"\n",
    "                #         ,\"AdaBoostClassifier\",\"QuadraticDiscriminantAnalysis\"\n",
    "                ,\"Ensemble\"]):\n",
    "     scores = cross_val_score(clf, e_X_train, e_y_train, cv=5, scoring='accuracy')\n",
    "     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.983529Z",
     "start_time": "2019-04-07T08:45:06.422Z"
    }
   },
   "outputs": [],
   "source": [
    "#Note: Must fit the model prior to running this function\n",
    "def FeatImportance(model, dataColumns, title): \n",
    "# Calculate the feature ranking - Top 10 \n",
    "    importances = model.feature_importances_ \n",
    "    indices = np.argsort(importances)[::-1] \n",
    "    print (\"%s Top 10 Features\\n\" %title)\n",
    "    for f in range(10): \n",
    "        print(\"%d. %s (%f)\" % (f + 1, dataColumns.columns[indices[f]], importances[indices[f]])) \n",
    "    #Mean Feature Importance \n",
    "    print (\"\\nMean Feature Importance %.6f\" %np.mean(importances))\n",
    "    #Plot the feature importances of the forest \n",
    "    indices=indices[:10] \n",
    "    plt.figure() \n",
    "    plt.title(title+\" Top 10 Features\") \n",
    "    plt.bar(range(10), importances[indices], color=\"gb\", align=\"center\") \n",
    "    plt.xticks(range(10), dataColumns.columns[indices], fontsize=12, rotation=90) \n",
    "    plt.xlim([-1, 10]) \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.986527Z",
     "start_time": "2019-04-07T08:45:06.427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Initial RF and identify most important features\n",
    "initialRF = RandomForestClassifier(n_estimators = 10)\n",
    "initialRF.fit(e_X_train, e_y_train)\n",
    "FeatImportance(initialRF, e_X_train, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look at the most important features we can see which variables are most effective in predicting the cover type.  As expected the `Elevation` of an area is the biggest predictor of the `cover_type`.  Following the elevation of an area, we see the distances to roadways, fire points, and water all having a large effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Decision Trees are a cousin to the Random Forests.  When a Random Forest is built, it builds many trees in parallel trying to maximize the information gain of each tree.  In Gradient Boosting, trees are made iteratively with each tree attempting to correct the errors of the previous one.  These tend to perform a little better than Random Forests, while maintaining their nice properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_validate_model(XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4), e_X_train, e_y_train, name=f'Gradient Boosted Decision Trees (XGBoost)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting has proven to be the most effective model so far, achieving the very high score of 0.88 across the board.  This is a pretty great result because classifying new data using a `XGBClassifier` is extremely quick despite the long time it takes to train a new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master Model Result List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.989525Z",
     "start_time": "2019-04-07T08:45:06.771Z"
    }
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record=False):\n",
    "    test_model(LogisticRegression(), e_X_train, e_y_train, e_X_test, e_y_test, name=\"LogisticRegression\")\n",
    "    \n",
    "    test_model(GaussianNB(), e_X_train, e_y_train, e_X_test, e_y_test, name=\"GaussianNB\")\n",
    "    \n",
    "    test_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, e_X_test, e_y_test, name=\"3 Nearest Neighbors\")\n",
    "    \n",
    "    test_model(LinearSVC(), e_X_train, e_y_train, e_X_test, e_y_test, name='linearSVC')\n",
    "    \n",
    "    test_model(RandomForestClassifier(n_estimators = 10), e_X_train, e_y_train, e_X_test, e_y_test, name='RandomForest')\n",
    "    \n",
    "    test_model(XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4), e_X_train, e_y_train, e_X_test, e_y_test, name=f'Gradient Boosted Decision Trees (XGBoost)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "    \n",
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annexes\"></a>\n",
    "    \n",
    "# Annexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annexA\"></a>\n",
    "\n",
    "## Annex A: Exploratory Data Analysis\n",
    "\n",
    "This appendix contains our exploratory data analysis. This includes the code used to generate the 5-number summaries of our data reflected in the [_About the Data_](#aboutTheData) and other summaries. The most informative portions are replicated in the main body of the report\n",
    "\n",
    "After we load the data from the source file, we examine the basic characteristics of the dataset.\n",
    "  1. We expect to see all of the features discussed above represented in our column names\n",
    "  1. As there is no separate dataset containing the labels for our observations, we would expect to see the 'Cover_Type' variable in our data\n",
    "  1. We would expect to see a shape of (15120, 55) - the 54 features plus our label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.994528Z",
     "start_time": "2019-04-07T08:45:08.073Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Columns: {full_data.columns}')\n",
    "print(f'Shape: {full_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the first several observations to get a sense for the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.996525Z",
     "start_time": "2019-04-07T08:45:08.249Z"
    }
   },
   "outputs": [],
   "source": [
    "full_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to get a high-level summary of each of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.998527Z",
     "start_time": "2019-04-07T08:45:08.415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Small function to give us a bird's-eye summary of the data\n",
    "def five_num_summary(df, column):\n",
    "    print(f'Column: {column:<35} | ' +\n",
    "          f'Max value: {np.max(df[column]):>6} | ' + \n",
    "          f'Min value: {np.min(df[column]):>7.2f} | ' +\n",
    "          f'Mean: {np.mean(df[column]):>7.2f} | ' +\n",
    "          f'Median: {np.median(df[column]):>7.2f}')\n",
    "\n",
    "for col_name in full_features.columns:\n",
    "    five_num_summary(full_features, col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Label Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful for us to understand whether we have an imbalanced dataset (i.e., one where certain labels/categories are overrepresented relative to others.) Here we'll quickly describe our training and test labels and just make sure our classes are balanced. We can do this both graphically and numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Colorize these columns by label? Do we need both train and test graphs?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.001528Z",
     "start_time": "2019-04-07T08:45:09.065Z"
    }
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3), sharey=True)\n",
    "bins = np.arange(8) + 0.5\n",
    "ax1.hist(y_train, bins, width = 0.8)\n",
    "ax1.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax1.set_title('Labels')\n",
    "ax2.hist(y_test, bins, width = 0.8)\n",
    "ax2.set_title('Test Labels')\n",
    "plt.xticks(range(8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.003529Z",
     "start_time": "2019-04-07T08:45:09.073Z"
    }
   },
   "outputs": [],
   "source": [
    "print(stats.describe(full_labels))\n",
    "print(stats.describe(y_test))\n",
    "for i in range(0, 8):\n",
    "        print(f'i = {i}: Train Ct: {(full_labels==i).sum():>5} | Test Ct: {(y_test==i).sum():>5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our classes are quite well-balanced in both our training data and the test data.\n",
    "\n",
    "This is good both because we will not need to deliberately compensate for imbalances and because our model will be unable to achieve reasonable performance simply by guessing the modal category. (Doing so would give accuracy on the training set of 1741/12096 = 0.145, and then accuracy on the test set of 411/3024 = 0.136.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Change this to be a visual representation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.005529Z",
     "start_time": "2019-04-07T08:45:09.547Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(full_features['Elevation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.010528Z",
     "start_time": "2019-04-07T08:45:09.553Z"
    }
   },
   "outputs": [],
   "source": [
    "N = len(full_features['Aspect'])\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:47:58.813936Z",
     "start_time": "2019-04-07T10:47:53.101745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapting code from https://stackoverflow.com/questions/22562364/circular-histogram-for-python\n",
    "\n",
    "# Start from the center of the disc\n",
    "bottom = 0\n",
    "\n",
    "# Break the arc up into 10-degree increments\n",
    "theta = range(0, 360, 10)\n",
    "\n",
    "# Bar height is equal to the number of `Aspect` values between X and X + 10\n",
    "binned_aspects = np.digitize(full_features['Aspect'], theta)\n",
    "radii = [sum(binned_aspects==x) for x in range(0, len(theta))]\n",
    "\n",
    "# Divide arc evenly between Xbands for bar width\n",
    "N = 36  # b/c 10-degree arcs\n",
    "width = (2 * np.pi) / N\n",
    "\n",
    "max_height = max(radii) + 10 # Highest\n",
    "ax = plt.subplot(111, polar=True)\n",
    "bars = ax.bar(theta, radii, width=width, bottom=bottom)\n",
    "\n",
    "ax.set_theta_zero_location('N') # 0 degrees North is up\n",
    "ax.set_theta_direction(-1) # Go clockwise\n",
    "\n",
    "# Use custom colors and opacity\n",
    "for r, bar in zip(radii, bars):\n",
    "#     bar.set_facecolor(plt.cm.jet(r / 10.))\n",
    "    bar.set_facecolor('green')\n",
    "    bar.set_alpha(0.8)\n",
    "\n",
    "plt.title('Distribution of \\'Aspect\\' values in 10-degree increments\\n',\n",
    "         fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing of note is that the `Soil_Type7` and `Soil_Type15` are never true, so this feature tells us nothing.  These features should be removed before any modeling is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.014528Z",
     "start_time": "2019-04-07T08:45:09.803Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = np.arange(0, 360, 10)\n",
    "cut = [0, 45, 90, 135, 180, 225, 270, 315, 360]\n",
    "\n",
    "print(bins)\n",
    "pd.cut(bins, cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.016527Z",
     "start_time": "2019-04-07T08:45:09.809Z"
    }
   },
   "outputs": [],
   "source": [
    "full_data['Total_Hillshade'] = full_data[['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']].sum(axis=1)\n",
    "full_data[['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Total_Hillshade']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:18:15.186610Z",
     "start_time": "2019-04-07T12:18:14.649610Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make 3D scatterplot to explore water, elevation, and hillshade concurrently\n",
    "\n",
    "%matplotlib qt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sparsifier = np.random.randint(0, full_features.shape[0], 5000)\n",
    "\n",
    "sparsified = full_features.iloc[sparsifier,:]\n",
    "sparse_labels = full_labels.iloc[sparsifier]\n",
    "# print(f'Length of sparsified dataset\\n: {sparsified}')\n",
    "\n",
    "full_features['Euclidean_distance_to_water'] = np.sqrt(full_features['Horizontal_Distance_To_Hydrology']**2 + full_features['Vertical_Distance_To_Hydrology']**2)\n",
    "dist_to_water = sparsified['Euclidean_distance_to_water']\n",
    "altitude = sparsified['Elevation']\n",
    "hillshade = sparsified['Hillshade_3pm']\n",
    "color_dict = {1: '#A7C6ED', 2: '#BA0C2F', 3: '#651D32', 4: '#8C8985',\n",
    "              5: '#212721', 6: '#002F6C', 7: '#FFC000'}\n",
    "coloration = [color_dict[x] for x in sparse_labels]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(hillshade, dist_to_water, altitude, color=coloration, alpha=0.6)\n",
    "# ax.title('Forest cover categorization\\nby distance to water and hillshade')\n",
    "ax.view_init(30, 115)\n",
    "# mouse_init(rotate_btn=1, zoom_btn=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:55:30.071405Z",
     "start_time": "2019-04-07T10:55:28.530404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "data = sparsified[['Hillshade_3pm', 'Hillshade_9am', 'Aspect']]\n",
    "X = pca.fit_transform(data)\n",
    "coloration = [color_dict[x] for x in sparse_labels]\n",
    "print(data.shape, X.shape, sparsified['Elevation'].shape)\n",
    "plt.scatter(sparsified['Elevation'], X, color=coloration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:44:35.499287Z",
     "start_time": "2019-04-07T10:44:35.475290Z"
    }
   },
   "outputs": [],
   "source": [
    "fire_X = np.array(sparsified[['Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Roadways']])\n",
    "# fire_Y = np.array(sparsified['Horizontal_Distance_To_Roadways']).reshape(-1, 1)\n",
    "fire_Y = sparse_labels\n",
    "fire_X.shape, fire_Y.shape\n",
    "\n",
    "regressor = LinearRegression().fit(fire_X, fire_Y)\n",
    "regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:45:28.499760Z",
     "start_time": "2019-04-07T10:45:27.218260Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(sparsified['Horizontal_Distance_To_Fire_Points'], sparsified['Horizontal_Distance_To_Roadways'], color=coloration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:46:13.632204Z",
     "start_time": "2019-04-07T10:46:12.003381Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(sparsified['Elevation'], sparsified['Euclidean_distance_to_water'],color=coloration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:51:01.707463Z",
     "start_time": "2019-04-07T10:50:57.657646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapting code from https://stackoverflow.com/questions/22562364/circular-histogram-for-python\n",
    "# Start from the center of the disc\n",
    "bottom = 0\n",
    "\n",
    "# Break the arc up into 10-degree increments\n",
    "theta = range(0, 360, 10)\n",
    "\n",
    "# Bar height is equal to the number of `Aspect` values between X and X + 10\n",
    "binned_aspects = np.digitize(full_features['Aspect'], theta)\n",
    "radii = [sum(binned_aspects==x) for x in range(0, len(theta))]\n",
    "\n",
    "# Divide arc evenly between Xbands for bar width\n",
    "N = 36  # b/c 10-degree arcs\n",
    "width = (2 * np.pi) / N\n",
    "\n",
    "max_height = max(radii) + 10 # Highest\n",
    "ax = plt.subplot(111, polar=True)\n",
    "bars = ax.bar(theta, radii, width=width, bottom=bottom)\n",
    "\n",
    "ax.set_theta_zero_location('N') # 0 degrees North is up\n",
    "ax.set_theta_direction(-1) # Go clockwise\n",
    "\n",
    "# Use custom colors and opacity\n",
    "for r, bar in zip(radii, bars):\n",
    "#     bar.set_facecolor(plt.cm.jet(r / 10.))\n",
    "    bar.set_facecolor('green')\n",
    "    bar.set_alpha(0.8)\n",
    "\n",
    "plt.title('Distribution of \\'Aspect\\' values in 10-degree increments\\n',\n",
    "         fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:54:35.387984Z",
     "start_time": "2019-04-07T10:54:34.065915Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(sparsified['Aspect'], sparsified['Hillshade_9am'], color=coloration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:55:01.726655Z",
     "start_time": "2019-04-07T10:55:00.365606Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(sparsified['Aspect'], sparsified['Hillshade_Noon'], color=coloration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:54:43.585824Z",
     "start_time": "2019-04-07T10:54:42.213659Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(sparsified['Aspect'], sparsified['Hillshade_3pm'], color=coloration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T10:57:08.487345Z",
     "start_time": "2019-04-07T10:57:06.987842Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(sparsified['Horizontal_Distance_To_Hydrology'], sparsified['Aspect'],color=coloration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T11:03:15.737305Z",
     "start_time": "2019-04-07T11:03:14.052461Z"
    }
   },
   "outputs": [],
   "source": [
    "fig.savefig('imgs/3dSparsifiedElevationHydrologyHillshade.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:16:16.764296Z",
     "start_time": "2019-04-07T12:16:16.748295Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_scatter(X, Y, color_scheme, alpha=1, title='MissingTitle', save=None, verbose=False):\n",
    "    plt.scatter(X, Y, color=color_scheme, alpha=alpha)\n",
    "    plt.title(title)\n",
    "    lgd = plt.legend(handles=[cat_1, cat_2, cat_3, cat_4, cat_5, cat_6, cat_7],\n",
    "           bbox_to_anchor=(1, 1),\n",
    "           bbox_transform=plt.gcf().transFigure)\n",
    "    if save is not None:\n",
    "        plt.savefig(save, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "        if verbose:\n",
    "            print(f'Figure saved to {save}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:17:39.167918Z",
     "start_time": "2019-04-07T12:16:20.959056Z"
    }
   },
   "outputs": [],
   "source": [
    "Xs = ['Elevation', 'Aspect', 'Euclidean_distance_to_water',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n",
    "Ys = ['Elevation', 'Aspect', 'Euclidean_distance_to_water',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n",
    "\n",
    "for x in Xs:\n",
    "    for y in Ys:\n",
    "        make_scatter(sparsified[x], sparsified[y], coloration, 0.7,\n",
    "                     str(f'{x} vs. {y}'), str(f'imgs/scatterplot{x}_vs_{y}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T11:32:46.963981Z",
     "start_time": "2019-04-07T11:31:47.527330Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(sparsified[['Elevation','Aspect', 'Euclidean_distance_to_water',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']], figsize=(16, 16), color=coloration)\n",
    "plt.savefig('imgs/scatterplotMatrixElevationAspectWaterHillshade.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:00:54.238935Z",
     "start_time": "2019-04-07T12:00:53.419100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=1)\n",
    "data = sparsified[['Hillshade_3pm', 'Hillshade_9am', 'Aspect']]\n",
    "X = pca.fit_transform(data)\n",
    "coloration = [color_dict[x] for x in sparse_labels]\n",
    "print(data.shape, X.shape, sparsified['Elevation'].shape)\n",
    "plt.scatter(sparsified['Elevation'], X, color=coloration)\n",
    "cat_1 = mpatches.Patch(color=color_dict[1], label='Spruce/Fir')\n",
    "cat_2 = mpatches.Patch(color=color_dict[2], label='Lodgepole Pine')\n",
    "cat_3 = mpatches.Patch(color=color_dict[3], label='Ponderosa Pine')\n",
    "cat_4 = mpatches.Patch(color=color_dict[4], label='Cottonwood/Willow')\n",
    "cat_5 = mpatches.Patch(color=color_dict[5], label='Aspen')\n",
    "cat_6 = mpatches.Patch(color=color_dict[6], label='Douglas Fir')\n",
    "cat_7 = mpatches.Patch(color=color_dict[7], label='Krummholz')\n",
    "# plt.legend(handles=[cat_1, cat_2, cat_3, cat_4, cat_5, cat_6, cat_7])\n",
    "plt.legend(handles=[cat_1, cat_2, cat_3, cat_4, cat_5, cat_6, cat_7],\n",
    "           bbox_to_anchor=(1, 1),\n",
    "           bbox_transform=plt.gcf().transFigure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs - Elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
