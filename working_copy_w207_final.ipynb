{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Spring 2019 Final Project\n",
    "## Kaggle Competition: Forest Cover Prediction\n",
    "**Pierce Coggins, Jake Mitchell, Debasish Mukhopadhyay, and Tim Slade**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents/Section Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction](#introduction)\n",
    "  - In which we discuss the problem and why it matters\n",
    "  - [Housekeeping](#housekeeping)\n",
    "    - In which we deal with basic prep and setup issues\n",
    "- [About the Data](#aboutTheData)\n",
    "  - EDA, charts, data cleaning\n",
    "- [Feature Engineering](#featureEngineering)\n",
    "  - Describe a basic model that we will use to test the usefulness of new features (LR or NB)\n",
    "  - Normalization\n",
    "  - Each added or removed feature\n",
    "- [Models](#models)\n",
    "  - Maybe choose 4 to test out?  Don't want this section to get too lengthy, and each model should be covered in some detail\n",
    "- [Results](#results)\n",
    "  - What went well, what went poorly\n",
    "  - Final comparison of models on test data\n",
    "- [Conclusion](#conclusion)\n",
    "- [Annexes](#annexA)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, we will attempt to predict the forest cover type (defined as the predominant type of tree cover) for a given area of land in Colorado given only cartographic variables as inputs.  This problem and dataset were initially posted as a Kaggle competition in 2015.  We have chosen to tackle this problem as it allows for many different machine learning techniques to be attempted and explored.  The report will go through the process of building a capable model from data cleaning through final testing.\n",
    "\n",
    "The problem of understanding what type of vegetation is present in a difficult to access area is a surprisingly important one.  In this particular example the forests of Colorado are very diverse, and each type of tree cover has its own benefits and dangers.  For example, many of the pine trees in Colorado are susceptible to the [mountain pine beetle](https://csfs.colostate.edu/forest-management/common-forest-insects-diseases/mountain-pine-beetle/), while the Spruce and Fir trees are relatively safe from the beetles.  Without directly going to every location in the mountains of Colorado, it is very difficult to distinguish these types of trees as they look very similar from the air.  It is relatively easy to get cartographic data for a large swath of the mountains, however, and if it is possible to accurately predict the tree type from the cartographic information alone then all of the Colorado forest could be mapped by likely forest cover type. That information would be invaluable to firefighters and forest service personnel to direct their efforts where it will have the most impact.\n",
    "\n",
    "If you would like to learn more about the problem or try for yourself, all information and data can be found from the kaggle competition:<br>[Kaggle's Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"housekeeping\"></a>\n",
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries, Helper Functions, and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T17:45:33.770078Z",
     "start_time": "2019-04-08T17:45:21.938324Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %matplotlib inline\n",
    "# %matplotlib notebook\n",
    "%matplotlib qt\n",
    "\n",
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "# Plotting and printing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "from matplotlib.image import imread\n",
    "import pprint\n",
    "\n",
    "# Model-building libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler, StandardScaler, RobustScaler, Normalizer, scale\n",
    "\n",
    "# SK-learn libraries for learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Run the helper functions notebook\n",
    "%run w207_final_helper_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forest cover types we aim to predict are bundled with the features used to predict them. Our first step is therefore to separate them out, lest we accidentally let our models peek at the outcomes. We also want to split the dataset into _train_ and _test_ subsets; this will give us insight into how well our chosen models and parameters will perform against out-of-sample data.\n",
    "\n",
    "The original dataset contained 15,120 observations. We will train our models on 90% of the data and hold out 10% for testing. We thus expect to have approximately 0.9 * 15,120 = 13,608 observations in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T17:45:57.918825Z",
     "start_time": "2019-04-08T17:45:57.471558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                 | Observations |  Features  |\n",
      "----------------------------------------------\n",
      "Training dataset |    13608     |     54     |\n",
      "Training labels  |    13608     |     --     |\n",
      "  Test dataset   |     1512     |     54     |\n",
      "  Test labels    |     1512     |     --     |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "full_data = pd.DataFrame.from_csv('./train.csv')\n",
    "full_data.shape\n",
    "\n",
    "# Separating out the labels\n",
    "full_labels = full_data['Cover_Type']\n",
    "full_features = full_data.drop('Cover_Type', axis=1)\n",
    "\n",
    "# Setting seed so we get consistent results from our splitting\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(full_features, full_labels, test_size=0.10)\n",
    "\n",
    "# Verifying our data shapes are as expected\n",
    "print(f'''\n",
    "{'':^16} | {'Observations':^12} | {'Features':^10} |\n",
    "{'-'*46}\n",
    "{'Training dataset':^16} | {X_train.shape[0]:^12} | {X_train.shape[1]:^10} |\n",
    "{'Training labels':^16} | {y_train.shape[0]:^12} | {'--':^10} |\n",
    "{'Test dataset':^16} | {X_test.shape[0]:^12} | {X_test.shape[1]:^10} |\n",
    "{'Test labels':^16} | {y_test.shape[0]:^12} | {'--':^10} |\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aboutTheData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aboutTheData\"></a>\n",
    "# About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from several wilderness areas in northern Colorado, specifically the Rawah Wilderness Area, Neota Wilderness Area, Comanche Peak Wilderness Area, and the Cache la Poudre Wilderness Area.  These are all fairly remote areas of Colorado, which may be why they were chosen; there is less human influence in these places to complicate the prediction task.\n",
    "\n",
    "The features in the dataset are all cartographic measures of a 30x30m square plot of land.  We have 10 simple features. The 11th and 12th - `wilderness_area` and `soil_type` - are categorical variables which are represented as 4 and 40 binary columns respectively in our dataset. We therefore have a total of 10 + 4 + 40 = 54 features to work with.\n",
    "The list below contains a short description of each feature, including where relevant its range, median, and mean. (See [Annex A](#annexA) for the associated code and further discussion of the exploratory data analysis).\n",
    "\n",
    "- `Elevation`: _Elevation in meters_\n",
    "  - **Range**: 1863 to 3849 | **Mean**: 2749.3 | **Median**: 2752\n",
    "\n",
    "\n",
    "- `Aspect`: _Aspect in degrees azimuth. i.e., degrees clockwise from a line pointed at true North. So North = 0$^\\circ$, East = 90$^\\circ$, South = 180$^\\circ$, and West = 270$^\\circ$_\n",
    "  - **Range**: 0 to 360 | **Mean**: 156.7 | **Median**: 126.0\n",
    "\n",
    "\n",
    "- `Slope`: _Slope in degrees. 0$^\\circ$ would indicate a flat plane; greater values represent steeper slopes._\n",
    "  - **Range**: 0 to 52 | **Mean**: 16.5 | **Median**: 15.0 \n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Hydrology`: _Horizontal distance to nearest surface water features. Units unspecified._\n",
    "  - **Range**: 0 to 1343 | **Mean**: 227.2 | **Median**: 180 \n",
    "\n",
    "\n",
    "- `Vertical_Distance_To_Hydrology`: _Vertical distance to nearest surface water features. Units unspecified._\n",
    "  - **Range**: -146 to 554 | **Mean**: 51.1 | **Median**: 32.0\n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Roadways`: _Horizontal distance to nearest roadway. Units unspecified._\n",
    "  - **Range**: 0 to 6890 | **Mean**: 1714.0 | **Median**: 1316\n",
    "\n",
    "\n",
    "- `Hillshade_9am`: _(0 to 255 index) - Hillshade index at 9am, summer solstice_\n",
    "  - **Range**: 0 to 254 | **Mean**: 212.7 | **Median**: 220\n",
    "\n",
    "\n",
    "- `Hillshade_Noon`: _(0 to 255 index) - Hillshade index at noon, summer solstice_\n",
    "  - **Range**: 99 to 254 | **Mean**: 219.0 | **Median**: 223\n",
    "\n",
    "\n",
    "- `Hillshade_3pm`: _(0 to 255 index) - Hillshade index at 3pm, summer solstice_\n",
    "  - **Range**: 0 to 248 | **Mean**: 135.1 | **Median**: 138.0\n",
    "\n",
    "\n",
    "- `Horizontal_Distance_To_Fire_Points`: _Horizontal distance to nearest wildfire ignition points. Units unspecified._\n",
    "  - **Range**: 0 to 6993 | **Mean**: 1511.2 | **Median**: 1256 \n",
    "\n",
    "\n",
    "- `Wilderness_Area`: _(4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation_\n",
    "  - % of cases - **Area 1**: 24% || **Area 2**: 3% || **Area 3**: 42% || **Area 4**: 31% \n",
    "\n",
    "\n",
    "- `Soil_Type`: _(40 binary columns, 0 = absence or 1 = presence) - Soil type designation_\n",
    "  - The soil types descriptions can be found at the [Kaggle Competition Data Page](https://www.kaggle.com/c/forest-cover-type-prediction/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Exploration of the Challenge\n",
    "The label indicating our data's categorization is contained in the`Cover_Type` variable, and is split up into 7 different designations. While the tree species discussed in the Colorado State Forest Service's [_Colorado's Major Tree Species_](https://csfs.colostate.edu/colorado-trees/colorados-major-tree-species/) article do not map perfectly to these categories, the article provides some insights that may prove useful in our categorization exercise.\n",
    "\n",
    "#### <span style='color:blue'>Category 1</span>: 'Spruce/Fir'\n",
    "- Species that might fit into this category include the **Blue Spruce** (which thrives at an altitude of 6700-11500 ft in sandy soils near moisture), the **Engelmann Spruce** (8000-11000 ft, moist north-facing slopes), the **Subalpine Fir** (8000-12000 ft, cold high-elevation forests), and the **White Fir** (7900-10200 ft, moist soils in valleys).\n",
    "\n",
    "<center>Blue Spruce</center> | <center>Engelmann Spruce</center> | <center>Subalpine Fir</center> | <center>White Fir</center>\n",
    "- | - | - | -\n",
    "<img src=\"imgs/1_blue-spruce-tree.jpg\" alt=\"BlueSpruce\" style=\"width: 250px;\"/>  | <img src=\"imgs/1_engelmann-spruce.jpg\" alt=\"EngelmannSpruce\" style=\"width: 250px;\"/> | <img src=\"imgs/1_subalpine-fir.jpg\" alt=\"SubalpineFir\" style=\"width: 250px;\"/> | <img src=\"imgs/1_white-fir-tree.jpg\" alt=\"WhiteFir\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 2</span>: 'Lodgepole Pine' and <span style='color:blue'>Category 3</span>: 'Ponderosa Pine'\n",
    "- The **Lodgepole Pine** thrives in well-drained soils at high elevations (6000-11000 ft).\n",
    "- The **Ponderosa Pine** thrives in dry, nutrient-poor soils at elevations of 6300-9500 ft. It is often found with Douglas Firs.\n",
    "\n",
    "<center>Lodgepole Pine</center> | <center>Ponderosa Pine</center> |\n",
    "- |-|\n",
    "<img src=\"imgs/2_lodgepole-pine.jpg\" alt=\"LodgepolePine\" style=\"width: 250px;\"/> | <img src=\"imgs/3_ponderosa-pine.jpg\" alt=\"PonderosaPine\" style=\"width: 250px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 4</span>: 'Cottonwood/Willow'\n",
    "- Species that might fit into this category include the **Plains Cottonwood** (which thrives at altitudes of 3500-6500 ft near sources of water), the **Narrowleaf Cottonwood** (5000-8000 ft, moist soils along streams), and the **Peachleaf Willow** (3500-7500 ft, near water sources).\n",
    "\n",
    "<center>Plains Cottonwood</center> | <center>Narrowleaf Cottonwood</center> | <center>Peachleaf Willow</center> |\n",
    "- |- |- |\n",
    "<img src=\"imgs/4_plains-cottonwood.jpg\" alt=\"PlainsCottonwood\" style=\"width: 250px;\"/> |<img src=\"imgs/4_narrowleaf-cottonwood.jpg\" alt=\"NarrowleafCottonwood\" style=\"width: 250px;\"/> |<img src=\"imgs/4_peachleaf-willow.jpg\" alt=\"PeachleafWillow\" style=\"width: 250px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 5</span>: 'Aspen' and <span style='color:blue'>Category 6</span>: 'Douglas Fir'\n",
    "- The **Quaking Aspen** thrives at altitudes of 6500-11500 ft. While it can be in many soil types, it is especially found on sandy and gravelly slopes.\n",
    "- The **Douglas Fir** thrives at altitudes of 6000-9500 ft in rocky soils of moist northern slopes.\n",
    "\n",
    "<center>Quaking Aspen</center> | <center>Douglas Fir</center> |\n",
    "- | - |\n",
    "<img src=\"imgs/5_aspen.jpg\" alt=\"QuakingAspen\" style=\"width: 250px;\"/> | <img src=\"imgs/6_douglas-fir.jpg\" alt=\"DouglasFir\" style=\"width: 250px;\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Category 7</span>: 'Krummholz'\n",
    "- Interestingly, _krummholz_ is not a species of tree; it is a type of tree formation (which can emerge among various tree species) that results from consistent long-term exposure to strong, cold winds. Per [Wikipedia](https://en.wikipedia.org/wiki/Krummholz), Subalpine Fir and Engelmann Spruce are often associated with Krummholz conditions (as is Lodgepole Pine, although that is more common in British Columbia).\n",
    "\n",
    "<center>Krummholz Banner Tree</center> | <center>Krummholz White Pine</center> | <center>Krummholz Bristlecone</center> \n",
    "- |- |- |\n",
    "<img src=\"imgs/7_krummholz-banner-tree.jpg\" alt=\"KrummholzBannerTree (Photo credit to John Spooner - flickr.com, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=5007578)\" style=\"width: 250px;\"/> | <img src=\"imgs/7_krummholz-white-pine.jpg\" alt=\"KrummholzWhitePine (Photo credit to Walter Siegmund [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)] https://commons.wikimedia.org/wiki/File:Pinus_albicaulis_7872.JPG\" style=\"width: 350px;\"/> |  <img src=\"imgs/7_krummholz-windswept-bristlecone.jpg\" alt=\"KrummholzBristlecone\" style=\"width: 400px;\"/> | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do we start?\n",
    "\n",
    "The brief descriptions we've seen already suggest some avenues of exploration: altitude ranges and access to water seem to be of primary importance.\n",
    "\n",
    "#### What can we learn from elevation alone?\n",
    "\n",
    "One place to begin would be to plot out the idealized elevation ranges within which the various tree species thrive. There may be certain elevations where certain tree species would be far more prevalent than others. The graph below illustrates the ranges in which the species of trees discussed the Colorado State Forest Service's [_Colorado's Major Tree Species_](https://csfs.colostate.edu/colorado-trees/colorados-major-tree-species/) thrive, per the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T16:28:41.024009Z",
     "start_time": "2019-04-06T16:28:40.675304Z"
    }
   },
   "source": [
    "<img src=\"imgs/altitudeRanges4.png\" alt=\"ElevationRangesIdealized\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that lower elevations would be strongly suggestive of the `Cottonwood/Willow` `Cover_Type`, while higher elevations might be more suggestive of the `Spruce/Fir`, `Lodgepole Pine`, `Aspen`, and `Krummholz` `Cover_Type`s. The graph above is based upon idealized data from outside sources, though, and our actual dataset might tell a different story. The graphs below present the observed _elevation_ ranges and quartiles by `Cover_Type` in our data.\n",
    "\n",
    "| <center>Elevation Ranges</center> | <center>Elevation Quartiles</center>\n",
    "|-|-\n",
    "|<img src=\"imgs/elevationRanges.png\" alt=\"ElevationRanges\" style=\"width: 600px;\"/> |<img src=\"imgs/elevationQuartiles.png\" alt=\"ElevationQuartiles\" style=\"width: 600px;\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the ranges, our dataset appears to differ from the idealized one in that the `Cottonwood/Willow` `Cover_Type` does not seem to occur at markedly lower elevations. When looking at the quartiles, though, patterns emerge that appear similar to what we would expect from the idealized presentation: `Cottonwood/Willow` tends to cluster at lower elevations, with the higher elevations dominated by `Spruce/Fir` and `Krummholz` cover types.\n",
    "\n",
    "The separations are surprisingly clean, suggesting that `Elevation` will be a powerful feature in our models. It might be especially powerful if we could develop a method to cluster the altitudes into the interquartile ranges presented in the model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we bring water into the picture?\n",
    "The other feature that the article suggests might be highly salient is moisture. How does the picture evolve if we add a measure of the distance to water to the mix?\n",
    "\n",
    "The graph below is a scatterplot of the Euclidean distance (derived from the `Horizontal_Distance_To_Hydrology` and `Vertical_Distance_To_Hydrology` features) and the `Elevation`, with data points colored by the `Cover_Type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T17:10:56.612522Z",
     "start_time": "2019-04-07T17:10:56.592521Z"
    }
   },
   "source": [
    "<img src=\"imgs/hydrologyAndElevationScatter.png\" alt=\"HydrologyAndElevationScatter\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance to hydrology appears to be informative: `Cover_Type`s 3, 4, and 6 are essentially not found when the distance to water exceeds 750. That said, it remains clear that `Elevation` is the predominant distinguishing feature.\n",
    "\n",
    "#### What if we consider exposure to sunlight and wind?\n",
    "From a layperson's perspective, the amount of sunlight to which a given plot of land is exposed would seem likely to influence the vegetation which thrives there. In our dataset, the `Hillshade` variables encode this information.\n",
    "\n",
    "The plot below compares the 1st quartile, median, and 3rd quartiles for each measure of `Hillshade` for each category of `Cover_Type`.\n",
    "\n",
    "<img src=\"imgs/hillshadeQuartiles.png\" alt=\"HillshadeQuartiles\" style=\"width: 600px;\"/>\n",
    "\n",
    "While the median `Hillshade` values appear to vary a little across categories in the morning and afternoon, the interquartile range largely overlaps across categories. The overall impression is that `Hillshade` is unlikely to be determinative on its own.\n",
    "\n",
    "Exposure to sunlight and wind would also be affected by the `Aspect`, which is essentially the compass direction (0$^\\circ$ is true North, 90$^\\circ$ is East, 180$^\\circ$ is South, 270$^\\circ$ is West) the plot is facing. While the exact nature of the interaction between these features may not be clear *a priori*, we can attempt to collapse the effect into a single feature by taking the first principal component of the `Hillshade_9am` and `Hillshade_3pm` features with the `Aspect` feature.\n",
    "\n",
    "The graph below plots this first principal component against `Elevation`, as we already know `Elevation` is strongly informative.\n",
    "\n",
    "<img src=\"imgs/hillshadeAspectPcaScatter.png\" alt=\"hillshadeAspectPcaScatter\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns we see are weak at best. While the `Douglas Fir` category appears to be more prevalent for greater and lesser values of this first principal component, and the `Ponderosa Pine` appears to be slightly more prevalent nearer to zero, it is clear that the `Elevation` remains the dominant feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the 'Kitchen Sink' approach?\n",
    "\n",
    "So far we've examined `Elevation`, `Hydrology`, `Aspect`, and `Hillshade` features on the basis of the write-ups regarding the various tree species. But what if we just took a look at all of our key features and how they relate to one another?\n",
    "\n",
    "The graph below is a scatterplot matrix incorporating all of the raw simple features in our data, as well as the `Euclidean_Distance_To_Hydrology` feature we composed from the horizontal and vertical distances to hydrology.\n",
    "\n",
    "<img src=\"imgs/scatterplotMatrixElevationAspectWaterHillshade.png\" alt=\"scatterplotMatrixElevationAspectWaterHillshade\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `Elevation` remains the feature that seems to provide the cleanest separation between `Cover_Type`s, two additional features seem to perform pretty well at discriminating the `Lodgepole Pine`s: `Horizontal_Distance_To_Roadways` and `Horizontal_Distance_To_Fire_Points`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "While exploring the data (see [Annex A](#annexA)), we noted that the `Soil_Type7` and `Soil_Type15` variables are never true. Because there is no variation in this feature, it contributes nothing to any of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T17:48:50.110694Z",
     "start_time": "2019-04-08T17:48:50.097737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing uninformative features\n",
    "full_features = full_features.drop(['Soil_Type7', 'Soil_Type15'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"featureEngineering\"></a>\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful machine learning projects often depend heavily on feature engineering. The most important feature in a dataset may be a latent one - that is, 'hidden' behind other features which serves as proxies for it. In such a case, the latent feature needs to be explicitly extracted. While we are exploring the potential of various synthetic/constructed features, we will also try to remove original features which are proving uninformative. Doing so will reduce the noise passed into our models. We can keep the engineered and source datasets separate by creating a deep copy of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance to Hydrology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the [About the Data](#aboutTheData) section, the `Cover_Type`s can be visually broken up based on their distance to hydrology, both horizontally and vertically.  By combining the features into a single feature, we can reduce the overall number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation of Hydrology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elevation and Hydrology are very important features when it comes to predicting the `Cover_Type` of an area.  By subtracting the vertical distance to hydrology from the elevation, we can find what the elevation of the hydrology itself it.  This may prove useful by providing a feature that would be able to discern an alpine lake vs a valley stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Distance to Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the [About the Data](#aboutTheData) section, the distance metrics group the data pretty well for classification.  We can engineer a new feature that incorporates the mean distance to hydrology, fire points, and roadways - the latter two features providing a fair approximation of an area's remoteness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set features 40 different types of soils.  When compared to the 7 possible labels, this number of soil types seems a bit extreme.  Different types of trees favor more rocky soils, and so combining all of the stony soil types into a single feature will allow a model to more easily pick up on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hillshade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T18:01:20.170251Z",
     "start_time": "2019-04-08T18:01:20.136734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>212.704299</td>\n",
       "      <td>135.091997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.561287</td>\n",
       "      <td>45.895189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>196.000000</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>220.000000</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>235.000000</td>\n",
       "      <td>167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>254.000000</td>\n",
       "      <td>248.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hillshade_9am  Hillshade_3pm\n",
       "count   15120.000000   15120.000000\n",
       "mean      212.704299     135.091997\n",
       "std        30.561287      45.895189\n",
       "min         0.000000       0.000000\n",
       "25%       196.000000     106.000000\n",
       "50%       220.000000     138.000000\n",
       "75%       235.000000     167.000000\n",
       "max       254.000000     248.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_features[['Hillshade_9am', 'Hillshade_3pm']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Data Assumptions Made\n",
    "\n",
    "One thing to notice about the data is that the `Hillshade_9am` and `Hillshade_3pm` features are missing several values.  We choose to replace these values with the median value for those features.  This will allow the areas with missing values to be more accurately classified as they no longer have un-usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T18:03:31.853815Z",
     "start_time": "2019-04-08T18:03:29.248556Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "engineered_features = pd.DataFrame.copy(full_features)\n",
    "engineered_features['Euclidean_Distance_To_Hydrology'] = engineered_features.apply(lambda row: math.sqrt(row.Horizontal_Distance_To_Hydrology**2 + row.Vertical_Distance_To_Hydrology**2), axis=1)\n",
    "engineered_features['Elevation_Of_Hydrology'] = engineered_features['Elevation']-engineered_features['Vertical_Distance_To_Hydrology']\n",
    "engineered_features['Mean_Distance_To_Feature'] = (engineered_features['Horizontal_Distance_To_Hydrology']+engineered_features['Horizontal_Distance_To_Roadways']+engineered_features['Horizontal_Distance_To_Fire_Points'])/3\n",
    "engineered_features['Stony'] = engineered_features[['Soil_Type1', 'Soil_Type2', 'Soil_Type6', 'Soil_Type9', 'Soil_Type12', 'Soil_Type18', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', ]].any(axis=1)\n",
    "median_hillshade_9am = np.median(engineered_features['Hillshade_9am'])\n",
    "engineered_features['Hillshade_9am'] = engineered_features.apply(lambda row: median_hillshade_9am if row.Hillshade_9am == 0 else row.Hillshade_9am, axis=1)\n",
    "median_hillshade_3pm = np.median(engineered_features['Hillshade_3pm'])\n",
    "engineered_features['Hillshade_3pm'] = engineered_features.apply(lambda row: median_hillshade_3pm if row.Hillshade_3pm == 0 else row.Hillshade_3pm, axis=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "e_X_train, e_X_test, e_y_train, e_y_test = train_test_split(engineered_features, full_labels, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Test Feature Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without _a priori_ knowledge of how the interplay between soil types, topography, hydrology, etc. affects forest cover, we need a way to view the performance of new features.  As such we will use a simple Gaussian Naive Bayes model to do predictions, and quanitify the results using cross-validation.  We will be tracking performance across precision, recall, and f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naïve Bayes\n",
    "\n",
    "One reasonable place to begin might be a Naïve Bayes classifier. While it is unlikely that all of the features at our disposal are _strictly_ independent, we may be able to relax the assumption of independence enough to explore how a NB model performs.\n",
    "\n",
    "We don't want a Bernoulli NB model: our features are not uniformly binary-valued. We also don't want a Multinomial NB model: per the documentation, it assumes integer feature counts. A Gaussian NB, on the other hand, might work well. While it assumes that the likelihoods of the features are Gaussian - and this is not necessarily strictly the case - it may be worth trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T18:03:24.390692Z",
     "start_time": "2019-04-08T18:03:24.367690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Base Data GaussianNB, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.72 | recall:  0.50 | f1-score:  0.59 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.13 | recall:  0.73 | f1-score:  0.22 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.73 | recall:  0.43 | f1-score:  0.54 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.92 | recall:  0.68 | f1-score:  0.78 | \n",
      "\t\t\tAspen                | precision:  0.73 | recall:  0.59 | f1-score:  0.65 | \n",
      "\t\t\tDouglas Fir          | precision:  0.07 | recall:  0.77 | f1-score:  0.12 | \n",
      "\t\t\tKrummholz            | precision:  0.82 | recall:  0.86 | f1-score:  0.84 | \n",
      "\t\t\tmacro avg            | precision:  0.59 | recall:  0.65 | f1-score:  0.54 | \n",
      "\t\t\tmicro avg            | precision:  0.59 | recall:  0.59 | f1-score:  0.59 | \n",
      "\t\t\tweighted avg         | precision:  0.76 | recall:  0.59 | f1-score:  0.65 | \n",
      "\n",
      "Model: Base Data GaussianNB, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.73 | recall:  0.53 | f1-score:  0.61 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.16 | recall:  0.73 | f1-score:  0.26 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.73 | recall:  0.44 | f1-score:  0.55 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.93 | recall:  0.69 | f1-score:  0.79 | \n",
      "\t\t\tAspen                | precision:  0.79 | recall:  0.62 | f1-score:  0.69 | \n",
      "\t\t\tDouglas Fir          | precision:  0.08 | recall:  0.80 | f1-score:  0.14 | \n",
      "\t\t\tKrummholz            | precision:  0.86 | recall:  0.85 | f1-score:  0.85 | \n",
      "\t\t\tmacro avg            | precision:  0.61 | recall:  0.66 | f1-score:  0.56 | \n",
      "\t\t\tmicro avg            | precision:  0.61 | recall:  0.61 | f1-score:  0.61 | \n",
      "\t\t\tweighted avg         | precision:  0.77 | recall:  0.61 | f1-score:  0.66 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on the base data\n",
    "cross_validate_model(GaussianNB(), X_train, y_train, name='Base Data GaussianNB', verbose=True)\n",
    "\n",
    "# Testing on the engineered data\n",
    "cross_validate_model(GaussianNB(), e_X_train, e_y_train, name='Base Data GaussianNB', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we will use two metrics to determine how well a particular model performs, precision and recall.  All in all, just throwing a Gaussian Naive Bayes classifier at the data performed better than expected.  It achieved a 76% weighted precision across 5 fold cross validation.  \n",
    "\n",
    "The engineered features do not provide as much improvement as hoped.  They resulted in 1-2% improvements across all of the metrics.  One positive however is that the improved features seem to help the poorly classified labels more than the already well classified labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed Engineered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every feature that is engineered is a useful addition to the data set.  Randomly adding new features can add noise to the dataset without providing any new information.  We have listed the failed features below.  Some highlights include mountain width and prominence (from the `Elevation` and `Slope` features), and a few different ways to view the elevation of an area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineered_features['Elevation_Away_From_Hydrology'] = engineered_features['Elevation']-engineered_features['Horizontal_Distance_To_Hydrology']\n",
    "# engineered_features['Mountain_Width'] = engineered_features.apply(lambda row: row.Elevation/math.tan(math.radians(row.Slope+.1)), axis=1)\n",
    "# engineered_features['Mountain_Prominence'] = engineered_features.apply(lambda row: row.Elevation/math.sin(math.radians(row.Slope+.1)), axis=1)\n",
    "# engineered_features['Mean_Hillshade'] = engineered_features.apply(lambda row: (row.Hillshade_9am + row.Hillshade_Noon + row.Hillshade_3pm)/3, axis=1)\n",
    "# engineered_features['Morning_Hillshade'] = engineered_features.apply(lambda row: (row.Hillshade_9am * row.Hillshade_Noon), axis=1)\n",
    "# engineered_features['Norm_Horizontal_Distance_To_Hydrology'] = engineered_features['Horizontal_Distance_To_Hydrology']/(np.mean(engineered_features['Horizontal_Distance_To_Hydrology']))\n",
    "# engineered_features['Norm_Elevation'] = engineered_features['Elevation']/(np.mean(engineered_features['Elevation']))\n",
    "# engineered_features['Log_Elevation'] = engineered_features.apply(lambda row: math.log(row.Elevation), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is a very important aspect of preparing data for consumption by machine learning models.  It brings all of the features into a simlilar range, meaning that the models will not end up with widely varying coefficients due to the differing scales of features.  By ensuring all features exist within a given range, we eliminate the possibility that a feature could dominate the weighting and prediction process simply by virtue of having a range that is a few orders of magnitude greater than that of another (potentially more meaningful) feature.   We will experiment with several different types of standardization to see which is the most effective. Specifically we will test Min-Max scaling, standard scaling, robust scaling and sklearn's normalizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating standardization using K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with Naïve Bayes models is that they are more or less invariant to feature scaling, and therefore cannot be used when testing different standardization methods.  We will use the `KNearestClassifier` with *K = 3* when testing out our performance on scaled data.  In testing a variety of values for `K`, we found that *K = 3* consistently produced the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T18:08:11.887347Z",
     "start_time": "2019-04-08T18:08:11.869348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.66 | recall:  0.74 | f1-score:  0.70 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.58 | recall:  0.71 | f1-score:  0.64 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.74 | recall:  0.83 | f1-score:  0.78 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.96 | recall:  0.90 | f1-score:  0.93 | \n",
      "\t\t\tAspen                | precision:  0.96 | recall:  0.83 | f1-score:  0.89 | \n",
      "\t\t\tDouglas Fir          | precision:  0.85 | recall:  0.80 | f1-score:  0.82 | \n",
      "\t\t\tKrummholz            | precision:  0.97 | recall:  0.88 | f1-score:  0.92 | \n",
      "\t\t\tmacro avg            | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "\t\t\tmicro avg            | precision:  0.82 | recall:  0.82 | f1-score:  0.82 | \n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on the unscaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KNearestClassifier` performed remarkably well on the base data with 83% precision and 81% recall.  This is a remarkably good result for a very basic model.  However with such a low K, it is important to keep overfitting in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In brief: standardizing was unhelpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested standardization of our feature set using the following models:\n",
    "+ `MinMaxScaler` with ranges of [-1, 1] and [0, 1]\n",
    "+ `StandardScaler` with the range [0, 1]\n",
    "+ `RobustScaler`\n",
    "+ `Normalizer`\n",
    "\n",
    "The code block below demonstrates the effect of applying each of those standardization approaches. (Removing the `%%capture` line will allow the output to be generated.) Output is currently omitted for parsimony, however, because the central message was that no approach to standardization materially improved performance on the KNN test case. As a result we have also elided extended discussion of the way the various scalers function and the apparent (in)coherence of the transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T18:37:29.643857Z",
     "start_time": "2019-04-08T18:37:29.604320Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "mm_neg1_1_scaled_df = apply_scaler(MinMaxScaler(feature_range=(-1, 1)), e_X_train)\n",
    "print(\"MinMaxScaler [-1,1]\")\n",
    "mm_neg1_1_scaled_df.head(5)\n",
    "\n",
    "mm_0_1_scaled_df = apply_scaler(MinMaxScaler(feature_range=(0, 1)), e_X_train)\n",
    "print(\"MinMaxScaler [0,1]\")\n",
    "mm_0_1_scaled_df.head(5)\n",
    "\n",
    "standard_scaled_df = apply_scaler(StandardScaler(), e_X_train)\n",
    "print(\"StandardScaler [0,1]\")\n",
    "standard_scaled_df.head(5)\n",
    "\n",
    "r_scaled_df = apply_scaler(RobustScaler(), e_X_train)\n",
    "print(\"RobustScaler [0,1]\")\n",
    "r_scaled_df.head(5)\n",
    "\n",
    "n_scaled_df = apply_scaler(Normalizer(), e_X_train)\n",
    "print(n_scaled_df.shape)\n",
    "n_scaled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:45:02.057657Z",
     "start_time": "2019-04-07T08:45:01.261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, MinMax scaled [-1,1]\n",
      "\t\t\tweighted avg         | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "Model: 3 Nearest Neighbors, MinMax scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "Model: 3 Nearest Neighbors, Standard scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.81 | recall:  0.80 | f1-score:  0.80 | \n",
      "Model: 3 Nearest Neighbors, Robust scaled [0,1]\n",
      "\t\t\tweighted avg         | precision:  0.81 | recall:  0.80 | f1-score:  0.80 | \n",
      "Model: 3 Nearest Neighbors, Normalized\n",
      "\t\t\tweighted avg         | precision:  0.73 | recall:  0.70 | f1-score:  0.71 | \n",
      "Model: 3 Nearest Neighbors\n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n"
     ]
    }
   ],
   "source": [
    "# Testing on the [-1,1] scaled data again for reference\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), mm_neg1_1_scaled_df, e_y_train, name='3 Nearest Neighbors, MinMax scaled [-1,1]')\n",
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), mm_0_1_scaled_df, e_y_train, name='3 Nearest Neighbors, MinMax scaled [0,1]')\n",
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), standard_scaled_df, e_y_train, name='3 Nearest Neighbors, Standard scaled [0,1]')\n",
    "# Testing on the Robust scaled data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), r_scaled_df, e_y_train, name='3 Nearest Neighbors, Robust scaled [0,1]')\n",
    "# Testing on the Normalized data\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), n_scaled_df, e_y_train, name='3 Nearest Neighbors, Normalized')\n",
    "# Testing on the unscaled data again for reference\n",
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test is to determine how much the scaled features affect the KNN model, so we have included the previous basic KNearestNeighbors results for reference.  \n",
    "+ The range used when applying the `MinMaxScaler` does not materially affect the results.\n",
    "+ The `StandardScaler` actually performs slightly worse than the `MinMaxScaler` when used on a KNN-3 model.\n",
    "+ The `RobustScaler` performs approximately as well as the previous models.\n",
    "+ The `Normalizer` generated the worst results of all.\n",
    "\n",
    "Many of the sklearn models that we will deploy apply some built-in standardization before training. Since manually standardizing the features has not improved performance, the remainder of our exploration will use the unstandardized features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "\n",
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a useful model because it is quite interpretable (it is possible to extract the coefficients for individual features), and when given enough data can perform remarkably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.65 | recall:  0.62 | f1-score:  0.64 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.50 | recall:  0.58 | f1-score:  0.54 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.54 | recall:  0.59 | f1-score:  0.56 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.88 | recall:  0.79 | f1-score:  0.83 | \n",
      "\t\t\tAspen                | precision:  0.69 | recall:  0.62 | f1-score:  0.65 | \n",
      "\t\t\tDouglas Fir          | precision:  0.54 | recall:  0.56 | f1-score:  0.55 | \n",
      "\t\t\tKrummholz            | precision:  0.84 | recall:  0.87 | f1-score:  0.86 | \n",
      "\t\t\tmacro avg            | precision:  0.66 | recall:  0.66 | f1-score:  0.66 | \n",
      "\t\t\tmicro avg            | precision:  0.66 | recall:  0.66 | f1-score:  0.66 | \n",
      "\t\t\tweighted avg         | precision:  0.67 | recall:  0.66 | f1-score:  0.67 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate_model(LogisticRegression(), e_X_train, e_y_train, name='Logistic Regression', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic logistic regression gives us a baseline against which to compare other models. The basic one-vs-many `LogisticRegression` classifier achieved an average precision of ~0.67, meaning about 2/3 of its predictions are correct. None of the f1 scores were stellar (above 0.90), but neither were any as terrible as what we saw earlier with a base Naïve Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed K Nearest Neighbors above when testing how effective different forms of scaling were.  KNN is a good fit for this type of problem because there are a large number of examples relative to the number of classes.  This means that every new data point will have many 'neighbors' to choose from. We found that the ideal number of neighbors was three as there is enough data to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 3 Nearest Neighbors, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.66 | recall:  0.74 | f1-score:  0.70 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.58 | recall:  0.71 | f1-score:  0.64 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.74 | recall:  0.83 | f1-score:  0.78 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.96 | recall:  0.90 | f1-score:  0.93 | \n",
      "\t\t\tAspen                | precision:  0.96 | recall:  0.83 | f1-score:  0.89 | \n",
      "\t\t\tDouglas Fir          | precision:  0.85 | recall:  0.80 | f1-score:  0.82 | \n",
      "\t\t\tKrummholz            | precision:  0.97 | recall:  0.88 | f1-score:  0.92 | \n",
      "\t\t\tmacro avg            | precision:  0.82 | recall:  0.81 | f1-score:  0.81 | \n",
      "\t\t\tmicro avg            | precision:  0.82 | recall:  0.82 | f1-score:  0.82 | \n",
      "\t\t\tweighted avg         | precision:  0.84 | recall:  0.82 | f1-score:  0.82 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, name='3 Nearest Neighbors', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What KNN does well is perform relatively well across all of the labels.  The lowest precision score it achieves is 56% on `Lodgepole Pines`.  As we saw in the [About the Data](#aboutTheData) section, Lodgepole pine trees thrive in areas that can be covered by many types of trees.  This makes classifying them especially hard with KNN as the Lodgepole covered areas often have neighbors of other cover types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are a common tool in a data scientist's kit.  They generally perform well on datasets that are semi linearly separable, but they are very slow to train.  We will take a look at the efficacy of SVMs on this data set, as it may give an indication as to how close to linearly separable this data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:46:42.652719Z",
     "start_time": "2019-04-07T08:45:05.680175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: linearSVC, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.19 | recall:  0.28 | f1-score:  0.16 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.24 | recall:  0.27 | f1-score:  0.14 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.35 | recall:  0.30 | f1-score:  0.22 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.54 | recall:  0.75 | f1-score:  0.52 | \n",
      "\t\t\tAspen                | precision:  0.09 | recall:  0.35 | f1-score:  0.14 | \n",
      "\t\t\tDouglas Fir          | precision:  0.43 | recall:  0.20 | f1-score:  0.21 | \n",
      "\t\t\tKrummholz            | precision:  0.49 | recall:  0.58 | f1-score:  0.48 | \n",
      "\t\t\tmacro avg            | precision:  0.33 | recall:  0.39 | f1-score:  0.27 | \n",
      "\t\t\tmicro avg            | precision:  0.33 | recall:  0.33 | f1-score:  0.33 | \n",
      "\t\t\tweighted avg         | precision:  0.80 | recall:  0.33 | f1-score:  0.40 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic Linear Support Vector machine \n",
    "cross_validate_model(LinearSVC(), e_X_train, e_y_train, name='linearSVC', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard linearSVC produces very poor results.  It has extremely low recall, and predicts the `Aspen` and `Ponderosa Pine` categories very poorly.  We will take a look at how it does on scaled data, as SVC's generally require some standardization.  The recommended scaling is mean 0 var 1, but we will see how the existing ones do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.961514Z",
     "start_time": "2019-04-07T08:46:42.659723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: linearSVC, Unscaled\n",
      "\t\t\tweighted avg         | precision:  0.80 | recall:  0.33 | f1-score:  0.40 | \n",
      "Model: linearSVC, MinMax scaled [-1,1]\n",
      "\t\t\tweighted avg         | precision:  0.68 | recall:  0.67 | f1-score:  0.67 | \n",
      "Model: linearSVC, Scaled to mean=0, variance=1\n",
      "\t\t\tweighted avg         | precision:  0.68 | recall:  0.67 | f1-score:  0.67 | \n"
     ]
    }
   ],
   "source": [
    "# Basic Linear Support Vector machine \n",
    "cross_validate_model(LinearSVC(), e_X_train, e_y_train, name='linearSVC, Unscaled')\n",
    "cross_validate_model(LinearSVC(), mm_neg1_1_scaled_df, e_y_train, name='linearSVC, MinMax scaled [-1,1]')\n",
    "scaled_X_train = scale(e_X_train)\n",
    "scaled_X_train_df = pd.DataFrame(data=scaled_X_train,    # values\n",
    "                         columns=e_X_train.columns)  # 1st row as the column names\n",
    "cross_validate_model(LinearSVC(), scaled_X_train_df, e_y_train, name='linearSVC, Scaled to mean=0, variance=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that using scaled features made the performance of the `LinearSVC` even worse. That will be the end of the SVC strategy for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests can be extremely effective on datasets with a myriad of features that each contain a little information.  The base data set contains 54 features, 44 of which are binary, making building many of the trees quick and easy. In addition, Random Forest models are also fairly interpretable, as the most salient features can be extracted from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.979530Z",
     "start_time": "2019-04-07T08:45:06.407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForest, with 5 folds\n",
      "\t\t\tSpruce/Fir           | precision:  0.76 | recall:  0.75 | f1-score:  0.75 | \n",
      "\t\t\tLodgepole Pine       | precision:  0.66 | recall:  0.76 | f1-score:  0.70 | \n",
      "\t\t\tPonderosa Pine       | precision:  0.82 | recall:  0.80 | f1-score:  0.81 | \n",
      "\t\t\tCottonwood/Willow    | precision:  0.97 | recall:  0.93 | f1-score:  0.95 | \n",
      "\t\t\tAspen                | precision:  0.93 | recall:  0.89 | f1-score:  0.91 | \n",
      "\t\t\tDouglas Fir          | precision:  0.82 | recall:  0.84 | f1-score:  0.83 | \n",
      "\t\t\tKrummholz            | precision:  0.96 | recall:  0.94 | f1-score:  0.95 | \n",
      "\t\t\tmacro avg            | precision:  0.84 | recall:  0.84 | f1-score:  0.84 | \n",
      "\t\t\tmicro avg            | precision:  0.84 | recall:  0.84 | f1-score:  0.84 | \n",
      "\t\t\tweighted avg         | precision:  0.85 | recall:  0.84 | f1-score:  0.85 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate_model(RandomForestClassifier(), e_X_train, e_y_train, name='RandomForest', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base `RandomForestClassifier` performs quite well out of the box, achieving a respectable ~0.83 across all of our metrics.  Lets see if using standardized data makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.981526Z",
     "start_time": "2019-04-07T08:45:06.413Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e_mm_0_1_scaled_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-25fba2b86d99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing on the [0,1] scaled data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcross_validate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_mm_0_1_scaled_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RandomForest, MinMax scaled [0,1]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Testing on the unscaled data again for reference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcross_validate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RandomForest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e_mm_0_1_scaled_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing on the [0,1] scaled data\n",
    "cross_validate_model(RandomForestClassifier(n_estimators = 10), e_mm_0_1_scaled_df, e_y_train, name='RandomForest, MinMax scaled [0,1]')\n",
    "# Testing on the unscaled data again for reference\n",
    "cross_validate_model(RandomForestClassifier(n_estimators = 10), e_X_train, e_y_train, name='RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the scaled features did not have an effect on the performance of the model.  Random Forests are made up of trees that only deal with a few features at a time, and do not care if a feature goes from [0,1] or [0,10000].  The tree's decision boundaries are set based on whatever scale that particular feature is at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Decision Trees are a cousin to the Random Forests.  When a Random Forest is built, it builds many trees in parallel trying to maximize the information gain of each tree.  In Gradient Boosting, trees are made iteratively with each tree attempting to correct the errors of the previous one.  These tend to perform a little better than Random Forests, while maintaining their nice properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_validate_model(XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4), e_X_train, e_y_train, name=f'Gradient Boosted Decision Trees (XGBoost)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting has proven to be the most effective model so far, achieving the very high score of 0.88 across the board.  This is a pretty great result because classifying new data using a `XGBClassifier` is extremely quick despite the long time it takes to train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.986527Z",
     "start_time": "2019-04-07T08:45:06.427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Top 10 Features\n",
      "\n",
      "1. Elevation (0.162398)\n",
      "2. Elevation_Of_Hydrology (0.151600)\n",
      "3. Mean_Distance_To_Feature (0.070873)\n",
      "4. Horizontal_Distance_To_Roadways (0.070431)\n",
      "5. Euclidean_Distance_To_Hydrology (0.057154)\n",
      "6. Horizontal_Distance_To_Fire_Points (0.051782)\n",
      "7. Hillshade_Noon (0.036232)\n",
      "8. Aspect (0.035743)\n",
      "9. Hillshade_9am (0.035738)\n",
      "10. Vertical_Distance_To_Hydrology (0.035153)\n",
      "\n",
      "Mean Feature Importance 0.017857\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate Initial RF and identify most important features\n",
    "initialRF = RandomForestClassifier(n_estimators = 10)\n",
    "initialRF.fit(e_X_train, e_y_train)\n",
    "FeatImportance(initialRF, e_X_train, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look at the most important features we can see which variables are most effective in predicting the cover type.  As expected the `Elevation` of an area is the biggest predictor of the `Cover_Type`.  Following the elevation of an area, we see the distances to roadways, fire points, and water all having a large effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "In the above analysis, some of the models' performance as measured by precision and recall varied widely across cover types. Most of the models did well with `Cottonwood/Willow` and poorly with `Lodgepole Pine`. We might be able to improve our results by drawing upon the strengths of each different model and take a vote to maximize the overall performance.\n",
    "\n",
    "We use an ensemble model for this purpose. However, we also know that if our top model to date, Random Forest (RF), is more accurate than other models, there is a possibility that the noise level from the poorly performing models will reduce overall performance of the ensemble. To diagnose this, we write our code in such a way that we can also see inside the ensemble, to assess how each individual model performed.\n",
    "\n",
    "In addition to RF, we have also tested KNN, LinearSVC and Logistic Regression models. We should take the opportunity to check other classifiers and see if they can beat our RF model. It is also important to model an ensemble of these various models with and optimizing different parameters to see if we can find a better alternative model. This is a costly and time consuming approach. Executing a GridSearch with multiple model ensembles each with 2-3 parameters often takes hours of processing even on high-end machines with many CPU cores. We, therefore, individually tested and optimized parameters for addition models such as MLP Classifier, Ada Boost, Quadratic Discriminant Analysis and Gaussian Process Classifier. Addditionally, we also varied the hyperparameters for KNN and RF individually to see the best outcome in each model. Finally, we created an ensemble model with 9 models in a \"hard\" and a \"soft\" voting model to see if we can get the best outcome.  \n",
    "\n",
    "#### Results\n",
    "Initial result showed what we suspected. The overall effect of the ensemble models were lower than the random forest. This is because the other models were adding noise to the overall. We then started process of elimination to get the best possible combination. Our simple RF can only be beaten by an ensemble model composed of KNN and RF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tSpruce/Fir           | precision:  0.85 | recall:  0.75 | f1-score:  0.80 | \n",
      "\t\tLodgepole Pine       | precision:  0.62 | recall:  0.80 | f1-score:  0.70 | \n",
      "\t\tPonderosa Pine       | precision:  0.89 | recall:  0.83 | f1-score:  0.86 | \n",
      "\t\tCottonwood/Willow    | precision:  0.97 | recall:  0.90 | f1-score:  0.94 | \n",
      "\t\tAspen                | precision:  0.97 | recall:  0.91 | f1-score:  0.94 | \n",
      "\t\tDouglas Fir          | precision:  0.81 | recall:  0.96 | f1-score:  0.88 | \n",
      "\t\tKrummholz            | precision:  0.97 | recall:  0.95 | f1-score:  0.96 | \n",
      "\t\tmacro avg            | precision:  0.87 | recall:  0.87 | f1-score:  0.87 | \n",
      "\t\tmicro avg            | precision:  0.87 | recall:  0.87 | f1-score:  0.87 | \n",
      "\t\tFinal                | precision:  0.88 | recall:  0.87 | f1-score:  0.87 | \n",
      "Accuracy: 0.84 (+/- 0.01) [RandomForestClassifier]\n",
      "Accuracy: 0.61 (+/- 0.01) [KNeighborsClassifier]\n",
      "Accuracy: 0.80 (+/- 0.01) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "# Flooowing code has been tried out with multiple options \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=1)\n",
    "clf1 =LinearSVC()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GaussianNB()\n",
    "clf4 = KNeighborsClassifier()\n",
    "clf5 = SVC(gamma=2, C=1)\n",
    "clf6 = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "clf7 = MLPClassifier(alpha=1,activation = 'tanh',solver = 'lbfgs' )\n",
    "#clf8 = AdaBoostClassifier()\n",
    "clf8 = XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4)\n",
    "clf9 = QuadraticDiscriminantAnalysis()\n",
    "eclf = VotingClassifier(estimators=[\n",
    "#                                    ('lsvc', clf1)\n",
    "#                                    , \n",
    "#                                    ('rf', clf2)\n",
    "#                                    , ('gnb', clf3)\n",
    "#                                   , \n",
    "                                    ('knn', clf4)\n",
    "#                                   , ('svc', clf5)\n",
    "#                                   , ('gpc', clf6)\n",
    "#                                   , ('mlp', clf7)\n",
    "                                   , ('GB', clf8)\n",
    "#                                   , ('qda', clf9)\n",
    "                                   ], voting='hard')\n",
    "\n",
    "params = {\n",
    "    #    'lsvc__C': [1.0, 100.0], \n",
    "    \"knn__n_neighbors\":[3,5,7,],\"knn__weights\":[\"uniform\",\"distance\"],\n",
    "         # 'rf__n_estimators': [5, 10], \"rf__criterion\": [\"gini\", \"entropy\"], \"rf__min_samples_split\":[5,7,10]\n",
    "     }\n",
    "\n",
    "model = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "#model.fit(scaled_FX_train, Fy_train)\n",
    "#testPrediction = model.predict(scaled_FX_test)\n",
    "model.fit(e_X_train, e_y_train)\n",
    "testPrediction = model.predict(e_X_test)\n",
    "#model.fit(X_train, y_train)\n",
    "#testPrediction = model.predict(X_test)\n",
    "testReport = metrics.classification_report(testPrediction, e_y_test, output_dict=True)\n",
    "verbose= True\n",
    "#print(f'Model: {name}')\n",
    "reportFields = ['precision', 'recall', 'f1-score']\n",
    "fields = sorted(testReport.keys()) if verbose else ['weighted avg']\n",
    "fieldLabels = [label_names[field] if field in label_names.keys() else field for field in fields]\n",
    "fieldLabels[-1] = \"Final\"\n",
    "for i in range(len(fields)):\n",
    "    output = f'\\t\\t{fieldLabels[i]:<20} | '\n",
    "    for outputField in reportFields:\n",
    "        output += f'{outputField}: {np.mean(testReport[fields[i]][outputField]):>5.2f} | '\n",
    "    print(output)\n",
    "for clf, label in zip([\n",
    "            #clf1,\n",
    "            clf2, clf3, clf4\n",
    "            #, clf5,clf6,clf7,clf8,clf9\n",
    "            , eclf]\n",
    "            , [ \n",
    "                #\"LinearSVC\",\n",
    "                \"RandomForestClassifier\"\n",
    "                #,\"GaussianNB()\"\n",
    "                ,\"KNeighborsClassifier\"\n",
    "                #,\"SVC\",\"GaussianProcessClassifier\"\n",
    "                #,\"MLPClassifier\"\n",
    "                #         ,\"AdaBoostClassifier\",\"QuadraticDiscriminantAnalysis\"\n",
    "                ,\"Ensemble\"]):\n",
    "     scores = cross_val_score(clf, e_X_train, e_y_train, cv=5, scoring='accuracy')\n",
    "     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.983529Z",
     "start_time": "2019-04-07T08:45:06.422Z"
    }
   },
   "outputs": [],
   "source": [
    "#Note: Must fit the model prior to running this function\n",
    "def FeatImportance(model, dataColumns, title): \n",
    "# Calculate the feature ranking - Top 10 \n",
    "    importances = model.feature_importances_ \n",
    "    indices = np.argsort(importances)[::-1] \n",
    "    print (\"%s Top 10 Features\\n\" %title)\n",
    "    for f in range(10): \n",
    "        print(\"%d. %s (%f)\" % (f + 1, dataColumns.columns[indices[f]], importances[indices[f]])) \n",
    "    #Mean Feature Importance \n",
    "    print (\"\\nMean Feature Importance %.6f\" %np.mean(importances))\n",
    "    #Plot the feature importances of the forest \n",
    "    indices=indices[:10] \n",
    "    plt.figure() \n",
    "    plt.title(title+\" Top 10 Features\") \n",
    "    plt.bar(range(10), importances[indices], color=\"gb\", align=\"center\") \n",
    "    plt.xticks(range(10), dataColumns.columns[indices], fontsize=12, rotation=90) \n",
    "    plt.xlim([-1, 10]) \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master Model Result List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.989525Z",
     "start_time": "2019-04-07T08:45:06.771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "\t\tFinal                | precision:  0.67 | recall:  0.66 | f1-score:  0.67 | \n",
      "Model: GaussianNB\n",
      "\t\tFinal                | precision:  0.79 | recall:  0.62 | f1-score:  0.67 | \n",
      "Model: 3 Nearest Neighbors\n",
      "\t\tFinal                | precision:  0.85 | recall:  0.84 | f1-score:  0.84 | \n",
      "Model: linearSVC\n",
      "\t\tFinal                | precision:  0.83 | recall:  0.35 | f1-score:  0.46 | \n",
      "Model: RandomForest\n",
      "\t\tFinal                | precision:  0.84 | recall:  0.84 | f1-score:  0.84 | \n",
      "Model: Gradient Boosted Decision Trees (XGBoost)\n",
      "\t\tFinal                | precision:  0.88 | recall:  0.87 | f1-score:  0.88 | \n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=False):\n",
    "    test_model(LogisticRegression(), e_X_train, e_y_train, e_X_test, e_y_test, name=\"LogisticRegression\")\n",
    "    \n",
    "    test_model(GaussianNB(), e_X_train, e_y_train, e_X_test, e_y_test, name=\"GaussianNB\")\n",
    "    \n",
    "    test_model(KNeighborsClassifier(n_neighbors=3), e_X_train, e_y_train, e_X_test, e_y_test, name=\"3 Nearest Neighbors\")\n",
    "    \n",
    "    test_model(LinearSVC(), e_X_train, e_y_train, e_X_test, e_y_test, name='linearSVC')\n",
    "    \n",
    "    test_model(RandomForestClassifier(n_estimators = 10), e_X_train, e_y_train, e_X_test, e_y_test, name='RandomForest')\n",
    "    \n",
    "    test_model(XGBClassifier(max_depth=10, learning_rate=0.3, n_estimators=200, n_jobs=4), e_X_train, e_y_train, e_X_test, e_y_test, name=f'Gradient Boosted Decision Trees (XGBoost)')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "    \n",
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show that the bet possible model that gives consistant results is the Gradient Boosted Decision Tree. We could reach 88% precision and recall. Our experiment with the ensemble model could not improve the accuracy any further. our best modes, XGBClassifier, with learning rate of 0.3 and maximum depth of 10, when run wiht 100 estimators gives us the 88% precision, 87% recall rate and F1 Score of .88.\n",
    "\n",
    "This model runs reasonably fast considering the complexity of the dataset. We were also careful to not to to make this model overfitted with the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Important Features \n",
    "The story that came out of the data was, in many ways, were quire counter intuitive. At the end of the analysis, the key components.\n",
    "    1. Elevation (0.162398)\n",
    "    2. Elevation_Of_Hydrology (0.151600)\n",
    "    3. Mean_Distance_To_Feature (0.070873)\n",
    "    4. Horizontal_Distance_To_Roadways (0.070431)\n",
    "    5. Euclidean_Distance_To_Hydrology (0.057154)\n",
    "    6. Horizontal_Distance_To_Fire_Points (0.051782)\n",
    "    7. Hillshade_Noon (0.036232)\n",
    "    8. Aspect (0.035743)\n",
    "    9. Hillshade_9am (0.035738)\n",
    "    10. Vertical_Distance_To_Hydrology (0.035153)\n",
    "We were surpised that all the details of the soil type, at the end, did not really matter for this dataset. It is true in this case may not be universally true.\n",
    "\n",
    "#### What can be done to improve further \n",
    "    1. More Data Engineering\n",
    "    Due to the time restrictions and large number of variables, we were not able to extract all possibly new features that could improve the models further.\n",
    "    2. Mode model and parameter testing in ensemble\n",
    "    The processing power needed to run complex combination of models, parameters take very long time to proces. and hence we could not test all possible combinations.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annexes\"></a>\n",
    "    \n",
    "# Annexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annexA\"></a>\n",
    "\n",
    "## Annex A: Exploratory Data Analysis\n",
    "\n",
    "This appendix contains some of our exploratory data analysis. This includes the code used to generate the 4-number summaries of our data reflected in the [_About the Data_](#aboutTheData) and other summaries. The most informative portions are replicated in the main body of the report.\n",
    "\n",
    "After we load the data from the source file, we examine the basic characteristics of the dataset.\n",
    "  1. We expect to see all of the features discussed above represented in our column names\n",
    "  1. As there is no separate dataset containing the labels for our observations, we would expect to see the 'Cover_Type' variable in our data\n",
    "  1. We would expect to see a shape of (15120, 55) - the 54 features plus our label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.994528Z",
     "start_time": "2019-04-07T08:45:08.073Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Columns: {full_data.columns}')\n",
    "print(f'Shape: {full_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the first several observations to get a sense for the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.996525Z",
     "start_time": "2019-04-07T08:45:08.249Z"
    }
   },
   "outputs": [],
   "source": [
    "full_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to get a high-level summary of each of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:14.998527Z",
     "start_time": "2019-04-07T08:45:08.415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Small function to give us a bird's-eye summary of the data\n",
    "def five_num_summary(df, column):\n",
    "    print(f'Column: {column:<35} | ' +\n",
    "          f'Max value: {np.max(df[column]):>6} | ' + \n",
    "          f'Min value: {np.min(df[column]):>7.2f} | ' +\n",
    "          f'Mean: {np.mean(df[column]):>7.2f} | ' +\n",
    "          f'Median: {np.median(df[column]):>7.2f}')\n",
    "\n",
    "for col_name in full_features.columns:\n",
    "    five_num_summary(full_features, col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Label Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful for us to understand whether we have an imbalanced dataset (i.e., one where certain labels/categories are overrepresented relative to others.) Here we'll quickly describe our training and test labels and just make sure our classes are balanced. We can do this both graphically and numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.001528Z",
     "start_time": "2019-04-07T08:45:09.065Z"
    }
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3), sharey=True)\n",
    "bins = np.arange(8) + 0.5\n",
    "ax1.hist(y_train, bins, width = 0.8)\n",
    "ax1.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax1.set_title('Labels')\n",
    "ax2.hist(y_test, bins, width = 0.8)\n",
    "ax2.set_title('Test Labels')\n",
    "plt.xticks(range(8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.003529Z",
     "start_time": "2019-04-07T08:45:09.073Z"
    }
   },
   "outputs": [],
   "source": [
    "print(stats.describe(full_labels))\n",
    "print(stats.describe(y_test))\n",
    "for i in range(0, 8):\n",
    "        print(f'i = {i}: Train Ct: {(full_labels==i).sum():>5} | Test Ct: {(y_test==i).sum():>5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our classes are quite well-balanced in both our training data and the test data.\n",
    "\n",
    "This is good both because we will not need to deliberately compensate for imbalances and because our model will be unable to achieve reasonable performance simply by guessing the modal category. (Doing so would give accuracy on the training set of 1741/12096 = 0.145, and then accuracy on the test set of 411/3024 = 0.136.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing of note is that the `Soil_Type7` and `Soil_Type15` are never true, so this feature tells us nothing.  These features should be removed before any modeling is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.014528Z",
     "start_time": "2019-04-07T08:45:09.803Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = np.arange(0, 360, 10)\n",
    "cut = [0, 45, 90, 135, 180, 225, 270, 315, 360]\n",
    "\n",
    "print(bins)\n",
    "pd.cut(bins, cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:48:15.016527Z",
     "start_time": "2019-04-07T08:45:09.809Z"
    }
   },
   "outputs": [],
   "source": [
    "full_data['Total_Hillshade'] = full_data[['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']].sum(axis=1)\n",
    "full_data[['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Total_Hillshade']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:18:15.186610Z",
     "start_time": "2019-04-07T12:18:14.649610Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make 3D scatterplot to explore water, elevation, and hillshade concurrently\n",
    "\n",
    "%matplotlib qt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sparsifier = np.random.randint(0, full_features.shape[0], 5000)\n",
    "\n",
    "sparsified = full_features.iloc[sparsifier,:]\n",
    "sparse_labels = full_labels.iloc[sparsifier]\n",
    "# print(f'Length of sparsified dataset\\n: {sparsified}')\n",
    "\n",
    "full_features['Euclidean_distance_to_water'] = np.sqrt(full_features['Horizontal_Distance_To_Hydrology']**2 + full_features['Vertical_Distance_To_Hydrology']**2)\n",
    "dist_to_water = sparsified['Euclidean_distance_to_water']\n",
    "altitude = sparsified['Elevation']\n",
    "hillshade = sparsified['Hillshade_3pm']\n",
    "color_dict = {1: '#A7C6ED', 2: '#BA0C2F', 3: '#651D32', 4: '#8C8985',\n",
    "              5: '#212721', 6: '#002F6C', 7: '#FFC000'}\n",
    "coloration = [color_dict[x] for x in sparse_labels]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(hillshade, dist_to_water, altitude, color=coloration, alpha=0.6)\n",
    "# ax.title('Forest cover categorization\\nby distance to water and hillshade')\n",
    "ax.view_init(30, 115)\n",
    "# mouse_init(rotate_btn=1, zoom_btn=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
